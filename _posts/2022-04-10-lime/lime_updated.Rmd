---
title: "Local Interpretable Model-Agnostic Explanations (LIME)"
description: |
   Don't worry, you won't feel any sourness while using the method
author:
  - name: Jasper Lok
    url: {}
date: 04-10-2022
output:
  distill::distill_article:
    toc: true
    toc_depth: 4
    self_contained: false
bibliography: ["ref.bib"]
biblio-style: "apa"
link-citations: true
categories:
  - Machine Learning
  - Model Explanability

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

packages <- c("captioner", "knitr", "kableExtra")

for (p in packages){
  if(!require (p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

knitr::opts_chunk$set(fig.retina = 3,                       
                      echo = TRUE,                       
                      eval = TRUE,                       
                      message = FALSE,                       
                      warning = FALSE,
                      out.width="100%")

library(captioner)

```


In this post, I will be exploring local interpretable model-agnostic explanations (LIME), i.e. one of the model explainability method.



















































