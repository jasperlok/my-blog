<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #20794d; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #20794d; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

  <!--radix_placeholder_meta_tags-->
  <title>Transfer Learning with Convolution Neural Network</title>

  <meta property="description" itemprop="description" content="Why learn from scratch when you can leverage the existing work?"/>


  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2021-09-17"/>
  <meta property="article:created" itemprop="dateCreated" content="2021-09-17"/>
  <meta name="article:author" content="Jasper Lok"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Transfer Learning with Convolution Neural Network"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Why learn from scratch when you can leverage the existing work?"/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Transfer Learning with Convolution Neural Network"/>
  <meta property="twitter:description" content="Why learn from scratch when you can leverage the existing work?"/>

  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Machine learning mastery: A gentle introduction to transfer learning for deep learning;citation_publication_date=2019;citation_author=Jason Brownlee"/>
  <meta name="citation_reference" content="citation_title=TensorFlow: Transfer learning and fine-tuning;citation_publication_date=2021;citation_author= Google"/>
  <meta name="citation_reference" content="citation_title=Analytics vidhya: Transfer learning and the art of using pre-trained models in deep learning;citation_publication_date=2017;citation_author=Dishashree Gupta"/>
  <meta name="citation_reference" content="citation_title=Machine learning mastery: How to configure image data augmentation in keras;citation_publication_date=2019;citation_author=Jason Brownlee"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","output","bibliography","biblio-style","link-citations","categories"]}},"value":[{"type":"character","attributes":{},"value":["Transfer Learning with Convolution Neural Network"]},{"type":"character","attributes":{},"value":["Why learn from scratch when you can leverage the existing work?\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Jasper Lok"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":[]}},"value":[]}]}]},{"type":"character","attributes":{},"value":["09-17-2021"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["toc","toc_depth","self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[4]},{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["ref.bib"]},{"type":"character","attributes":{},"value":["apa"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["Tensorflow/Keras","Deep Learning","Image Recognition","Transfer Learning"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["Deep-Learning_MultiClass_files/anchor/anchor.min.js","Deep-Learning_MultiClass_files/bowser/bowser.min.js","Deep-Learning_MultiClass_files/distill/template.v2.js","Deep-Learning_MultiClass_files/figure-html5/unnamed-chunk-13-1.png","Deep-Learning_MultiClass_files/figure-html5/unnamed-chunk-19-1.png","Deep-Learning_MultiClass_files/figure-html5/unnamed-chunk-21-1.png","Deep-Learning_MultiClass_files/header-attrs/header-attrs.js","Deep-Learning_MultiClass_files/jquery/jquery.min.js","Deep-Learning_MultiClass_files/popper/popper.min.js","Deep-Learning_MultiClass_files/tippy/tippy-bundle.umd.min.js","Deep-Learning_MultiClass_files/tippy/tippy-light-border.css","Deep-Learning_MultiClass_files/tippy/tippy.css","Deep-Learning_MultiClass_files/tippy/tippy.umd.min.js","Deep-Learning_MultiClass_files/webcomponents/webcomponents.js","image/cat.jpeg","image/cat_sleep.jpg","image/CNN Structure.png","image/dog.jpeg","image/fine_tune.png","image/image_on_wall.jpg","image/squirrel.jpeg","image/tf_hub.png","ref.bib"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure img {
    width: 100%;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        var refHtml = $('#ref-' + $(this).attr('data-cites')).html();
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="Deep-Learning_MultiClass_files/header-attrs/header-attrs.js"></script>
  <script src="Deep-Learning_MultiClass_files/jquery/jquery.min.js"></script>
  <script src="Deep-Learning_MultiClass_files/popper/popper.min.js"></script>
  <link href="Deep-Learning_MultiClass_files/tippy/tippy.css" rel="stylesheet" />
  <link href="Deep-Learning_MultiClass_files/tippy/tippy-light-border.css" rel="stylesheet" />
  <script src="Deep-Learning_MultiClass_files/tippy/tippy.umd.min.js"></script>
  <script src="Deep-Learning_MultiClass_files/anchor/anchor.min.js"></script>
  <script src="Deep-Learning_MultiClass_files/bowser/bowser.min.js"></script>
  <script src="Deep-Learning_MultiClass_files/webcomponents/webcomponents.js"></script>
  <script src="Deep-Learning_MultiClass_files/distill/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Transfer Learning with Convolution Neural Network","description":"Why learn from scratch when you can leverage the existing work?","authors":[{"author":"Jasper Lok","authorURL":{},"affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2021-09-17T00:00:00.000+08:00","citationText":"Lok, 2021"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Transfer Learning with Convolution Neural Network</h1>
<!--radix_placeholder_categories-->
<div class="dt-tags">
<div class="dt=tag">Tensorflow/Keras</div>
<div class="dt=tag">Deep Learning</div>
<div class="dt=tag">Image Recognition</div>
<div class="dt=tag">Transfer Learning</div>
</div>
<!--/radix_placeholder_categories-->
<p><p>Why learn from scratch when you can leverage the existing work?</p></p>
</div>

<div class="d-byline">
  Jasper Lok true 
  
<br/>09-17-2021
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#how-is-image-recognition-relevant-for-insurance">How is image recognition relevant for insurance?</a>
<ul>
<li><a href="#shorten-operation-process">Shorten operation process</a></li>
<li><a href="#additional-pricing-parameters">Additional Pricing Parameters</a></li>
<li><a href="#additional-parameters-in-claim-estimation">Additional Parameters in Claim Estimation</a></li>
</ul></li>
<li><a href="#what-is-transfer-learning">What is ‘Transfer Learning?’</a></li>
<li><a href="#pre-trained-models">Pre-trained Models</a></li>
<li><a href="#considerations-when-using-pre-trained-models">Considerations when using Pre-trained Models</a>
<ul>
<li><a href="#data-similarity-is-the-current-dataset-similar-to-the-dataset-used-to-train-pre-trained-model">Data similarity: Is the current dataset similar to the dataset used to train pre-trained model?</a></li>
<li><a href="#size-of-the-data-set-how-big-is-our-dataset">Size of the data set: How big is our dataset?</a></li>
<li><a href="#how-are-we-training-the-model">How are we training the model?</a></li>
</ul></li>
<li><a href="#model-building-steps">Model Building Steps</a></li>
<li><a href="#demonstration">Demonstration</a>
<ul>
<li><a href="#setup-the-environment">Setup the environment</a></li>
<li><a href="#import-data">Import data</a></li>
<li><a href="#import-the-pre-trained-model">Import the Pre-trained model</a></li>
<li><a href="#add-on-classifier-layer">Add on Classifier Layer</a></li>
<li><a href="#image-augmentation">Image Augmentation</a></li>
<li><a href="#model-preparation">Model Preparation</a>
<ul>
<li><a href="#model-fitting">Model Fitting</a></li>
<li><a href="#visualizing-the-model-results">Visualizing the Model Results</a></li>
</ul></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</nav>
</div>
<p>Recently I happened to come across this post by <span class="citation" data-cites="chollet2017image">(<a href="#ref-chollet2017image" role="doc-biblioref">Chollet and Allaire 2017</a>)</span> on RStudio AI Blog and inspired me to give it a try to build a convolution neural network (CNN) model to solve an image classification problem.</p>
<p>Before jumping into the discussion, let’s take a look at how image recognition can be used in the insurance context.</p>
<h2 id="how-is-image-recognition-relevant-for-insurance">How is image recognition relevant for insurance?</h2>
<p>Papers are suggesting how the image recognition technology can be used in the insurance context.</p>
<p>In the ‘Applying Image Recognition to Insurance’ report, <span class="citation" data-cites="Shang2018">(<a href="#ref-Shang2018" role="doc-biblioref">Shang 2018</a>)</span> explored a few examples of how insurers could leverage image recognition.</p>
<h3 id="shorten-operation-process">Shorten operation process</h3>
<p>Image recognition can be utilized at different stages of the insurance stage (eg. policy inception, update policy information, policy claim), where such use cases in this area can be observed in some countries.</p>
<p>For example, instead of manually inputting necessary info into the different documents at the point of policy inception, insurers could build an algorithm that would capture the details in the images uploaded. This could potentially shorten the turnaround time, resulting in an improvement in customer satisfaction.</p>
<h3 id="additional-pricing-parameters">Additional Pricing Parameters</h3>
<p><span class="citation" data-cites="Shang2018">(<a href="#ref-Shang2018" role="doc-biblioref">Shang 2018</a>)</span> also discussed how these techniques can be used for insurance pricing. For example, in property insurance, pictures can be used to understand the riskiness of the property, providing additional parameters for pricing and underwriting purpose.</p>
<p>Nevertheless, this could allow the insurers in implementing dynamic pricing on the relevant business lines, improving the profitability of the business.</p>
<h3 id="additional-parameters-in-claim-estimation">Additional Parameters in Claim Estimation</h3>
<p><span class="citation" data-cites="Shang2018">(<a href="#ref-Shang2018" role="doc-biblioref">Shang 2018</a>)</span> also provided an example of how image recognition could be applied in real-time risk monitoring and risk management. Insurers can use the information extracted from the image to predict claim count and claim amount.</p>
<p>Nevertheless, in this exercise, instead of building the model from scratch, I will use what is known as “transfer learning” to speed up the model-building problem.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="image/image_on_wall.jpg" width="80%" /></p>
</div>
<p><em>Photo by cottonbro from Pexels</em></p>
<h2 id="what-is-transfer-learning">What is ‘Transfer Learning?’</h2>
<p><span class="citation" data-cites="Brownlee2019">(<a href="#ref-Brownlee2019" role="doc-biblioref">Brownlee 2019a</a>)</span> explained that transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.</p>
<p><span class="citation" data-cites="Google2021">(<a href="#ref-Google2021" role="doc-biblioref">Google 2021</a>)</span> also explained that the intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.</p>
<h2 id="pre-trained-models">Pre-trained Models</h2>
<p>There are different types of pre-trained models that are trained to solve different types of problems. Some are trained to classify images, some are trained to handle text-related problems, and so on.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="image/tf_hub.png" width="80%" /></p>
</div>
<p>Do check out this <a href="https://tfhub.dev/">link</a> to know about the different pre-trained models in TensorFlow.</p>
<p>For this analysis, I will focus the pre-trained models on the image classification problem. I will use <a href="https://keras.rstudio.com/reference/application_vgg.html">VGG16 model</a> to build a multi-class classification for the dataset. The relevant paper of this paper can be found in this <a href="https://arxiv.org/abs/1409.1556">link</a>.</p>
<h2 id="considerations-when-using-pre-trained-models">Considerations when using Pre-trained Models</h2>
<p>The considerations of using pre-trained models can be summarized as the following diagram:</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="image/fine_tune.png" width="70%" /></p>
</div>
<p><em>Diagram on Model Tuning Considerations <span class="citation" data-cites="Gupta2017">(<a href="#ref-Gupta2017" role="doc-biblioref">Gupta 2017</a>)</span></em></p>
<h3 id="data-similarity-is-the-current-dataset-similar-to-the-dataset-used-to-train-pre-trained-model">Data similarity: Is the current dataset similar to the dataset used to train pre-trained model?</h3>
<p>These pre-trained models are usually commonly trained by using image data from <a href="https://www.image-net.org/index.php">ImageNet</a>, where ImageNet is an image database with more than 14 million images. As such, this makes the pre-trained models generalized models.</p>
<p>So, before using the pre-trained models, we should ask ourselves how similar is between our dataset and the dataset used in pre-trained model. If our images are very different from the image data used in training the pre-trained models, the accuracy of our model is likely to be low.</p>
<h3 id="size-of-the-data-set-how-big-is-our-dataset">Size of the data set: How big is our dataset?</h3>
<p>Another important consideration is how big is our dataset. The model is unlikely to perform well if the dataset is not large enough to train the models. Therefore, pre-trained models can be used as an alternative if our dataset is not large enough.</p>
<h3 id="how-are-we-training-the-model">How are we training the model?</h3>
<p>These pre-trained models typically come with the model weights that are derived by fitting the models with images from ImageNet. Hence, when we are using pre-trained models, one of the key questions we should answer is whether we should be using the model weights in the pre-trained models.</p>
<p>The usual approach is to freeze all the layers in the pre-trained model, except the few top layers. The few top unfreeze layers will be fine-tuned together with the classifier layer.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="image/CNN%20Structure.png" width="100%" /></p>
</div>
<p><em>CNN structure <span class="citation" data-cites="Sumit2018">(<a href="#ref-Sumit2018" role="doc-biblioref">Saha 2018</a>)</span></em></p>
<p>Do note that the more layers we unfreeze, the longer it would take to fit the model.</p>
<h2 id="model-building-steps">Model Building Steps</h2>
<p>In general, the model building steps can be summarized as follows:</p>
<ul>
<li><p>Split the dataset into train and validation dataset</p></li>
<li><p>Import pre-trained model into the environment and exclude the classifier layer from the pre-trained model</p></li>
<li><p>Add on classifier layer on top of the pre-trained model</p></li>
<li><p>Freeze the weights of the pre-trained model, except for a few top layers to be tuned together with the classifier layer</p></li>
<li><p>Train the unfreeze top layers and classifier layer to make the model more relevant for the specific task</p></li>
</ul>
<h2 id="demonstration">Demonstration</h2>
<p>I will be using <a href="https://www.kaggle.com/alessiocorrado99/animals10">this image dataset</a> from Kaggle. There are about 28k medium-quality images in this data. These photos are from 10 different categories of animals, which consist of dog, cat, horse, spider, butterfly, chicken, sheep, cow, squirrel, and elephant.</p>
<p>Below are some of the extracted images from the dataset:</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="image/cat.jpeg" width="80%" style="display: block; margin: auto;" /><img src="image/dog.jpeg" width="80%" style="display: block; margin: auto;" /><img src="image/squirrel.jpeg" width="80%" style="display: block; margin: auto;" /></p>
</div>
<p><em>I have chosen these as the examples as I thought they looks funny.</em></p>
<h3 id="setup-the-environment">Setup the environment</h3>
<p>As usual, I will start by calling all the relevant packages I would need in the analysis later on.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>packages</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>'tidyverse'</span>, <span class='st'>'readr'</span>, <span class='st'>'skimr'</span>, <span class='st'>'keras'</span><span class='op'>)</span>

<span class='kw'>for</span><span class='op'>(</span><span class='va'>p</span> <span class='kw'>in</span> <span class='va'>packages</span><span class='op'>)</span><span class='op'>{</span>
  <span class='kw'>if</span><span class='op'>(</span><span class='op'>!</span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>require</a></span> <span class='op'>(</span><span class='va'>p</span>, character.only <span class='op'>=</span> <span class='cn'>T</span><span class='op'>)</span><span class='op'>)</span><span class='op'>{</span>
    <span class='fu'><a href='https://rdrr.io/r/utils/install.packages.html'>install.packages</a></span><span class='op'>(</span><span class='va'>p</span><span class='op'>)</span>
  <span class='op'>}</span>
  <span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'>p</span>, character.only <span class='op'>=</span> <span class='cn'>T</span><span class='op'>)</span>
<span class='op'>}</span>
</code></pre>
</div>
</div>
<h3 id="import-data">Import data</h3>
<p>Next, I will import the dataset into the environment.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>As mentioned in the post earlier, there are 10 different categories in the dataset. Hence this is a multi-class classification problem. Since the photos are saved under the respective categories, I will use <code>dir</code> function to extract out the list of categories, where <code>train_dir</code> is the link to the folder I stored the image dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>label_list</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/list.files.html'>dir</a></span><span class='op'>(</span><span class='va'>train_dir</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Next, <code>length</code> function is used to perform a count on the number of categories.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>output_n</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/length.html'>length</a></span><span class='op'>(</span><span class='va'>label_list</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Next, I will define the input size to be passed into the CNN model later. I have also defined the channel to be 3 as the photos are colored photos.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>width</span> <span class='op'>&lt;-</span> <span class='fl'>224</span>
<span class='va'>height</span><span class='op'>&lt;-</span> <span class='fl'>224</span>
<span class='va'>rgb</span> <span class='op'>&lt;-</span> <span class='fl'>3</span> 

<span class='va'>target_size</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>width</span>, <span class='va'>height</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<h3 id="import-the-pre-trained-model">Import the Pre-trained model</h3>
<p>As discussed earlier, I would use the pre-trained model to classify the image. Over here, I will be using VGG16 as shown in the code chunk below. As I won’t be re-trained the entire network, I will indicate the model weights should follow the derived weights when the model is trained by using ImageNet dataset.</p>
<p>Also, to include our classifier layer, I will indicate the <code>include_top</code> argument to be false so that the pre-trained model is imported without the classifier layer.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>conv_base</span> <span class='op'>&lt;-</span> <span class='fu'>application_vgg16</span><span class='op'>(</span>
  weights <span class='op'>=</span> <span class='st'>"imagenet"</span>,
  include_top <span class='op'>=</span> <span class='cn'>FALSE</span>,
  input_shape <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>width</span>, <span class='va'>height</span>, <span class='va'>rgb</span><span class='op'>)</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<p>For other pre-trained models, do refer to the ‘Applications’ section under either <a href="https://keras.rstudio.com/reference/index.html#section-applications">Keras documentation page</a> or <a href="https://tensorflow.rstudio.com/reference/keras/">TensorFlow documentation page</a> for more information.</p>
<p>Next, <code>summary</code> function can be used to visualize how the pre-trained model looks like.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/base/summary.html'>summary</a></span><span class='op'>(</span><span class='va'>conv_base</span><span class='op'>)</span>
</code></pre>
</div>
<pre><code>Model: &quot;vgg16&quot;
______________________________________________________________________
Layer (type)                   Output Shape                Param #    
======================================================================
input_1 (InputLayer)           [(None, 224, 224, 3)]       0          
______________________________________________________________________
block1_conv1 (Conv2D)          (None, 224, 224, 64)        1792       
______________________________________________________________________
block1_conv2 (Conv2D)          (None, 224, 224, 64)        36928      
______________________________________________________________________
block1_pool (MaxPooling2D)     (None, 112, 112, 64)        0          
______________________________________________________________________
block2_conv1 (Conv2D)          (None, 112, 112, 128)       73856      
______________________________________________________________________
block2_conv2 (Conv2D)          (None, 112, 112, 128)       147584     
______________________________________________________________________
block2_pool (MaxPooling2D)     (None, 56, 56, 128)         0          
______________________________________________________________________
block3_conv1 (Conv2D)          (None, 56, 56, 256)         295168     
______________________________________________________________________
block3_conv2 (Conv2D)          (None, 56, 56, 256)         590080     
______________________________________________________________________
block3_conv3 (Conv2D)          (None, 56, 56, 256)         590080     
______________________________________________________________________
block3_pool (MaxPooling2D)     (None, 28, 28, 256)         0          
______________________________________________________________________
block4_conv1 (Conv2D)          (None, 28, 28, 512)         1180160    
______________________________________________________________________
block4_conv2 (Conv2D)          (None, 28, 28, 512)         2359808    
______________________________________________________________________
block4_conv3 (Conv2D)          (None, 28, 28, 512)         2359808    
______________________________________________________________________
block4_pool (MaxPooling2D)     (None, 14, 14, 512)         0          
______________________________________________________________________
block5_conv1 (Conv2D)          (None, 14, 14, 512)         2359808    
______________________________________________________________________
block5_conv2 (Conv2D)          (None, 14, 14, 512)         2359808    
______________________________________________________________________
block5_conv3 (Conv2D)          (None, 14, 14, 512)         2359808    
______________________________________________________________________
block5_pool (MaxPooling2D)     (None, 7, 7, 512)           0          
======================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
______________________________________________________________________</code></pre>
</div>
<h3 id="add-on-classifier-layer">Add on Classifier Layer</h3>
<p>Next, I will add on the classifier layer as discussed in the earlier post. As this is a multi-class classification problem, hence <code>softmax</code> is selected to be the last activation function.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>model</span> <span class='op'>&lt;-</span> <span class='fu'>keras_model_sequential</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
  <span class='va'>conv_base</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>layer_flatten</span><span class='op'>(</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='fl'>256</span>, activation <span class='op'>=</span> <span class='st'>"relu"</span><span class='op'>)</span> <span class='op'>%&gt;%</span> 
  <span class='fu'>layer_dense</span><span class='op'>(</span>units <span class='op'>=</span> <span class='va'>output_n</span>, activation <span class='op'>=</span> <span class='st'>"softmax"</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>I will call the <code>summary</code> function to check on the model before fitting the model.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/base/summary.html'>summary</a></span><span class='op'>(</span><span class='va'>model</span><span class='op'>)</span>
</code></pre>
</div>
<pre><code>Model: &quot;sequential&quot;
______________________________________________________________________
Layer (type)                   Output Shape                Param #    
======================================================================
vgg16 (Functional)             (None, 7, 7, 512)           14714688   
______________________________________________________________________
flatten (Flatten)              (None, 25088)               0          
______________________________________________________________________
dense_1 (Dense)                (None, 256)                 6422784    
______________________________________________________________________
dense (Dense)                  (None, 10)                  2570       
======================================================================
Total params: 21,140,042
Trainable params: 21,140,042
Non-trainable params: 0
______________________________________________________________________</code></pre>
</div>
<h3 id="image-augmentation">Image Augmentation</h3>
<p>Image augmentation is a commonly used technique to ensure the model is not overfit.</p>
<p><span class="citation" data-cites="BrownleeKeras2019">(<a href="#ref-BrownleeKeras2019" role="doc-biblioref">Brownlee 2019b</a>)</span> explained this technique allows one to artificially created new training data from existing data. The author further explained that the reason for using such a method is this method will aid the modern deep learning algorithms to learn the different features that are similar, but not entirely the same as the training photo.</p>
<p>For example, it would be a problem if the objects always appear on the same side of the photo in our dataset. The algorithm may not be able to identify the objects if the objects do not appear on the same side as what is observed under the training dataset in the new photo.</p>
<p>So, the code chunk below will perform image augmentation on the training data.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>train_datagen</span> <span class='op'>=</span> <span class='fu'>image_data_generator</span><span class='op'>(</span>
  rescale <span class='op'>=</span> <span class='fl'>1</span><span class='op'>/</span><span class='fl'>255</span>,
  rotation_range <span class='op'>=</span> <span class='fl'>40</span>,
  width_shift_range <span class='op'>=</span> <span class='fl'>0.2</span>,
  height_shift_range <span class='op'>=</span> <span class='fl'>0.2</span>,
  shear_range <span class='op'>=</span> <span class='fl'>0.2</span>,
  zoom_range <span class='op'>=</span> <span class='fl'>0.2</span>,
  horizontal_flip <span class='op'>=</span> <span class='cn'>TRUE</span>,
  fill_mode <span class='op'>=</span> <span class='st'>"nearest"</span>,
  validation_split <span class='op'>=</span> <span class='fl'>0.2</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Apart from that, 20% of the training dataset is being held back as the validation dataset.</p>
<p>Also, one important note to take note of while performing image augmentation is the validation &amp; test dataset should not be augmented.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>validation_datagen</span> <span class='op'>&lt;-</span> <span class='fu'>image_data_generator</span><span class='op'>(</span>rescale <span class='op'>=</span> <span class='fl'>1</span><span class='op'>/</span><span class='fl'>255</span><span class='op'>)</span>  
</code></pre>
</div>
</div>
<h3 id="model-preparation">Model Preparation</h3>
<p>To leverage on transfer learning, typically we will freeze the base model and only unfreeze the connecting layers so that we can fine-tune the layers together with the classifier layer.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>freeze_weights</span><span class='op'>(</span><span class='va'>conv_base</span><span class='op'>)</span>

<span class='fu'>unfreeze_weights</span><span class='op'>(</span><span class='va'>conv_base</span>, from <span class='op'>=</span> <span class='st'>"block5_conv1"</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Next, I will generate batches of data in the code chunk below.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>train_generator</span> <span class='op'>&lt;-</span> <span class='fu'>flow_images_from_directory</span><span class='op'>(</span>
  <span class='va'>train_dir</span>,                  <span class='co'># Target directory  </span>
  <span class='va'>train_datagen</span>,              <span class='co'># Data generator</span>
  target_size <span class='op'>=</span> <span class='va'>target_size</span>,  <span class='co'># Resizes all images to 150 × 150</span>
  class_mode <span class='op'>=</span> <span class='st'>"categorical"</span>       <span class='co'># binary_crossentropy loss for binary labels</span>
<span class='op'>)</span>

<span class='va'>validation_generator</span> <span class='op'>&lt;-</span> <span class='fu'>flow_images_from_directory</span><span class='op'>(</span>
  <span class='va'>train_dir</span>,
  <span class='va'>validation_datagen</span>,
  target_size <span class='op'>=</span> <span class='va'>target_size</span>,
  class_mode <span class='op'>=</span> <span class='st'>"categorical"</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Lastly, I will define all the different model components before fitting the model.</p>
<p>I will be using <code>optimizer_sgd</code> (ie. stochastic gradient descent optimizer) to fit the model. Do check out the documentation page on the different optimizers.</p>
<p>As this is a multi-class classification problem, hence I will be using <code>categorical_accuracy</code> as the performance metric.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>model</span> <span class='op'>%&gt;%</span> <span class='fu'>compile</span><span class='op'>(</span>
  loss <span class='op'>=</span> <span class='st'>"binary_crossentropy"</span>,
  optimizer <span class='op'>=</span> <span class='fu'>optimizer_sgd</span><span class='op'>(</span><span class='op'>)</span>,
  metrics <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"categorical_accuracy"</span><span class='op'>)</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<h4 id="model-fitting">Model Fitting</h4>
<p>Once the different model components are being defined, I will start the model fitting as shown below.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>history</span> <span class='op'>&lt;-</span> <span class='va'>model</span> <span class='op'>%&gt;%</span> <span class='fu'>fit_generator</span><span class='op'>(</span>
  <span class='va'>train_generator</span>,
  steps_per_epoch <span class='op'>=</span> <span class='fl'>100</span>,
  epochs <span class='op'>=</span> <span class='fl'>10</span>,
  validation_data <span class='op'>=</span> <span class='va'>validation_generator</span>,
  validation_steps <span class='op'>=</span> <span class='fl'>50</span>
<span class='op'>)</span>
</code></pre>
</div>
</div>
<h4 id="visualizing-the-model-results">Visualizing the Model Results</h4>
<p>Once the model is fit, the history from the model fitting is passed into <code>plot</code> function to visualize how the results change when the epochs increase.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>history</span><span class='op'>)</span>
</code></pre>
</div>
<p><img src="Deep-Learning_MultiClass_files/figure-html5/unnamed-chunk-21-1.png" width="80%" /></p>
</div>
<p>Yay! Based on the accuracy shown above, it shows that the built model does have some levels of predictability, ie. the model will perform better than a random guess on the image category. Indeed, as the epoch increases, the model accuracy increases as well.</p>
<p>Also, as we can see in the graph above, the accuracy increases when the epoch increases. This suggests the accuracy could increase further if we increase the number of epoch.</p>
<h2 id="conclusion">Conclusion</h2>
<p>That’s all for today!</p>
<p>Thanks for reading the post until the end.</p>
<p>Do check out on the <a href="https://keras.rstudio.com/index.html">Keras documentation page</a> or <a href="https://tensorflow.rstudio.com/">TensorFlow documentation page</a> if you want to find out more on deep learning.</p>
<p>Feel free to contact me through <a href="mailto:jasper.jh.lok@gmail.com">email</a> or <a href="https://www.linkedin.com/in/jasper-l-13426232/">LinkedIn</a> if you have any suggestions on future topics to share.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="image/cat_sleep.jpg" width="80%" /></p>
</div>
<p><em>Photo by Александар Цветановић from Pexels</em></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Brownlee2019" class="csl-entry" role="doc-biblioentry">
Brownlee, Jason. 2019a. <span>“Machine Learning Mastery: A Gentle Introduction to Transfer Learning for Deep Learning.”</span> <a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/">https://machinelearningmastery.com/transfer-learning-for-deep-learning/</a>.
</div>
<div id="ref-BrownleeKeras2019" class="csl-entry" role="doc-biblioentry">
———. 2019b. <span>“Machine Learning Mastery: How to Configure Image Data Augmentation in Keras.”</span> <a href="https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/">https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/</a>.
</div>
<div id="ref-chollet2017image" class="csl-entry" role="doc-biblioentry">
Chollet, François, and J. J. Allaire. 2017. <span>“RStudio AI Blog: Image Classification on Small Datasets with Keras.”</span> <a href="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/">https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/</a>.
</div>
<div id="ref-Google2021" class="csl-entry" role="doc-biblioentry">
Google. 2021. <span>“TensorFlow: Transfer Learning and Fine-Tuning.”</span> <a href="https://www.tensorflow.org/tutorials/images/transfer_learning">https://www.tensorflow.org/tutorials/images/transfer_learning</a>.
</div>
<div id="ref-Gupta2017" class="csl-entry" role="doc-biblioentry">
Gupta, Dishashree. 2017. <span>“Analytics Vidhya: Transfer Learning and the Art of Using Pre-Trained Models in Deep Learning.”</span> <a href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/">https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/</a>.
</div>
<div id="ref-Sumit2018" class="csl-entry" role="doc-biblioentry">
Saha, Sumit. 2018. <span>“Towards Data Science: A Comprehensive Guide to Convolutional Neural Networks — the Eli5 Way.”</span> <a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a>.
</div>
<div id="ref-Shang2018" class="csl-entry" role="doc-biblioentry">
Shang, Kailan, ed. 2018. <span>“Applying Image Recognition to Insurance.”</span> Society of Actuaries. <a href="https://www.soa.org/globalassets/assets/Files/resources/research-report/2018/applying-image-recognition.pdf">https://www.soa.org/globalassets/assets/Files/resources/research-report/2018/applying-image-recognition.pdf</a>.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
