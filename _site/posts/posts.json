[
  {
    "path": "posts/2021-04-03-SAS-sharing-20210422/",
    "title": "Sharing at Singapore Actuarial Society",
    "description": "And yes, after  (many) ^ n  sleepless nights.....",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-04-03",
    "categories": [
      "talks"
    ],
    "contents": "\r\nUpdated on 13 May 2021\r\nSummary of update: - Include link to the presentation video, slides and documents\r\n\r\n\r\n\r\nAnd finally after months preparations, I will be sharing at Singapore Actuarial Society on how actuaries could leverage on modern data science packages in R to perform necessary actuarial analysis.\r\nBelow are the details of the event:\r\nEvent Organizer: Singapore Actuarial Society\r\nDate: 22 April 2021\r\nTime: 10 - 11.30 am\r\nEvent Link: Link\r\nWhile this research project focus on the end to end of data science project process (ie. from data wrangling, pre-processing, modeling and communication), this sharing will only be focusing on the modeling part due to the time constraint.\r\nFollowing is the links to the various documents from the presentations:\r\nPresentation video: https://youtu.be/CKPRjkiKqbQ.\r\nPresentation slides: link\r\nOther documents (eg. R codes, research report): https://github.com/jasperlok/SAS-Sharing_20210422\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-03-SAS-sharing-20210422/image/SAS Talk.jpg",
    "last_modified": "2021-05-14T00:56:55+08:00",
    "input_file": "sas-sharing.utf8.md"
  },
  {
    "path": "posts/2021-01-22-data-wrangling/",
    "title": "Data Wrangling - The Quest to Tame The 'Beast'",
    "description": "Rawwwwwwwwww!",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "dataWrangling"
    ],
    "contents": "\r\n\r\n\r\n\r\nArtwork on Data Wrangling by Allison Horst\r\nAs mentioned in my last post, it has been a long journey to learn data science pretty much from scratch. I intend to share some of my learning to help others to start their data science journey as well.\r\nTherefore, for my second post, I will share about data wrangling, which is often the essential task before building any machine learning models. Hence this post will serve as a building block of my future machine learning posts.\r\nAlso, I intend to share some of the modern data science R package and how these packages could benefit some of the analysis tasks.\r\nBackground\r\nIn R for Data Science book, Wickham & Grolemund mentioned that a typical data science project would look as following:\r\n\r\n\r\n\r\nExtracted from R for Data Science\r\nAfter importing the data, the very first step usually involves some levels of data wrangling. This is probably one of the most important steps as without a good dataset, the analysis is unlikely to be meaningful.\r\nGolden rule –> Garbage in, garbage out\r\nA good data exploratory will help to answer the following questions (not limited to):  - Does this dataset answer the business questions (eg. does the dataset have the necessary info to predict which customers are likely to purchase in the next promotion?)?  - Are there any issues that exist in the dataset (eg. missing value, excessive categories, value range vary widely)?  - Any data transformation required? As some of the machine learning models can accept certain data types. \r\nThrough understanding the data, it will also give us some insights on whether there is any more “gold” we can squeeze out from data through feature engineering.\r\nIt will also provide us some clues whether the variable is a good predictor even without needing one to fit the model.\r\nThe conventional method of data wrangling is to perform data cleaning and transformation in Excel.\r\n\r\n\r\n\r\nPhoto by Mika Baumeister on Unsplash\r\nHowever, this method is not very sustainable/ideal due to the following reasons:  - Excel has memory limitation and the software would keep on crashing as the data size increases  - Become a hurdle when we want to join different tables together or perform a transformation on the data  - More challenging to extract insights from the unstructured data (eg. perform text mining) \r\nIntroduction to Tidyverse\r\nWhen my professor first introduced this R package to me, I was amazed by what we could perform by using this package. And in fact, the more I use it, the more I love it.\r\ntidyverse is a collection of R packages designed by RStudio.\r\n\r\n\r\n\r\nThe very awesome thing about tidyverse is all of the packages are following the tidy data concept.\r\nSo what the heck is tidy data?\r\nTidy data is a concept on how to store/structure the data/results introduced by Hadley Wickham.\r\n\r\n\r\n\r\nArtwork on Tidy Data by Allison Horst\r\nBelow are the definition of tidy data:  - Each variable must have its column  - Each observation must have its row  - Each value must have its cell \r\n\r\n\r\n\r\nExtraction from R for Data Science book\r\nWhy is tidy data awesome?\r\nThis is as simple as the time spent transforming the dataset/output is now lesser.\r\nWhy less time is required?\r\nThe functions are aligned on the input data formats they require.\r\nOften this allows us to ‘join’ the function together (also often known as “pipe” in tidyverse context. Don’t worry about the meaning of “pipe” for now, will be covering what I mean by “pipe” later in this post).\r\nTherefore, as a user, I find that the learning curve of using tidyverse to perform data wrangling is less steep than other programming languages. This has effectively allowed me to spend more time on the analysis itself.\r\n\r\n\r\n\r\nArtwork by Allison Horst\r\nEnough talking, let’s get our hands dirty.\r\nIllustration\r\nI will be using the motor insurance dataset taken from CASdataset as a demonstration. In this illustration, I am interested to find out how the different profiles of the insured affected the total premium.\r\nPreparation of the “equipment”\r\nOver here, I call the necessary R packages by using a for-loop function.\r\n\r\n\r\npackage <- c('CASdatasets', 'tidyverse', 'skimr', 'funModeling', 'ggplot2', 'plotly', 'ggstatsplot')\r\n\r\nfor (p in package){\r\n  if(!require (p, character.only = TRUE)){\r\n    install(p)\r\n  }\r\n  library(p, character.only = TRUE)\r\n}\r\n\r\n\r\n\r\nThe code chunk will first check whether the relevant packages are installed in the machine.\r\nIf it is not installed, it will first install the required package.\r\nAfter that, the relevant packages will be attached to the environment.\r\nCalling the “beast”\r\nAfter loading the relevant packages, I will indicate to R the dataset I want through data function as there are many datasets within CASdatasets package. I have renamed the dataset to df to shorten the name after attaching the dataset into the environment.\r\n\r\n\r\ndata(fremotor1prem0304a)\r\ndf <- fremotor1prem0304a\r\n\r\n\r\n\r\nNow, tame the “beast”\r\nConventionally, summary function is used to check the quality of the data.\r\n\r\n\r\nsummary(df)\r\n\r\n\r\n    IDpol                Year         DrivAge      DrivGender\r\n Length:51949       Min.   :2003   Min.   :18.00   F:17794   \r\n Class :character   1st Qu.:2003   1st Qu.:31.00   M:34155   \r\n Mode  :character   Median :2003   Median :38.00             \r\n                    Mean   :2003   Mean   :39.84             \r\n                    3rd Qu.:2004   3rd Qu.:47.00             \r\n                    Max.   :2004   Max.   :97.00             \r\n                                                             \r\n    MaritalStatus     BonusMalus       LicenceNb    \r\n Cohabiting:10680   Min.   : 50.00   Min.   :1.000  \r\n Divorced  :  189   1st Qu.: 50.00   1st Qu.:1.000  \r\n Married   : 3554   Median : 57.00   Median :2.000  \r\n Single    : 2037   Mean   : 62.98   Mean   :1.885  \r\n Widowed   :  541   3rd Qu.: 72.00   3rd Qu.:2.000  \r\n NA's      :34948   Max.   :156.00   Max.   :7.000  \r\n                                                    \r\n        PayFreq                  JobCode          VehAge      \r\n Annual     :17579   Private employee: 9147   Min.   : 0.000  \r\n Half-yearly:28978   Public employee : 5045   1st Qu.: 4.000  \r\n Monthly    : 1486   Retiree         : 1035   Median : 7.000  \r\n Quarterly  : 3906   Other           :  856   Mean   : 7.525  \r\n                     Craftsman       :  637   3rd Qu.:10.000  \r\n                     (Other)         :  281   Max.   :89.000  \r\n                     NA's            :34948                   \r\n        VehClass        VehPower        VehGas     \r\n Cheapest   :17894   P10    :9148   Diesel :20615  \r\n Cheaper    :15176   P12    :8351   Regular:31334  \r\n Cheap      : 9027   P11    :8320                  \r\n Medium low : 5566   P9     :6671                  \r\n Medium     : 2021   P13    :6605                  \r\n Medium high: 1385   P8     :5188                  \r\n (Other)    :  880   (Other):7666                  \r\n                   VehUsage                           Garage     \r\n Private+trip to office:50435   Closed collective parking: 9820  \r\n Professional          : 1089   Closed zbox              :26318  \r\n Professional run      :  425   Opened collective parking: 8620  \r\n                                Street                   : 7191  \r\n                                                                 \r\n                                                                 \r\n                                                                 \r\n      Area                Region      Channel   Marketing \r\n A5     :15108   Center      :27486   A:30220   M1:26318  \r\n A3     :12696   Headquarters: 9788   B: 6111   M2: 8620  \r\n A2     : 7414   Paris area  : 7860   L:15618   M3: 9820  \r\n A7     : 6879   South West  : 6815             M4: 7191  \r\n A4     : 3779                                            \r\n A9     : 3397                                            \r\n (Other): 2676                                            \r\n PremWindscreen     PremDamAll         PremFire         PremAcc1    \r\n Min.   :  0.00   Min.   :   0.00   Min.   : 0.000   Min.   : 0.00  \r\n 1st Qu.: 13.00   1st Qu.:   0.00   1st Qu.: 0.000   1st Qu.: 0.00  \r\n Median : 22.00   Median :   0.00   Median : 4.000   Median : 0.00  \r\n Mean   : 25.78   Mean   :  83.06   Mean   : 4.421   Mean   :12.94  \r\n 3rd Qu.: 35.00   3rd Qu.: 144.00   3rd Qu.: 7.000   3rd Qu.:32.00  \r\n Max.   :264.00   Max.   :1429.00   Max.   :50.000   Max.   :77.00  \r\n                                                                    \r\n    PremAcc2       PremLegal        PremTPLM         PremTPLV     \r\n Min.   :  0.0   Min.   : 0.00   Min.   :  38.9   Min.   : 0.000  \r\n 1st Qu.:  0.0   1st Qu.: 8.00   1st Qu.: 102.6   1st Qu.: 5.000  \r\n Median :  0.0   Median :10.00   Median : 141.5   Median : 7.000  \r\n Mean   : 15.4   Mean   :10.43   Mean   : 167.7   Mean   : 8.571  \r\n 3rd Qu.: 45.0   3rd Qu.:12.00   3rd Qu.: 204.8   3rd Qu.:11.000  \r\n Max.   :198.0   Max.   :50.00   Max.   :1432.7   Max.   :68.000  \r\n                                                                  \r\n    PremServ        PremTheft         PremTot      \r\n Min.   :  0.00   Min.   :  0.00   Min.   :  91.0  \r\n 1st Qu.: 51.00   1st Qu.:  0.00   1st Qu.: 269.7  \r\n Median : 53.00   Median : 38.00   Median : 381.4  \r\n Mean   : 53.77   Mean   : 46.58   Mean   : 428.7  \r\n 3rd Qu.: 57.00   3rd Qu.: 68.00   3rd Qu.: 530.4  \r\n Max.   :237.00   Max.   :642.00   Max.   :3163.3  \r\n                                                   \r\n\r\nHowever, the data quality result shown under summary function is often not sufficient. Instead of just looking at the quantile, it’s also important to check other information, such as there is any missing value, what is the distribution of the variable look like, what is coefficient variation, and so on.\r\nBelow are three examples of modern R functions to check the data quality:\r\nskim\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n30\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nfactor\r\n13\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nIDpol\r\n0\r\n1\r\n5\r\n13\r\n0\r\n32117\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nDrivGender\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 34155, F: 17794\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nPayFreq\r\n0\r\n1.00\r\nFALSE\r\n4\r\nHal: 28978, Ann: 17579, Qua: 3906, Mon: 1486\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\nVehClass\r\n0\r\n1.00\r\nFALSE\r\n9\r\nChe: 17894, Che: 15176, Che: 9027, Med: 5566\r\nVehPower\r\n0\r\n1.00\r\nFALSE\r\n15\r\nP10: 9148, P12: 8351, P11: 8320, P9: 6671\r\nVehGas\r\n0\r\n1.00\r\nFALSE\r\n2\r\nReg: 31334, Die: 20615\r\nVehUsage\r\n0\r\n1.00\r\nFALSE\r\n3\r\nPri: 50435, Pro: 1089, Pro: 425\r\nGarage\r\n0\r\n1.00\r\nFALSE\r\n4\r\nClo: 26318, Clo: 9820, Ope: 8620, Str: 7191\r\nArea\r\n0\r\n1.00\r\nFALSE\r\n10\r\nA5: 15108, A3: 12696, A2: 7414, A7: 6879\r\nRegion\r\n0\r\n1.00\r\nFALSE\r\n4\r\nCen: 27486, Hea: 9788, Par: 7860, Sou: 6815\r\nChannel\r\n0\r\n1.00\r\nFALSE\r\n3\r\nA: 30220, L: 15618, B: 6111\r\nMarketing\r\n0\r\n1.00\r\nFALSE\r\n4\r\nM1: 26318, M3: 9820, M2: 8620, M4: 7191\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nprofiling_num\r\n\r\n\r\nprofiling_num(df)\r\n\r\n\r\n         variable        mean     std_dev variation_coef   p_01\r\n1            Year 2003.381759   0.4858226   0.0002425013 2003.0\r\n2         DrivAge   39.835146  11.8652020   0.2978576243   21.0\r\n3      BonusMalus   62.983330  15.3002405   0.2429252393   50.0\r\n4       LicenceNb    1.884733   0.6664325   0.3535951452    1.0\r\n5          VehAge    7.525477   4.7817572   0.6354091975    0.0\r\n6  PremWindscreen   25.779053  20.4951432   0.7950308962    0.0\r\n7      PremDamAll   83.055285 105.7145996   1.2728220676    0.0\r\n8        PremFire    4.421317   4.4846822   1.0143317368    0.0\r\n9        PremAcc1   12.942001  16.7326001   1.2928912922    0.0\r\n10       PremAcc2   15.400219  23.8030155   1.5456283309    0.0\r\n11      PremLegal   10.426399   3.9025924   0.3742991579    4.0\r\n12       PremTPLM  167.737897  96.9041558   0.5777117614   51.8\r\n13       PremTPLV    8.570521   5.2053077   0.6073501958    0.0\r\n14       PremServ   53.770852   5.1332147   0.0954646327   42.0\r\n15      PremTheft   46.584169  48.7219715   1.0458911785    0.0\r\n16        PremTot  428.687713 224.5838408   0.5238868156  129.1\r\n     p_05   p_25   p_50   p_75   p_95     p_99  skewness  kurtosis\r\n1  2003.0 2003.0 2003.0 2004.0 2004.0 2004.000 0.4867707  1.236946\r\n2    24.0   31.0   38.0   47.0   63.0   73.000 0.8090189  3.339693\r\n3    50.0   50.0   57.0   72.0   93.0  106.000 1.1457567  3.756333\r\n4     1.0    1.0    2.0    2.0    3.0    4.000 0.9380943  5.925007\r\n5     1.0    4.0    7.0   10.0   15.0   21.000 1.5128291 15.246811\r\n6     0.0   13.0   22.0   35.0   65.0   92.000 1.5801010  8.575197\r\n7     0.0    0.0    0.0  144.0  283.0  420.000 1.5981568  7.474360\r\n8     0.0    0.0    4.0    7.0   13.0   20.000 1.7369281  9.079772\r\n9     0.0    0.0    0.0   32.0   39.0   44.000 0.5764758  1.461663\r\n10    0.0    0.0    0.0   45.0   57.0   64.000 0.9548824  2.088081\r\n11    5.0    8.0   10.0   12.0   17.0   23.000 1.3283374  6.830506\r\n12   68.0  102.6  141.5  204.8  352.0  512.100 2.1710206 11.739705\r\n13    3.0    5.0    7.0   11.0   18.0   26.000 1.9178789 10.133990\r\n14   46.0   51.0   53.0   57.0   62.0   68.000 1.8713567 51.639566\r\n15    0.0    0.0   38.0   68.0  136.0  216.000 1.9999025 11.271121\r\n16  169.8  269.7  381.4  530.4  852.3 1187.504 1.7016729  8.533766\r\n     iqr          range_98         range_80\r\n1    1.0      [2003, 2004]     [2003, 2004]\r\n2   16.0          [21, 73]         [27, 56]\r\n3   22.0         [50, 106]         [50, 85]\r\n4    1.0            [1, 4]           [1, 3]\r\n5    6.0           [0, 21]          [2, 14]\r\n6   22.0           [0, 92]          [0, 52]\r\n7  144.0          [0, 420]         [0, 221]\r\n8    7.0           [0, 20]          [0, 10]\r\n9   32.0           [0, 44]          [0, 36]\r\n10  45.0           [0, 64]          [0, 53]\r\n11   4.0           [4, 23]          [6, 15]\r\n12 102.2     [51.8, 512.1]    [78.8, 290.8]\r\n13   6.0           [0, 26]          [4, 15]\r\n14   6.0          [42, 68]         [48, 60]\r\n15  68.0          [0, 216]         [0, 106]\r\n16 260.7 [129.1, 1187.504] [199.48, 713.12]\r\n\r\nstatus\r\n\r\n\r\nstatus(df)\r\n\r\n\r\n         variable q_zeros      p_zeros  q_na      p_na q_inf p_inf\r\n1           IDpol       0 0.0000000000     0 0.0000000     0     0\r\n2            Year       0 0.0000000000     0 0.0000000     0     0\r\n3         DrivAge       0 0.0000000000     0 0.0000000     0     0\r\n4      DrivGender       0 0.0000000000     0 0.0000000     0     0\r\n5   MaritalStatus       0 0.0000000000 34948 0.6727367     0     0\r\n6      BonusMalus       0 0.0000000000     0 0.0000000     0     0\r\n7       LicenceNb       0 0.0000000000     0 0.0000000     0     0\r\n8         PayFreq       0 0.0000000000     0 0.0000000     0     0\r\n9         JobCode       0 0.0000000000 34948 0.6727367     0     0\r\n10         VehAge    1712 0.0329553986     0 0.0000000     0     0\r\n11       VehClass       0 0.0000000000     0 0.0000000     0     0\r\n12       VehPower       0 0.0000000000     0 0.0000000     0     0\r\n13         VehGas       0 0.0000000000     0 0.0000000     0     0\r\n14       VehUsage       0 0.0000000000     0 0.0000000     0     0\r\n15         Garage       0 0.0000000000     0 0.0000000     0     0\r\n16           Area       0 0.0000000000     0 0.0000000     0     0\r\n17         Region       0 0.0000000000     0 0.0000000     0     0\r\n18        Channel       0 0.0000000000     0 0.0000000     0     0\r\n19      Marketing       0 0.0000000000     0 0.0000000     0     0\r\n20 PremWindscreen    7153 0.1376927371     0 0.0000000     0     0\r\n21     PremDamAll   26087 0.5021655855     0 0.0000000     0     0\r\n22       PremFire   14450 0.2781574236     0 0.0000000     0     0\r\n23       PremAcc1   32183 0.6195114439     0 0.0000000     0     0\r\n24       PremAcc2   36379 0.7002829698     0 0.0000000     0     0\r\n25      PremLegal       2 0.0000384993     0 0.0000000     0     0\r\n26       PremTPLM       0 0.0000000000     0 0.0000000     0     0\r\n27       PremTPLV    1353 0.0260447747     0 0.0000000     0     0\r\n28       PremServ       2 0.0000384993     0 0.0000000     0     0\r\n29      PremTheft   14654 0.2820843520     0 0.0000000     0     0\r\n30        PremTot       0 0.0000000000     0 0.0000000     0     0\r\n        type unique\r\n1  character  32117\r\n2    numeric      2\r\n3    numeric     72\r\n4     factor      2\r\n5     factor      5\r\n6    numeric     68\r\n7    numeric      7\r\n8     factor      4\r\n9     factor      7\r\n10   numeric     49\r\n11    factor      9\r\n12    factor     15\r\n13    factor      2\r\n14    factor      3\r\n15    factor      4\r\n16    factor     10\r\n17    factor      4\r\n18    factor      3\r\n19    factor      4\r\n20   numeric    177\r\n21   numeric    612\r\n22   numeric     49\r\n23   numeric     45\r\n24   numeric     63\r\n25   numeric     43\r\n26   numeric   1097\r\n27   numeric     58\r\n28   numeric     70\r\n29   numeric    377\r\n30   numeric   9140\r\n\r\nThese functions show more info than just quantile. They also show info such as:  - Number of unique categories under categorical variables  - The proportion of missing values in the data  - Standard deviation of the numeric variables \r\nBelow are some insights we could draw from the functions above: \r\nExcessive missing value (i.e. more than half of the values are missing) in MaritalStatus & JobCode. More than 65% of the data from these two columns have missing values. So, it doesn’t like the variables will yield meaningful results if they are used to build machine learning models \r\n\r\n\r\ndf %>% dplyr::select(MaritalStatus, JobCode) %>% skim()\r\n\r\n\r\nTable 2: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n2\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\n\r\nAbout 15 unique categories under VehPower. Might not ideal to keep so many unique categories within the variables as it might create noise when fitting machine learning models \r\n\r\n\r\ndf %>% group_by(VehPower) %>% tally()\r\n\r\n\r\n# A tibble: 15 x 2\r\n   VehPower     n\r\n   <fct>    <int>\r\n 1 P10       9148\r\n 2 P11       8320\r\n 3 P12       8351\r\n 4 P13       6605\r\n 5 P14       3773\r\n 6 P15       1510\r\n 7 P16        720\r\n 8 P17         58\r\n 9 P2          15\r\n10 P4          46\r\n11 P5         597\r\n12 P6           2\r\n13 P7         945\r\n14 P8        5188\r\n15 P9        6671\r\n\r\nAlso, note that the count of some categories is relatively low. Perhaps it is better to group these ‘low count categories’ since they are unlikely to have a significant impact on the predicted results.\r\nMaximum BonusMalus can go up to 156. According to the data dictionary, <100 means bonus, >100 means malus. Hence, this is okay. \r\n\r\n\r\ndf %>% dplyr::select(BonusMalus) %>% skim()\r\n\r\n\r\nTable 3: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.3\r\n50\r\n50\r\n57\r\n72\r\n156\r\n▇▂▁▁▁\r\n\r\nInterestingly, the premium for different benefits can range very widely. This is probably due to the difference in the respective burning cost under each benefit. \r\n\r\n\r\ndf %>% dplyr::select(starts_with(\"Prem\")) %>% skim()\r\n\r\n\r\nTable 4: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nI prefer skim function as its output is in a nice & neat format. It also provides most of the crucial info one is looking for in checking the data quality.\r\nIf I am only interested in checking the data quality for the numeric variables, this can be found by piping the variables together.\r\nFirst, I have specified the dataset. Next, I use select_if function to select all the numeric columns before using skim function to skim through the dataset.\r\n\r\n\r\ndf %>% \r\n  select_if(is.numeric) %>%\r\n  skim()\r\n\r\n\r\nTable 5: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nThe piping method is made possible because the functions are following the tidy data concept, which makes it easier to “play” with the data.\r\nOften, visualizing data might provide us unexpected insights. The data can “tell” us underlying/hidden stories by leveraging on the power of data visualization.\r\nFor example, I would like to find out the scatterplot between PremTot against the selected numeric variables.\r\n\r\n\r\nnum_list <- c(\"Year\", \"DrivAge\", \"BonusMalus\", \"LicenceNb\", \"VehAge\")\r\n\r\n\r\nfor (i in num_list){\r\n  print(ggplot(df, aes(x = get(i), y = log(PremTot))) +\r\n      geom_point() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nThe graphs show that the data type for Year and LicenceNb is incorrect. They are supposed to read as factors, instead of numeric variables.\r\nTo fix this, I will use mutate and factor function to transform the data into the correct data type.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(Year = factor(Year),\r\n         LicenceNb = factor(LicenceNb))\r\n\r\n\r\n\r\nAlternatively, sometimes we just want to find out the graphs of a list of variables, instead of a small subset of variables. By typing down all the variables names can be a hassle and prone to human error.\r\nThis is where the different R functions come to the “rescue”.\r\nFor example, I am interested to find out the boxplot of all the factor variables against PremTot. The code chunk below has shown how we could pipe the different functions together to select all the factor columns and extract out the column names as a list.\r\n\r\n\r\ncat_list <- df_1 %>%\r\n  select_if(is.factor) %>%\r\n  names()\r\n\r\n\r\n\r\nSubsequently, I will use for loop to plot the necessary graphs. I will explain the awesome-ness of ggplot function in my future post.\r\n\r\n\r\nfor (i in cat_list){\r\n  print(ggplot(df_1, aes(x = get(i), y = log(PremTot))) +\r\n      geom_boxplot() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nBelow are some of the insights drawn from the graphs above (not limited to):  - Average PremTot for Retailer is higher than the rest  - Average PremTot also varies significantly across different VehPower  - Somehow the premium for a diesel car is higher than a regular car  - PremTot for professional & professional run is higher than private+trip to office \r\nConclusion\r\nOkay, that’s all the sharing for this post!\r\nI have shown the awesome-ness of tidyverse through this post. There are many more functions within tidyverse universe, which are not covered in this post. Do check out their website for many more awesome functions!\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-22-data-wrangling/image/data_cowboy.png",
    "last_modified": "2021-02-15T01:43:26+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-my-data-science-journey/",
    "title": "The Unconventional Path",
    "description": "Journey of Joining the \"Dark\" Side",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nPhoto taken in Taipei\r\nDuring a stormy night in 2018, it was raining cats and dogs outside. Suddenly there was thunder and lightning. Jeng jeng jeng, I suddenly decided to take up a master degree to study data science.\r\nNah, I always wish it was that simple to make such a big decision in life.\r\nIt took me about 5 years to decide that I would go back to university for a data science degree.\r\nIt took me 2.5 years to have the courage in taking the leap to go for a change in job function for the 1st time. \r\nWas I worried or scared when I made these choice? Oh absolutely.\r\nUnconventional Path - Work After working on actuarial pricing for more than 4 years, I have asked for a change in job function, which I have chickened out twice times after the requests. There were a lot of ‘what-if’ in my head.\r\n“What if I don’t like the new function?”\r\n“What if I struggle to understand the new concepts?”\r\n“What if I screwed up?”\r\n“What if I get along with them?”\r\nHowever, on the third time, I came to the conclusion that if I don’t take the leap of faith, I would never step out from my comfort zone.\r\nMeanwhile, unfortunately sometimes life is not a bed of roses. Even if it is a bed of roses, it is also a bed of roses with full of thorns.\r\nInitially, things were not as smooth as what I wanted. Team mates left one after another one and things were pretty messy. It was through these tough times I discovered my strength, how I could play my strength and make a difference.\r\nFast forward to 1.5 years later, I was being offered to change my job scope again at my work. However, this change in job scope was even more drastic. I was being offered to change my job scope from life insurance to general insurance.\r\nAm I scared? Still as worried as usual.\r\nHowever, I decided that I would regret if I did not take the leap of faith again. My industry friends were shocked, they even checked with me to verify the news. Deep down I knew that worry does not solve anything. So, I worked my a** hard to put in the extra effort to understand the fundamental concepts for general insurance.\r\nWithin less than 3 months after joining the team, I was asked to cover group insurance while my colleague was on maternity leave. At the moment, I was worried as my main expertise was never in short term insurance business and I was expecting to guide one of the junior on this piece of work. Despite of the hiccups at the beginning, things turned out better than what I initially imagined.\r\nThese past success have nevertheless given me more confidence in myself.\r\nUnconventional Path - Data Science Journey I first discovered my interest for data science back in my undergraduate studies. I still remember it was generalized linear model project on Titanic dataset that sparked my interest.\r\n\r\n\r\n\r\nScreenshot of my GLM report\r\nAfter I started working, I never forget my interest in this area. However, as life goes on, I was not sure this data science was for me since actuarial science and data science are still different (in terms of concepts and work) although they are somewhat similar.\r\nUntil few years ago, I was once exposed a bit on data science at work again. That re-kindled my interest for data science. I knew that it is not going to work if I rely others for my learning, eg. waiting to learn from work, waiting for others to teach me and so on. Eventually, I decided to go back into university to study data science.\r\nThroughout these processes, I have encountered many “Why” from myself and others.\r\n“Why would you still want to study, given that you have been studying for your actuarial exams after graduating from university?”\r\n“Why are you still studying this when you hold an actuarial science degree?”\r\n“Why are you studying when your working hours can be quite bad?”\r\n“Why are you spending so much money on getting another degree? Are you sure you can earn back the money?”\r\n“Just why??”\r\nI knew it is going to tough and tiring to work full time + study part time, although I was complaining that I did not its going to be this tiring.\r\nLooking back now, did I regret anything? Absolutely no.\r\nIn fact, I met some of my “gui ren” (ie. people who are great help to one’s life) through my this journey. Prof Kam Tin Seong has spent so much time with me guiding me through my capstone project, explaining the concepts of data science techniques (eg. how to do proper clustering, considerations when building a model), making me working very hard for the weekly data visualization makeover and many more. Another of my gui ren has to be Prof Wong Yuet Nan. The valuable advice and experience he shared with me has built up my knowledge and confidence.\r\nWas I stressed during these process? Super duper stressed. I was so so stressed out until there were days my brain couldn’t think or function.\r\nDeep down I knew what are opportunity cost and price I am paying, but I am willing to going through the pain. Sleeping at 2am almost every day, occasionally running back to work after class ended at 10pm and fell asleep at random places due to lack of enough sleep are really nothing when I compared to these valuable experience and confidence I have gained.\r\nWould I trade these experience for anything? Definitely no.\r\nTakeaway Do I have the certainty of what is going to happen next when I was making the above decisions? Nope, often I am not sure how things will work out.\r\nDo I want to have the certainty in these situations? Oh absolutely yes. However, I knew life does not work this way. I would miss out so much fun if I demand on the certainty.\r\nIt was taking these seemed “unconventional paths” that provided me the opportunities to discover amazing things. Just like the the photo shown below, it was taken with no filter during my Japan trip. Instead of walking along the main road like what other tourists were doing, I followed another path that is less taken and discovered this awesome view.\r\n\r\n\r\n\r\nPhoto taken in Hakone, Japan\r\nNevertheless, this is my story of unconventional path.\r\nLife is short, so go & live a life with no regret! :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-my-data-science-journey/image/journey.jpg",
    "last_modified": "2021-01-20T23:23:41+08:00",
    "input_file": {}
  }
]
