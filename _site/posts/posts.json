[
  {
    "path": "posts/2021-05-23-model-stacking/",
    "title": "Model Stacking",
    "description": "When the sum of all component is greater than individual component",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-05-22",
    "categories": [
      "machineLearning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Ensemble method?\r\nSo, how does stacking work?\r\nWhy is this method popular?\r\nStacks R Package\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Splitting\r\nDefine Recipe\r\nFitting Individual Machine Learning Models\r\nStack the Models!\r\nCompare the Model Performance\r\nPerformance Metric of Individual Models\r\nPerformance Metric of Stack Model\r\nSummary\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Crissy Jarvis on Unsplash\r\nIn this series of modeling, I will be exploring model stacking.\r\nIt has gained popularity especially in data science competition as such method could further improve the accuracy of the machine learning models.\r\nBefore jumping straight into model stacking, let‚Äôs understand what is ensemble method.\r\nWhat is Ensemble method?\r\n\r\n\r\n\r\nExtracted from vas3k blog (vas3k)\r\nEnsemble method is one of the machine learning method.\r\n(Geron 2019) described that ensemble method is similar to aggregate the answer of a complex question from thousands of random people. This anchored on the idea of wisdom of the crowd. Often this approach would produce a better answer than an answer from an expert.\r\nSuch method can be either applied on the same machine learning algorithms or aggregate across different machine learning algorithms.\r\nAs the graph shown above, following are the three common ensemble methods:\r\nBagging: Different models are trained by using different dataset randomly drawn with replacement from the training dataset. The final prediction is the average of predictions from all the fitted models. Random forest is one of the algorithm that leverages on bagging to improve the model accuracy.\r\nBoosting: The idea is to a combination of weak learners could form a strong learner. XGBoost is one commonly used model algorithm that uses boosting method.\r\nStacking: As the name suggested, we ‚Äústack‚Äù the models on one another and create a new model.\r\nIn this post, I will be exploring model stacking and mainly covering the following topics:\r\nHow does model stacking work?\r\nHow to implement model stacking in R?\r\nSo, how does stacking work?\r\nIf we were to visualize the steps for model stacking, this would be how it would look like:\r\n\r\n\r\n\r\nFollowing are the steps to perform model stacking:\r\nFirst, fit different individual machine learning models\r\nNext, use target variable from the training dataset and the predictions from the different machine learning models as input to fit a model\r\nThe conventional way to stack the model is to use linear model. Although in theory we could use other machine learning algorithm to stack the models, this would increase the complexity of the final model, making it even harder to interpret.\r\nWhy is this method popular?\r\n(Funda Gunes and Tan 2017) discussed in their paper that ensemble method enables the users to average out the noise from the diverse models and thereby enhance the generalizable signal.\r\nTherefore, such method has proven to increase model accuracy in Kaggle competitions. Often, even a slight uplift in improvement could affect our model performance rankings in the Kaggle competitions.\r\nWhile this method does not guarantee an improvement in the accuracy, it is worthwhile to fit such models to attempt to improve the accuracy of the predictions.\r\nHowever, by stacking the different models together, this has increased the complexity of the models, making it even harder to understand on how the model reaches the decisions.\r\nThe time required for model fitting would be longer as well since we would need to fit the base model before perform model stacking.\r\nStacks R Package\r\nOkay, let‚Äôs perform some model stacking!\r\n\r\n\r\n\r\nPhoto by La-Rel Easter on Unsplash\r\nIn this demonstration, I will be using a package from tidymodels that allows the users to perform model stacks with just a few lines of codes. This package is called stacks.\r\n\r\n\r\n\r\nstacks package\r\nAnd yes, this package is it conforms to the tidy data concept. It is also designed to work together with the various packages under tidymodels.\r\nWHAT A BIG RELIEF!\r\n\r\n\r\n\r\nPhoto by Andrea Piacquadio from Pexels\r\nThis would make it easier for one to use perform model stacking.\r\nDemonstration\r\nSetup the environment\r\nBefore I start the demonstration, I will setup the environment by calling the necessary packages.\r\n\r\n\r\npackages <- c(\"tidyverse\", \"tidymodels\", \"stacks\")\r\n\r\nfor (p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nI will also define the common parameters that will be used in the analysis below.\r\n\r\n\r\n# Define the random seeds for reproducibility\r\nset.seed(1234)\r\n\r\n# Proportion between training and testing dataset\r\nprop_train_test <- 0.6\r\n\r\n# Define the model performance metrics we would like to output later\r\nmodel_metrics <- metric_set(rmse, rsq)\r\n\r\n# The number of grid to be used in the analysis later\r\ngrid_num <- 5\r\n\r\n\r\n\r\nTo stack model, we need to include following as well. According to the documentation page, this is to ensure the predictions and necessary info are output in the necessary format required by stacks package later.\r\n\r\n\r\nctrl_grid <- control_stack_grid()\r\nctrl_res <- control_stack_resamples()\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nFor this demonstration, I will be using a dataset from Actuarial Loss Prediction Kaggle Competition.\r\nAlso, this sharing will be focusing on the model stacking, instead of end to end process. Hence, I will import the data I have previously cleaned.\r\nRefer to my EDA code file on my Github page for the steps taken to clean the data.\r\n\r\n\r\ndf <- read_csv(\"data/data_eda_actLoss_3.csv\") %>%\r\n  dplyr::select(-c(ClaimNumber,\r\n                   num_week_paid_ult,\r\n                   InitialIncurredClaimCost,\r\n                   UltimateIncurredClaimCost)) %>%\r\n  dplyr::select(-starts_with(c(\"acc\", \"report\"))) %>%\r\n  filter(Gender != \"U\") %>%\r\n  drop_na() %>%\r\n  sample_frac(0.2)\r\n\r\n\r\n\r\nWhile importing the data, I have also:\r\nDrop ‚ÄòClaimNumber,‚Äô ‚Äònum_week_paid_ult,‚Äô ‚ÄòInitialIncurredClaim Cost,‚Äô ‚ÄòUltimateIncurredClaimCost‚Äô & column headers that starts with ‚Äòacc‚Äô & ‚Äòreport‚Äô since I am not using them later\r\nFilter out gender ‚ÄòU‚Äô as according to data dictionary, indicator ‚ÄòU‚Äô means missing values\r\nDrop the data with missing values\r\nSample a subset of the data to use for the analysis, otherwise it would take forever to run on my laptop üò¢\r\nData Splitting\r\nNext, I will split the dataset into training and testing dataset.\r\n\r\n\r\n# Split dataset\r\ndf_split <- initial_split(df,\r\n                            prop = prop_train_test,\r\n                            strata = init_ult_diff)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nThen, I will prepare the dataset for the k-fold validation later.\r\n\r\n\r\n# Cross validation\r\ndf_folds <- vfold_cv(df_train, strata = init_ult_diff)\r\n\r\n\r\n\r\nDefine Recipe\r\nIn this section, I will define the general recipe for the different machine learning models.\r\n\r\n\r\ngen_recipe <- recipe(init_ult_diff ~ ., data = df_train) %>%\r\n  update_role(c(DateTimeOfAccident, DateReported, ClaimDescription), new_role = \"id\") %>% # update the roles of original date variables to \"id\"\r\n  prep()\r\n\r\n\r\n\r\nFitting Individual Machine Learning Models\r\nBefore fitting a stack model, let‚Äôs fit individual models first. To shorten the running time, I will skip the parameters tuning in this post.\r\nAlso as the focus of this post is on model stacking, hence I won‚Äôt be going through the model building steps below. For more information on the model building steps, you can refer to my previous presentations on the model building or the documentation page of tidymodels.\r\nNevertheless, let‚Äôs start building the different machine learning models!\r\nRandom Forest\r\n\r\n\r\nranger_spec <- \r\n  rand_forest() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"ranger\", importance = \"impurity\")\r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(gen_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\nset.seed(51107)\r\nranger_tune <-\r\n  tune_grid(ranger_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\nxgboost_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal())\r\n\r\nxgboost_spec <- \r\n  boost_tree() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"xgboost\") \r\n\r\nxgboost_workflow <- \r\n  workflow() %>% \r\n  \r\n  add_recipe(xgboost_recipe) %>% \r\n  add_model(xgboost_spec) \r\n\r\nset.seed(12071)\r\nxgboost_tune <-\r\n  tune_grid(xgboost_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\nearth_recipe <- gen_recipe %>% \r\n  step_novel(all_nominal(), -all_outcomes()) %>% \r\n  step_dummy(all_nominal(), -all_outcomes()) %>% \r\n  step_zv(all_predictors()) \r\n\r\nearth_spec <- \r\n  mars() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"earth\") \r\n\r\nearth_workflow <- \r\n  workflow() %>% \r\n  add_recipe(earth_recipe) %>% \r\n  add_model(earth_spec) \r\n\r\nearth_grid <- tidyr::crossing(num_terms = 2 * (1:6), prod_degree = 1:2) \r\n\r\nearth_tune <- \r\n  tune_grid(earth_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nStack the Models!\r\nOkay, we have fitted all the individual models. I will move on and fit a stack model.\r\nTo do so, I will use stack function to create an empty stack model. Then the individual models are being added to empty stack model so that we can fit a stack model later.\r\n\r\n\r\nstack_model <-\r\n  stacks() %>%\r\n  add_candidates(ranger_tune) %>%\r\n  add_candidates(xgboost_tune) %>%\r\n  add_candidates(earth_tune)\r\n\r\n\r\n\r\nWe can also call the stack model object we have just created to review the details of the models will be used for stacking.\r\n\r\n\r\nstack_model\r\n\r\n\r\n# A data stack with 3 model definitions and 3 candidate members:\r\n#   ranger_tune: 1 model configuration\r\n#   xgboost_tune: 1 model configuration\r\n#   earth_tune: 1 model configuration\r\n# Outcome: init_ult_diff (numeric)\r\n\r\nNext, blend_predictions function is used to determine the weights of the individual models.\r\n\r\n\r\nstack_model_pred <-\r\n  stack_model %>%\r\n  blend_predictions()\r\n\r\n\r\n\r\nAlthough we could use different machine learning algorithms to stack the models, stacks seems to only support regularized linear model to stack the model at the moment according to the documentation page.\r\nWorry not!\r\nIt‚Äôs not the end of the world. With a few simple tricks, we would be able to get around this.\r\nIf we want to use other machine learning algorithms to stack the models, we could merge the predicted values into a tibble table and find the optimal weight that would give us the best model performance. In other words, we will just perform the usual model fitting on the predicted values from the different individual models against the target value.\r\nInstead of use the stack model to perform the predictions, the new predicted value from the stack model will be calculated by taking the weighted average of the predicted values from the selected individual models from the model stacking steps.\r\nTherefore, if we were to visualize the steps, it would look something like this:\r\n\r\n\r\n\r\nAnyway, back to our analysis!\r\nTo see the weights of the models, we can call the object we have created in the previous step.\r\n\r\n\r\nstack_model_pred\r\n\r\n\r\n# A tibble: 3 x 3\r\n  member           type        weight\r\n  <chr>            <chr>        <dbl>\r\n1 ranger_tune_1_1  rand_forest  0.472\r\n2 xgboost_tune_1_1 boost_tree   0.420\r\n3 earth_tune_1_1   mars         0.211\r\n\r\nAs the results shown above, random forest, xgboost and mars are used to create the stacked model. The relevant weights of each model also can be found in the table above.\r\nWe can also use the autoplot function to visualize the weight for different models.\r\n\r\n\r\nautoplot(stack_model_pred, type = \"weights\")\r\n\r\n\r\n\r\n\r\nOnce the weights of the different models are determined, I will prepare the fitted stack model by using fit_members function.\r\n\r\n\r\nstack_model_fit <- stack_model_pred %>%\r\n  fit_members()\r\n\r\nstack_model_fit\r\n\r\n\r\n# A tibble: 3 x 3\r\n  member           type        weight\r\n  <chr>            <chr>        <dbl>\r\n1 ranger_tune_1_1  rand_forest  0.472\r\n2 xgboost_tune_1_1 boost_tree   0.420\r\n3 earth_tune_1_1   mars         0.211\r\n\r\nDone!\r\nWith the few simple steps above, we have successfully created a stack model.\r\n\r\n\r\n\r\nPhoto by Andrea Piacquadio from Pexels\r\nCompare the Model Performance\r\nTo check whether the stack model is better than the individual models, I will calculate the model performance for both individual models and stack model.\r\nPerformance Metric of Individual Models\r\nTo calculate the model performance from the different models, I will do the following:\r\nFirst, finalize the workflow by selecting the best parameters & perform last_fit on the model\r\nThen, collect the model predictions by using collect_predictions\r\nLastly, calculate the model performance, create a column named ‚Äòmodel‚Äô and pivot the columns\r\nRandom Forest\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nranger_fit <- ranger_workflow %>%\r\n  finalize_workflow(select_best(ranger_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nranger_pred <- ranger_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nranger_metric <- model_metrics(ranger_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"ranger\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nxgboost_fit <- xgboost_workflow %>%\r\n  finalize_workflow(select_best(xgboost_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nxgboost_pred <- xgboost_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nxgboost_metric <- model_metrics(xgboost_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"xgboost\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nearth_fit <- earth_workflow %>%\r\n  finalize_workflow(select_best(earth_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nearth_pred <- earth_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nearth_metric <- model_metrics(earth_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"earth\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nPerformance Metric of Stack Model\r\nNext, I will calculate the model performance for the stack model.\r\n\r\n\r\nstack_metric <- predict(stack_model_fit, df_test) %>%\r\n  bind_cols(init_ult_diff = df_test$init_ult_diff) %>%\r\n  model_metrics(truth = init_ult_diff, estimate = .pred) %>%\r\n  mutate(model = \"stack\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nSummary\r\nNow I will combine all the results in a tibble table so that its easier to compare the performance metrics from different models.\r\n\r\n\r\ntibble() %>%\r\n  bind_rows(stack_metric) %>%\r\n  bind_rows(ranger_metric) %>%\r\n  bind_rows(xgboost_metric) %>%\r\n  bind_rows(earth_metric)\r\n\r\n\r\n# A tibble: 4 x 4\r\n  .estimator model    rmse   rsq\r\n  <chr>      <chr>   <dbl> <dbl>\r\n1 standard   stack   1592. 0.412\r\n2 standard   ranger  1618. 0.397\r\n3 standard   xgboost 1623. 0.390\r\n4 standard   earth   1708. 0.338\r\n\r\nAs shown above, both RMSE and R square did improve after we stacked the models, showing that the combination of the different models could outperform the individual models.\r\nConclusion\r\nThat‚Äôs all for the day!\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page if you want to find out more on model stacking.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nFunda Gunes, Russ Wolfinger, and Pei-Yi Tan, eds. 2017. ‚ÄúStacked Ensemble Models for Improved Prediction Accuracy.‚Äù http://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf.\r\n\r\n\r\nGeron, Aurelien, ed. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow. O‚ÄôReilly Media.\r\n\r\n\r\nvas3k, ed. ‚ÄúMachine Learning for Everyone - in Simple Words. With Real-World Examples. Yes, Again.‚Äù https://vas3k.com/blog/machine_learning/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-23-model-stacking/image/cup_stack.jpg",
    "last_modified": "2021-05-22T17:42:18+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-22-data-wrangling/",
    "title": "Data Wrangling - The Quest to Tame The 'Beast'",
    "description": "Rawwwwwwwwww!",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "dataWrangling"
    ],
    "contents": "\r\n\r\n\r\n\r\nArtwork on Data Wrangling by Allison Horst\r\nAs mentioned in my last post, it has been a long journey to learn data science pretty much from scratch. I intend to share some of my learning to help others to start their data science journey as well.\r\nTherefore, for my second post, I will share about data wrangling, which is often the essential task before building any machine learning models. Hence this post will serve as a building block of my future machine learning posts.\r\nAlso, I intend to share some of the modern data science R package and how these packages could benefit some of the analysis tasks.\r\nBackground\r\nIn R for Data Science book, Wickham & Grolemund mentioned that a typical data science project would look as following:\r\n\r\n\r\n\r\nExtracted from R for Data Science\r\nAfter importing the data, the very first step usually involves some levels of data wrangling. This is probably one of the most important steps as without a good dataset, the analysis is unlikely to be meaningful.\r\nGolden rule ‚Äì> Garbage in, garbage out\r\nA good data exploratory will help to answer the following questions (not limited to):  - Does this dataset answer the business questions (eg. does the dataset have the necessary info to predict which customers are likely to purchase in the next promotion?)?  - Are there any issues that exist in the dataset (eg. missing value, excessive categories, value range vary widely)?  - Any data transformation required? As some of the machine learning models can accept certain data types. \r\nThrough understanding the data, it will also give us some insights on whether there is any more ‚Äúgold‚Äù we can squeeze out from data through feature engineering.\r\nIt will also provide us some clues whether the variable is a good predictor even without needing one to fit the model.\r\nThe conventional method of data wrangling is to perform data cleaning and transformation in Excel.\r\n\r\n\r\n\r\nPhoto by Mika Baumeister on Unsplash\r\nHowever, this method is not very sustainable/ideal due to the following reasons:  - Excel has memory limitation and the software would keep on crashing as the data size increases  - Become a hurdle when we want to join different tables together or perform a transformation on the data  - More challenging to extract insights from the unstructured data (eg. perform text mining) \r\nIntroduction to Tidyverse\r\nWhen my professor first introduced this R package to me, I was amazed by what we could perform by using this package. And in fact, the more I use it, the more I love it.\r\ntidyverse is a collection of R packages designed by RStudio.\r\n\r\n\r\n\r\nThe very awesome thing about tidyverse is all of the packages are following the tidy data concept.\r\nSo what the heck is tidy data?\r\nTidy data is a concept on how to store/structure the data/results introduced by Hadley Wickham.\r\n\r\n\r\n\r\nArtwork on Tidy Data by Allison Horst\r\nBelow are the definition of tidy data:  - Each variable must have its column  - Each observation must have its row  - Each value must have its cell \r\n\r\n\r\n\r\nExtraction from R for Data Science book\r\nWhy is tidy data awesome?\r\nThis is as simple as the time spent transforming the dataset/output is now lesser.\r\nWhy less time is required?\r\nThe functions are aligned on the input data formats they require.\r\nOften this allows us to ‚Äòjoin‚Äô the function together (also often known as ‚Äúpipe‚Äù in tidyverse context. Don‚Äôt worry about the meaning of ‚Äúpipe‚Äù for now, will be covering what I mean by ‚Äúpipe‚Äù later in this post).\r\nTherefore, as a user, I find that the learning curve of using tidyverse to perform data wrangling is less steep than other programming languages. This has effectively allowed me to spend more time on the analysis itself.\r\n\r\n\r\n\r\nArtwork by Allison Horst\r\nEnough talking, let‚Äôs get our hands dirty.\r\nIllustration\r\nI will be using the motor insurance dataset taken from CASdataset as a demonstration. In this illustration, I am interested to find out how the different profiles of the insured affected the total premium.\r\nPreparation of the ‚Äúequipment‚Äù\r\nOver here, I call the necessary R packages by using a for-loop function.\r\n\r\n\r\npackage <- c('CASdatasets', 'tidyverse', 'skimr', 'funModeling', 'ggplot2', 'plotly', 'ggstatsplot')\r\n\r\nfor (p in package){\r\n  if(!require (p, character.only = TRUE)){\r\n    install(p)\r\n  }\r\n  library(p, character.only = TRUE)\r\n}\r\n\r\n\r\n\r\nThe code chunk will first check whether the relevant packages are installed in the machine.\r\nIf it is not installed, it will first install the required package.\r\nAfter that, the relevant packages will be attached to the environment.\r\nCalling the ‚Äúbeast‚Äù\r\nAfter loading the relevant packages, I will indicate to R the dataset I want through data function as there are many datasets within CASdatasets package. I have renamed the dataset to df to shorten the name after attaching the dataset into the environment.\r\n\r\n\r\ndata(fremotor1prem0304a)\r\ndf <- fremotor1prem0304a\r\n\r\n\r\n\r\nNow, tame the ‚Äúbeast‚Äù\r\nConventionally, summary function is used to check the quality of the data.\r\n\r\n\r\nsummary(df)\r\n\r\n\r\n    IDpol                Year         DrivAge      DrivGender\r\n Length:51949       Min.   :2003   Min.   :18.00   F:17794   \r\n Class :character   1st Qu.:2003   1st Qu.:31.00   M:34155   \r\n Mode  :character   Median :2003   Median :38.00             \r\n                    Mean   :2003   Mean   :39.84             \r\n                    3rd Qu.:2004   3rd Qu.:47.00             \r\n                    Max.   :2004   Max.   :97.00             \r\n                                                             \r\n    MaritalStatus     BonusMalus       LicenceNb    \r\n Cohabiting:10680   Min.   : 50.00   Min.   :1.000  \r\n Divorced  :  189   1st Qu.: 50.00   1st Qu.:1.000  \r\n Married   : 3554   Median : 57.00   Median :2.000  \r\n Single    : 2037   Mean   : 62.98   Mean   :1.885  \r\n Widowed   :  541   3rd Qu.: 72.00   3rd Qu.:2.000  \r\n NA's      :34948   Max.   :156.00   Max.   :7.000  \r\n                                                    \r\n        PayFreq                  JobCode          VehAge      \r\n Annual     :17579   Private employee: 9147   Min.   : 0.000  \r\n Half-yearly:28978   Public employee : 5045   1st Qu.: 4.000  \r\n Monthly    : 1486   Retiree         : 1035   Median : 7.000  \r\n Quarterly  : 3906   Other           :  856   Mean   : 7.525  \r\n                     Craftsman       :  637   3rd Qu.:10.000  \r\n                     (Other)         :  281   Max.   :89.000  \r\n                     NA's            :34948                   \r\n        VehClass        VehPower        VehGas     \r\n Cheapest   :17894   P10    :9148   Diesel :20615  \r\n Cheaper    :15176   P12    :8351   Regular:31334  \r\n Cheap      : 9027   P11    :8320                  \r\n Medium low : 5566   P9     :6671                  \r\n Medium     : 2021   P13    :6605                  \r\n Medium high: 1385   P8     :5188                  \r\n (Other)    :  880   (Other):7666                  \r\n                   VehUsage                           Garage     \r\n Private+trip to office:50435   Closed collective parking: 9820  \r\n Professional          : 1089   Closed zbox              :26318  \r\n Professional run      :  425   Opened collective parking: 8620  \r\n                                Street                   : 7191  \r\n                                                                 \r\n                                                                 \r\n                                                                 \r\n      Area                Region      Channel   Marketing \r\n A5     :15108   Center      :27486   A:30220   M1:26318  \r\n A3     :12696   Headquarters: 9788   B: 6111   M2: 8620  \r\n A2     : 7414   Paris area  : 7860   L:15618   M3: 9820  \r\n A7     : 6879   South West  : 6815             M4: 7191  \r\n A4     : 3779                                            \r\n A9     : 3397                                            \r\n (Other): 2676                                            \r\n PremWindscreen     PremDamAll         PremFire         PremAcc1    \r\n Min.   :  0.00   Min.   :   0.00   Min.   : 0.000   Min.   : 0.00  \r\n 1st Qu.: 13.00   1st Qu.:   0.00   1st Qu.: 0.000   1st Qu.: 0.00  \r\n Median : 22.00   Median :   0.00   Median : 4.000   Median : 0.00  \r\n Mean   : 25.78   Mean   :  83.06   Mean   : 4.421   Mean   :12.94  \r\n 3rd Qu.: 35.00   3rd Qu.: 144.00   3rd Qu.: 7.000   3rd Qu.:32.00  \r\n Max.   :264.00   Max.   :1429.00   Max.   :50.000   Max.   :77.00  \r\n                                                                    \r\n    PremAcc2       PremLegal        PremTPLM         PremTPLV     \r\n Min.   :  0.0   Min.   : 0.00   Min.   :  38.9   Min.   : 0.000  \r\n 1st Qu.:  0.0   1st Qu.: 8.00   1st Qu.: 102.6   1st Qu.: 5.000  \r\n Median :  0.0   Median :10.00   Median : 141.5   Median : 7.000  \r\n Mean   : 15.4   Mean   :10.43   Mean   : 167.7   Mean   : 8.571  \r\n 3rd Qu.: 45.0   3rd Qu.:12.00   3rd Qu.: 204.8   3rd Qu.:11.000  \r\n Max.   :198.0   Max.   :50.00   Max.   :1432.7   Max.   :68.000  \r\n                                                                  \r\n    PremServ        PremTheft         PremTot      \r\n Min.   :  0.00   Min.   :  0.00   Min.   :  91.0  \r\n 1st Qu.: 51.00   1st Qu.:  0.00   1st Qu.: 269.7  \r\n Median : 53.00   Median : 38.00   Median : 381.4  \r\n Mean   : 53.77   Mean   : 46.58   Mean   : 428.7  \r\n 3rd Qu.: 57.00   3rd Qu.: 68.00   3rd Qu.: 530.4  \r\n Max.   :237.00   Max.   :642.00   Max.   :3163.3  \r\n                                                   \r\n\r\nHowever, the data quality result shown under summary function is often not sufficient. Instead of just looking at the quantile, it‚Äôs also important to check other information, such as there is any missing value, what is the distribution of the variable look like, what is coefficient variation, and so on.\r\nBelow are three examples of modern R functions to check the data quality:\r\nskim\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n30\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nfactor\r\n13\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nIDpol\r\n0\r\n1\r\n5\r\n13\r\n0\r\n32117\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nDrivGender\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 34155, F: 17794\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nPayFreq\r\n0\r\n1.00\r\nFALSE\r\n4\r\nHal: 28978, Ann: 17579, Qua: 3906, Mon: 1486\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\nVehClass\r\n0\r\n1.00\r\nFALSE\r\n9\r\nChe: 17894, Che: 15176, Che: 9027, Med: 5566\r\nVehPower\r\n0\r\n1.00\r\nFALSE\r\n15\r\nP10: 9148, P12: 8351, P11: 8320, P9: 6671\r\nVehGas\r\n0\r\n1.00\r\nFALSE\r\n2\r\nReg: 31334, Die: 20615\r\nVehUsage\r\n0\r\n1.00\r\nFALSE\r\n3\r\nPri: 50435, Pro: 1089, Pro: 425\r\nGarage\r\n0\r\n1.00\r\nFALSE\r\n4\r\nClo: 26318, Clo: 9820, Ope: 8620, Str: 7191\r\nArea\r\n0\r\n1.00\r\nFALSE\r\n10\r\nA5: 15108, A3: 12696, A2: 7414, A7: 6879\r\nRegion\r\n0\r\n1.00\r\nFALSE\r\n4\r\nCen: 27486, Hea: 9788, Par: 7860, Sou: 6815\r\nChannel\r\n0\r\n1.00\r\nFALSE\r\n3\r\nA: 30220, L: 15618, B: 6111\r\nMarketing\r\n0\r\n1.00\r\nFALSE\r\n4\r\nM1: 26318, M3: 9820, M2: 8620, M4: 7191\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÅ\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n\r\nprofiling_num\r\n\r\n\r\nprofiling_num(df)\r\n\r\n\r\n         variable        mean     std_dev variation_coef   p_01\r\n1            Year 2003.381759   0.4858226   0.0002425013 2003.0\r\n2         DrivAge   39.835146  11.8652020   0.2978576243   21.0\r\n3      BonusMalus   62.983330  15.3002405   0.2429252393   50.0\r\n4       LicenceNb    1.884733   0.6664325   0.3535951452    1.0\r\n5          VehAge    7.525477   4.7817572   0.6354091975    0.0\r\n6  PremWindscreen   25.779053  20.4951432   0.7950308962    0.0\r\n7      PremDamAll   83.055285 105.7145996   1.2728220676    0.0\r\n8        PremFire    4.421317   4.4846822   1.0143317368    0.0\r\n9        PremAcc1   12.942001  16.7326001   1.2928912922    0.0\r\n10       PremAcc2   15.400219  23.8030155   1.5456283309    0.0\r\n11      PremLegal   10.426399   3.9025924   0.3742991579    4.0\r\n12       PremTPLM  167.737897  96.9041558   0.5777117614   51.8\r\n13       PremTPLV    8.570521   5.2053077   0.6073501958    0.0\r\n14       PremServ   53.770852   5.1332147   0.0954646327   42.0\r\n15      PremTheft   46.584169  48.7219715   1.0458911785    0.0\r\n16        PremTot  428.687713 224.5838408   0.5238868156  129.1\r\n     p_05   p_25   p_50   p_75   p_95     p_99  skewness  kurtosis\r\n1  2003.0 2003.0 2003.0 2004.0 2004.0 2004.000 0.4867707  1.236946\r\n2    24.0   31.0   38.0   47.0   63.0   73.000 0.8090189  3.339693\r\n3    50.0   50.0   57.0   72.0   93.0  106.000 1.1457567  3.756333\r\n4     1.0    1.0    2.0    2.0    3.0    4.000 0.9380943  5.925007\r\n5     1.0    4.0    7.0   10.0   15.0   21.000 1.5128291 15.246811\r\n6     0.0   13.0   22.0   35.0   65.0   92.000 1.5801010  8.575197\r\n7     0.0    0.0    0.0  144.0  283.0  420.000 1.5981568  7.474360\r\n8     0.0    0.0    4.0    7.0   13.0   20.000 1.7369281  9.079772\r\n9     0.0    0.0    0.0   32.0   39.0   44.000 0.5764758  1.461663\r\n10    0.0    0.0    0.0   45.0   57.0   64.000 0.9548824  2.088081\r\n11    5.0    8.0   10.0   12.0   17.0   23.000 1.3283374  6.830506\r\n12   68.0  102.6  141.5  204.8  352.0  512.100 2.1710206 11.739705\r\n13    3.0    5.0    7.0   11.0   18.0   26.000 1.9178789 10.133990\r\n14   46.0   51.0   53.0   57.0   62.0   68.000 1.8713567 51.639566\r\n15    0.0    0.0   38.0   68.0  136.0  216.000 1.9999025 11.271121\r\n16  169.8  269.7  381.4  530.4  852.3 1187.504 1.7016729  8.533766\r\n     iqr          range_98         range_80\r\n1    1.0      [2003, 2004]     [2003, 2004]\r\n2   16.0          [21, 73]         [27, 56]\r\n3   22.0         [50, 106]         [50, 85]\r\n4    1.0            [1, 4]           [1, 3]\r\n5    6.0           [0, 21]          [2, 14]\r\n6   22.0           [0, 92]          [0, 52]\r\n7  144.0          [0, 420]         [0, 221]\r\n8    7.0           [0, 20]          [0, 10]\r\n9   32.0           [0, 44]          [0, 36]\r\n10  45.0           [0, 64]          [0, 53]\r\n11   4.0           [4, 23]          [6, 15]\r\n12 102.2     [51.8, 512.1]    [78.8, 290.8]\r\n13   6.0           [0, 26]          [4, 15]\r\n14   6.0          [42, 68]         [48, 60]\r\n15  68.0          [0, 216]         [0, 106]\r\n16 260.7 [129.1, 1187.504] [199.48, 713.12]\r\n\r\nstatus\r\n\r\n\r\nstatus(df)\r\n\r\n\r\n         variable q_zeros      p_zeros  q_na      p_na q_inf p_inf\r\n1           IDpol       0 0.0000000000     0 0.0000000     0     0\r\n2            Year       0 0.0000000000     0 0.0000000     0     0\r\n3         DrivAge       0 0.0000000000     0 0.0000000     0     0\r\n4      DrivGender       0 0.0000000000     0 0.0000000     0     0\r\n5   MaritalStatus       0 0.0000000000 34948 0.6727367     0     0\r\n6      BonusMalus       0 0.0000000000     0 0.0000000     0     0\r\n7       LicenceNb       0 0.0000000000     0 0.0000000     0     0\r\n8         PayFreq       0 0.0000000000     0 0.0000000     0     0\r\n9         JobCode       0 0.0000000000 34948 0.6727367     0     0\r\n10         VehAge    1712 0.0329553986     0 0.0000000     0     0\r\n11       VehClass       0 0.0000000000     0 0.0000000     0     0\r\n12       VehPower       0 0.0000000000     0 0.0000000     0     0\r\n13         VehGas       0 0.0000000000     0 0.0000000     0     0\r\n14       VehUsage       0 0.0000000000     0 0.0000000     0     0\r\n15         Garage       0 0.0000000000     0 0.0000000     0     0\r\n16           Area       0 0.0000000000     0 0.0000000     0     0\r\n17         Region       0 0.0000000000     0 0.0000000     0     0\r\n18        Channel       0 0.0000000000     0 0.0000000     0     0\r\n19      Marketing       0 0.0000000000     0 0.0000000     0     0\r\n20 PremWindscreen    7153 0.1376927371     0 0.0000000     0     0\r\n21     PremDamAll   26087 0.5021655855     0 0.0000000     0     0\r\n22       PremFire   14450 0.2781574236     0 0.0000000     0     0\r\n23       PremAcc1   32183 0.6195114439     0 0.0000000     0     0\r\n24       PremAcc2   36379 0.7002829698     0 0.0000000     0     0\r\n25      PremLegal       2 0.0000384993     0 0.0000000     0     0\r\n26       PremTPLM       0 0.0000000000     0 0.0000000     0     0\r\n27       PremTPLV    1353 0.0260447747     0 0.0000000     0     0\r\n28       PremServ       2 0.0000384993     0 0.0000000     0     0\r\n29      PremTheft   14654 0.2820843520     0 0.0000000     0     0\r\n30        PremTot       0 0.0000000000     0 0.0000000     0     0\r\n        type unique\r\n1  character  32117\r\n2    numeric      2\r\n3    numeric     72\r\n4     factor      2\r\n5     factor      5\r\n6    numeric     68\r\n7    numeric      7\r\n8     factor      4\r\n9     factor      7\r\n10   numeric     49\r\n11    factor      9\r\n12    factor     15\r\n13    factor      2\r\n14    factor      3\r\n15    factor      4\r\n16    factor     10\r\n17    factor      4\r\n18    factor      3\r\n19    factor      4\r\n20   numeric    177\r\n21   numeric    612\r\n22   numeric     49\r\n23   numeric     45\r\n24   numeric     63\r\n25   numeric     43\r\n26   numeric   1097\r\n27   numeric     58\r\n28   numeric     70\r\n29   numeric    377\r\n30   numeric   9140\r\n\r\nThese functions show more info than just quantile. They also show info such as:  - Number of unique categories under categorical variables  - The proportion of missing values in the data  - Standard deviation of the numeric variables \r\nBelow are some insights we could draw from the functions above: \r\nExcessive missing value (i.e.¬†more than half of the values are missing) in MaritalStatus & JobCode. More than 65% of the data from these two columns have missing values. So, it doesn‚Äôt like the variables will yield meaningful results if they are used to build machine learning models \r\n\r\n\r\ndf %>% dplyr::select(MaritalStatus, JobCode) %>% skim()\r\n\r\n\r\nTable 2: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n2\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\n\r\nAbout 15 unique categories under VehPower. Might not ideal to keep so many unique categories within the variables as it might create noise when fitting machine learning models \r\n\r\n\r\ndf %>% group_by(VehPower) %>% tally()\r\n\r\n\r\n# A tibble: 15 x 2\r\n   VehPower     n\r\n   <fct>    <int>\r\n 1 P10       9148\r\n 2 P11       8320\r\n 3 P12       8351\r\n 4 P13       6605\r\n 5 P14       3773\r\n 6 P15       1510\r\n 7 P16        720\r\n 8 P17         58\r\n 9 P2          15\r\n10 P4          46\r\n11 P5         597\r\n12 P6           2\r\n13 P7         945\r\n14 P8        5188\r\n15 P9        6671\r\n\r\nAlso, note that the count of some categories is relatively low. Perhaps it is better to group these ‚Äòlow count categories‚Äô since they are unlikely to have a significant impact on the predicted results.\r\nMaximum BonusMalus can go up to 156. According to the data dictionary, <100 means bonus, >100 means malus. Hence, this is okay. \r\n\r\n\r\ndf %>% dplyr::select(BonusMalus) %>% skim()\r\n\r\n\r\nTable 3: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.3\r\n50\r\n50\r\n57\r\n72\r\n156\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\n\r\nInterestingly, the premium for different benefits can range very widely. This is probably due to the difference in the respective burning cost under each benefit. \r\n\r\n\r\ndf %>% dplyr::select(starts_with(\"Prem\")) %>% skim()\r\n\r\n\r\nTable 4: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÅ\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n\r\nI prefer skim function as its output is in a nice & neat format. It also provides most of the crucial info one is looking for in checking the data quality.\r\nIf I am only interested in checking the data quality for the numeric variables, this can be found by piping the variables together.\r\nFirst, I have specified the dataset. Next, I use select_if function to select all the numeric columns before using skim function to skim through the dataset.\r\n\r\n\r\ndf %>% \r\n  select_if(is.numeric) %>%\r\n  skim()\r\n\r\n\r\nTable 5: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÅ\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\r\n\r\nThe piping method is made possible because the functions are following the tidy data concept, which makes it easier to ‚Äúplay‚Äù with the data.\r\nOften, visualizing data might provide us unexpected insights. The data can ‚Äútell‚Äù us underlying/hidden stories by leveraging on the power of data visualization.\r\nFor example, I would like to find out the scatterplot between PremTot against the selected numeric variables.\r\n\r\n\r\nnum_list <- c(\"Year\", \"DrivAge\", \"BonusMalus\", \"LicenceNb\", \"VehAge\")\r\n\r\n\r\nfor (i in num_list){\r\n  print(ggplot(df, aes(x = get(i), y = log(PremTot))) +\r\n      geom_point() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nThe graphs show that the data type for Year and LicenceNb is incorrect. They are supposed to read as factors, instead of numeric variables.\r\nTo fix this, I will use mutate and factor function to transform the data into the correct data type.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(Year = factor(Year),\r\n         LicenceNb = factor(LicenceNb))\r\n\r\n\r\n\r\nAlternatively, sometimes we just want to find out the graphs of a list of variables, instead of a small subset of variables. By typing down all the variables names can be a hassle and prone to human error.\r\nThis is where the different R functions come to the ‚Äúrescue‚Äù.\r\nFor example, I am interested to find out the boxplot of all the factor variables against PremTot. The code chunk below has shown how we could pipe the different functions together to select all the factor columns and extract out the column names as a list.\r\n\r\n\r\ncat_list <- df_1 %>%\r\n  select_if(is.factor) %>%\r\n  names()\r\n\r\n\r\n\r\nSubsequently, I will use for loop to plot the necessary graphs. I will explain the awesome-ness of ggplot function in my future post.\r\n\r\n\r\nfor (i in cat_list){\r\n  print(ggplot(df_1, aes(x = get(i), y = log(PremTot))) +\r\n      geom_boxplot() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nBelow are some of the insights drawn from the graphs above (not limited to):  - Average PremTot for Retailer is higher than the rest  - Average PremTot also varies significantly across different VehPower  - Somehow the premium for a diesel car is higher than a regular car  - PremTot for professional & professional run is higher than private+trip to office \r\nConclusion\r\nOkay, that‚Äôs all the sharing for this post!\r\nI have shown the awesome-ness of tidyverse through this post. There are many more functions within tidyverse universe, which are not covered in this post. Do check out their website for many more awesome functions!\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-22-data-wrangling/image/data_cowboy.png",
    "last_modified": "2021-02-15T01:43:26+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-my-data-science-journey/",
    "title": "The Unconventional Path",
    "description": "Journey of Joining the \"Dark\" Side",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nPhoto taken in Taipei\r\nDuring a stormy night in 2018, it was raining cats and dogs outside. Suddenly there was thunder and lightning. Jeng jeng jeng, I suddenly decided to take up a master degree to study data science.\r\nNah, I always wish it was that simple to make such a big decision in life.\r\nIt took me about 5 years to decide that I would go back to university for a data science degree.\r\nIt took me 2.5 years to have the courage in taking the leap to go for a change in job function for the 1st time. \r\nWas I worried or scared when I made these choice? Oh absolutely.\r\nUnconventional Path - Work After working on actuarial pricing for more than 4 years, I have asked for a change in job function, which I have chickened out twice times after the requests. There were a lot of ‚Äòwhat-if‚Äô in my head.\r\n‚ÄúWhat if I don‚Äôt like the new function?‚Äù\r\n‚ÄúWhat if I struggle to understand the new concepts?‚Äù\r\n‚ÄúWhat if I screwed up?‚Äù\r\n‚ÄúWhat if I get along with them?‚Äù\r\nHowever, on the third time, I came to the conclusion that if I don‚Äôt take the leap of faith, I would never step out from my comfort zone.\r\nMeanwhile, unfortunately sometimes life is not a bed of roses. Even if it is a bed of roses, it is also a bed of roses with full of thorns.\r\nInitially, things were not as smooth as what I wanted. Team mates left one after another one and things were pretty messy. It was through these tough times I discovered my strength, how I could play my strength and make a difference.\r\nFast forward to 1.5 years later, I was being offered to change my job scope again at my work. However, this change in job scope was even more drastic. I was being offered to change my job scope from life insurance to general insurance.\r\nAm I scared? Still as worried as usual.\r\nHowever, I decided that I would regret if I did not take the leap of faith again. My industry friends were shocked, they even checked with me to verify the news. Deep down I knew that worry does not solve anything. So, I worked my a** hard to put in the extra effort to understand the fundamental concepts for general insurance.\r\nWithin less than 3 months after joining the team, I was asked to cover group insurance while my colleague was on maternity leave. At the moment, I was worried as my main expertise was never in short term insurance business and I was expecting to guide one of the junior on this piece of work. Despite of the hiccups at the beginning, things turned out better than what I initially imagined.\r\nThese past success have nevertheless given me more confidence in myself.\r\nUnconventional Path - Data Science Journey I first discovered my interest for data science back in my undergraduate studies. I still remember it was generalized linear model project on Titanic dataset that sparked my interest.\r\n\r\n\r\n\r\nScreenshot of my GLM report\r\nAfter I started working, I never forget my interest in this area. However, as life goes on, I was not sure this data science was for me since actuarial science and data science are still different (in terms of concepts and work) although they are somewhat similar.\r\nUntil few years ago, I was once exposed a bit on data science at work again. That re-kindled my interest for data science. I knew that it is not going to work if I rely others for my learning, eg. waiting to learn from work, waiting for others to teach me and so on. Eventually, I decided to go back into university to study data science.\r\nThroughout these processes, I have encountered many ‚ÄúWhy‚Äù from myself and others.\r\n‚ÄúWhy would you still want to study, given that you have been studying for your actuarial exams after graduating from university?‚Äù\r\n‚ÄúWhy are you still studying this when you hold an actuarial science degree?‚Äù\r\n‚ÄúWhy are you studying when your working hours can be quite bad?‚Äù\r\n‚ÄúWhy are you spending so much money on getting another degree? Are you sure you can earn back the money?‚Äù\r\n‚ÄúJust why??‚Äù\r\nI knew it is going to tough and tiring to work full time + study part time, although I was complaining that I did not its going to be this tiring.\r\nLooking back now, did I regret anything? Absolutely no.\r\nIn fact, I met some of my ‚Äúgui ren‚Äù (ie. people who are great help to one‚Äôs life) through my this journey. Prof Kam Tin Seong has spent so much time with me guiding me through my capstone project, explaining the concepts of data science techniques (eg. how to do proper clustering, considerations when building a model), making me working very hard for the weekly data visualization makeover and many more. Another of my gui ren has to be Prof Wong Yuet Nan. The valuable advice and experience he shared with me has built up my knowledge and confidence.\r\nWas I stressed during these process? Super duper stressed. I was so so stressed out until there were days my brain couldn‚Äôt think or function.\r\nDeep down I knew what are opportunity cost and price I am paying, but I am willing to going through the pain. Sleeping at 2am almost every day, occasionally running back to work after class ended at 10pm and fell asleep at random places due to lack of enough sleep are really nothing when I compared to these valuable experience and confidence I have gained.\r\nWould I trade these experience for anything? Definitely no.\r\nTakeaway Do I have the certainty of what is going to happen next when I was making the above decisions? Nope, often I am not sure how things will work out.\r\nDo I want to have the certainty in these situations? Oh absolutely yes. However, I knew life does not work this way. I would miss out so much fun if I demand on the certainty.\r\nIt was taking these seemed ‚Äúunconventional paths‚Äù that provided me the opportunities to discover amazing things. Just like the the photo shown below, it was taken with no filter during my Japan trip. Instead of walking along the main road like what other tourists were doing, I followed another path that is less taken and discovered this awesome view.\r\n\r\n\r\n\r\nPhoto taken in Hakone, Japan\r\nNevertheless, this is my story of unconventional path.\r\nLife is short, so go & live a life with no regret! :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-my-data-science-journey/image/journey.jpg",
    "last_modified": "2021-01-20T23:23:41+08:00",
    "input_file": {}
  }
]
