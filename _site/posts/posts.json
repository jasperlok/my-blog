[
  {
    "path": "posts/2021-10-17-clustering/",
    "title": "Clustering",
    "description": "Mirror, mirror on the wall, which data are similar to one another?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-10-17",
    "categories": [
      "Machine Learning",
      "Unsupervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nUnsupervised Learning\r\nApplication of Clustering\r\nConsideration when Performing Clustering\r\nPurpose of Clustering Analysis\r\nAppropriateness of Features in Separating Policies into Clusters\r\nNumber of Data Points within Each Cluster\r\nCluster Selection\r\nLimitation of Selected Clustering Algorithm\r\n\r\nConclusion\r\n\r\nDuring the Q&A session in my last August sharing, there were some questions on different unsupervised learning methods. It has triggered my thought to do a sharing on unsupervised learning methods, in particular clustering.\r\nIt is one of the basic topics one would learn when he/she learns data science.\r\nInstead of having the explanations and demonstration in the same post, I have decided to split up the post into two so that each post is not too long to read.\r\nIn this post, I will focus on what one should know before perform clustering analysis.\r\nThe demonstration on clustering will be included in the next post.\r\nNevertheless, let’s understand a bit more about clustering.\r\n\r\n\r\n\r\nPhoto by v2osk on Unsplash\r\nUnsupervised Learning\r\nIn general, the classical machine learning models can be further split into two broad categories, which are supervised and unsupervised learning methods.\r\nUnsupervised learning is a machine learning method to discover hidden patterns or data groupings without human intervention (IBM Cloud Education 2020).\r\nIn other words, this method aims to model the underlying structure, affinities, or distribution in the data in order to learn more about its intrinsic characteristics (Ivo 2018).\r\nThe unsupervised learning method comprises clustering, association, and dimension reduction as shown in the graph below.\r\n\r\n\r\n\r\nExtracted from vas3k blog\r\nBelow are the descriptions of each unsupervised learning methods:\r\n\r\n\r\nTechniques\r\n\r\n\r\nDescriptions\r\n\r\n\r\nClustering\r\n\r\n\r\nClustering is one of the basic algorithms any beginners would learn when they started their data science journey.\r\nThe clustering algorithms include k-means, hierarchical clustering, latent class analysis, and so on. It is commonly used in many applications as shown following:\r\nFind the different customer segments so that one could derive a more targeted strategy to boost the sales\r\nGroup the customers into different groups based on their behaviors\r\n\r\n\r\nAssociation\r\n\r\n\r\nApriori is one of the common algorithms used to mine the association rules between the different items. In other words, this method allows one to discover how the items are “associated” with one another.\r\n\r\nFollowing are some of the common examples of how the association rules are used:\r\nProduct bundling by bundling products with higher profit margin and closely associated to boost the sales\r\nPerform cross-selling and upselling products\r\nArranging the closely associated products together to increase the sales\r\n\r\nThe more advanced technique includes taking time into considerations while performing association algorithms. This would allow one to understand whether there is a change in the customer purchasing behaviors.\r\n\r\n\r\nDimension Reduction\r\n\r\n\r\nAs the data increases, often one would face the “curve of dimensionality.” This is an issue while building machine learning models as the data points in high-dimensional space are sparse, resulting in a less desirable machine learning model.\r\nTherefore, the common way to resolve this is to perform principal component analysis (PCA). The key idea of PCA is the algorithm attempts to find a low-dimensional representation of a dataset that contains as much as possible of the variation (James et al. 2021).\r\nHowever, as the principal components derived from the algorithm are not directly observable from the dataset, hence it is often more challenging to explain or understand the results.\r\n\r\n\r\nIn this series, I will focus on clustering and leave the other two unsupervised learning methods for future posts.\r\nApplication of Clustering\r\n(Google Developers 2020) has listed a list of common applications for clustering, which includes:\r\nmarket segmentation\r\nsocial network analysis\r\nsearch result grouping\r\nmedical imaging\r\nimage segmentation\r\nanomaly detection\r\nConsideration when Performing Clustering\r\nWhile clustering could be a useful tool to discover insights within the dataset, below are some of the important considerations when performing clustering:\r\nPurpose of Clustering Analysis\r\n\r\n\r\n\r\nPhoto by Mark Fletcher-Brown on Unsplash\r\n(Logan and Pentimonti 2016) recommended the users to think about how the clustering results will be used. For example, what is the business problem we be solving through the clustering results? The required dataset could defer depending on the business problem we are solving.\r\nNevertheless, the answer to this question will guide users in selecting the appropriate variables for the clustering analysis, deciding the appropriate clustering algorithms, and so on.\r\nAppropriateness of Features in Separating Policies into Clusters\r\nAs the name unsupervised machine learning suggested, the clustering algorithm has no mechanism for differentiating relevant or irrelevant variables. Also, the clustering results can be very dependent on the variables included in the analysis (Kam 2019). Therefore, to obtain meaningful clustering results, the selected variables should be able to reflect the inherent differences between the different clusters.\r\nFor example, if we know there are the fundamental structural difference between the different types of customers/businesses, the relevant parameters should be used in clustering so that the clusters are less likely to overlap with one another.\r\n\r\n\r\n\r\nPhoto by David Rotimi on Unsplash\r\nOf course, the variables that could effectively separate the dataset may not be very apparent sometimes. Often, it requires some level of understanding of the underlying data or even the context of the data.\r\nAs the old saying goes, garbage in garbage out. Without thinking through and selecting the appropriate variables, the clustering results may not be meaningful.\r\nBesides, the number of variables used in clustering is also quite important. The rule of thumb recommended by Goodman is to have at least three variables in the clustering analysis (Goodman 1974). The clustering results may not be meaningful if insufficient variables are used in clustering.\r\nMeanwhile, having too many variables may introduce too much noise, resulting in less meaningful clustering results. Therefore, only the variables that could help us in separating the different characteristics of the model points should be included in the analysis.\r\nNumber of Data Points within Each Cluster\r\n\r\n\r\n\r\nPhoto by Monstera from Pexels\r\nThe general rule of thumb is the number of data points within each cluster should not be less than 5. The idea is that the cluster is unlikely to be meaningful when the number of data points contained in the cluster is relatively low.\r\nFor example, imagine the company is leveraging the clustering results to set up the different operations to serve different groups based on the values customers bring in. The company would be better off choosing a lower number of clusters that contain a decent number of data points within the clusters even though the higher number of clusters might be producing a better clustering result. The cost of having an additional headcount or separate process for a cluster without a decent size is likely to be hard to justify.\r\nCluster Selection\r\n\r\n\r\n\r\nPhoto by Edu Grande on Unsplash\r\nFor clustering, the number of clusters is a parameter that needs to be defined upfront before clustering analysis. However, this poses a challenge - how do we know what is the optimal number of clusters for the given dataset?\r\nThe common approach is to run the algorithm under a different number of clusters and use some measurements to compare the clustering results, providing us with a more objective method to determine the optimal number of clusters.\r\nFor example, the elbow curve is one of the common approaches to find the optimal number of clusters. This approach plots the within-cluster sum of the square against the number of clusters. Within-cluster sum of square measures the variability within the cluster. With the same dataset, as the number of clusters increases, the within-cluster sum of squares tends to decrease.\r\nFollowing is an example of how the elbow curve graph looks like, where the x-axis represents the number of clusters and the y-axis represents the total withinness between clusters.\r\n\r\n\r\n\r\nExample of elbow curve graph\r\nHowever, having too many clusters could introduce noises into the clustering results, increasing the within-cluster sum of squares. The rule of thumb is to choose the point that the decrease in the within-cluster sum of squares would be insignificant when the number of clusters increases. This is because a simpler model is preferred if the simpler model can differentiate the segments well.\r\nAlso, note that one could compare the clustering results only if the clustering results are run based on the same set of data. This is because supervised learning methods have an explicit target, so different accuracy measurements can be used to measure the performance of the different methods.\r\nOn the other hand, unlike the supervised learning method, the fundamental idea of the unsupervised learning method is to use the algorithms to find the various underlying patterns within the dataset. Hence, the unsupervised learning method does not have an explicit target for the algorithm to learn and measure on.\r\nHence, it is not meaningful to compare clustering results if the underlying dataset used in the clustering is different. However, we could compare the cluster results qualitatively to see which cluster models provide a more meaningful result. Following are some considerations in comparing the cluster results qualitatively:\r\nDoes each cluster have a reasonable amount of data points within each cluster (eg. not less than 5 data points within each cluster)?\r\nIs there any overlapping between the clusters? A good cluster should be able to show the differences between the data points\r\nLimitation of Selected Clustering Algorithm\r\n\r\n\r\n\r\nPhoto by lucas souza from Pexels\r\nEach clustering algorithm has its limitations, hence appropriate data transformation might require before the analysis.\r\nFor example, the K-means clustering algorithm can only accept numeric variables. Hence the non-numeric variables will need to be transformed before they can be used for clustering purposes.\r\nAlso, the common clustering algorithm assumes the data is time-independent. This assumption is not appropriate when we are dealing with sequential data, such as time-series data, speech data, and so on. Hence, other clustering methods (eg. dynamic time warping) that take time into considerations when performing clustering would be more appropriate. Do check out my previous poster on how we could use dynamic time warping to analyze time-series data in this post.\r\n\r\n\r\n\r\nPoster of my project on Dynamic Time Warping\r\nHence, it is important to understand the limitations of the selected clustering algorithm before the analysis to ensure meaningful results.\r\nConclusion\r\nThat’s all for the day!\r\nAbove are some of the important considerations while performing clustering analysis.\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nGoodman, L. A. 1974. “Exploratory Latent Structure Analysis Using Both Identifiable and Unidentifiable Models.” Biometrika.\r\n\r\n\r\nGoogle Developers. 2020. “Clustering in Machine Learning.” https://developers.google.com/machine-learning/clustering/overview.\r\n\r\n\r\nIBM Cloud Education. 2020. “Unsupervised Learning.” https://www.ibm.com/cloud/learn/unsupervised-learning.\r\n\r\n\r\nIvo, Dinov D. 2018. Data Science and Predictive Analytics: Biomedical and Health Applications Using r. Springer.\r\n\r\n\r\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in r. Springer.\r\n\r\n\r\nKam, T. Seong, ed. 2019. “Isss602 Data Analytics Lab Week 4 Lesson Slides - Cluster Analysis: Concepts, Algorithms and Methods.”\r\n\r\n\r\nLogan, Jessica AR., and Jill M. Pentimonti. 2016. Introduction to Latent Class Analysis for Reading Fluency Research. Edited by Kelli D. Cummings and Yaacov Petscher. Springer.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-17-clustering/image/categorize.jpg",
    "last_modified": "2021-10-17T09:29:08+08:00",
    "input_file": "clustering.knit.md"
  },
  {
    "path": "posts/2021-09-17-transfer-learning-with-cnn/",
    "title": "Transfer Learning with Convolution Neural Network",
    "description": "Why learn from scratch when you can leverage the existing work?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-09-17",
    "categories": [
      "Tensorflow/Keras",
      "Deep Learning",
      "Image Recognition",
      "Transfer Learning"
    ],
    "contents": "\r\n\r\nContents\r\nHow is image recognition relevant for insurance?\r\nShorten operation process\r\nAdditional Pricing Parameters\r\nAdditional Parameters in Claim Estimation\r\n\r\nWhat is ‘Transfer Learning?’\r\nPre-trained Models\r\nConsiderations when using Pre-trained Models\r\nData similarity: Is the current dataset similar to the dataset used to train pre-trained model?\r\nSize of the data set: How big is our dataset?\r\nHow are we training the model?\r\n\r\nModel Building Steps\r\nDemonstration\r\nSetup the environment\r\nImport data\r\nImport the Pre-trained model\r\nAdd on Classifier Layer\r\nImage Augmentation\r\nModel Preparation\r\nModel Fitting\r\nVisualizing the Model Results\r\n\r\n\r\nConclusion\r\n\r\nRecently I happened to come across this post by (Chollet and Allaire 2017) on RStudio AI Blog and inspired me to give it a try to build a convolution neural network (CNN) model to solve an image classification problem.\r\nBefore jumping into the discussion, let’s take a look at how image recognition can be used in the insurance context.\r\nHow is image recognition relevant for insurance?\r\nPapers are suggesting how the image recognition technology can be used in the insurance context.\r\nIn the ‘Applying Image Recognition to Insurance’ report, (Shang 2018) explored a few examples of how insurers could leverage image recognition.\r\nShorten operation process\r\nImage recognition can be utilized at different stages of the insurance stage (eg. policy inception, update policy information, policy claim), where such use cases in this area can be observed in some countries.\r\nFor example, instead of manually inputting necessary info into the different documents at the point of policy inception, insurers could build an algorithm that would capture the details in the images uploaded. This could potentially shorten the turnaround time, resulting in an improvement in customer satisfaction.\r\nAdditional Pricing Parameters\r\n(Shang 2018) also discussed how these techniques can be used for insurance pricing. For example, in property insurance, pictures can be used to understand the riskiness of the property, providing additional parameters for pricing and underwriting purpose.\r\nNevertheless, this could allow the insurers in implementing dynamic pricing on the relevant business lines, improving the profitability of the business.\r\nAdditional Parameters in Claim Estimation\r\n(Shang 2018) also provided an example of how image recognition could be applied in real-time risk monitoring and risk management. Insurers can use the information extracted from the image to predict claim count and claim amount.\r\nNevertheless, in this exercise, instead of building the model from scratch, I will use what is known as “transfer learning” to speed up the model-building problem.\r\n\r\n\r\n\r\nPhoto by cottonbro from Pexels\r\nWhat is ‘Transfer Learning?’\r\n(Brownlee 2019a) explained that transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\r\n(Google 2021) also explained that the intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\r\nPre-trained Models\r\nThere are different types of pre-trained models that are trained to solve different types of problems. Some are trained to classify images, some are trained to handle text-related problems, and so on.\r\n\r\n\r\n\r\nDo check out this link to know about the different pre-trained models in TensorFlow.\r\nFor this analysis, I will focus the pre-trained models on the image classification problem. I will use VGG16 model to build a multi-class classification for the dataset. The relevant paper of this paper can be found in this link.\r\nConsiderations when using Pre-trained Models\r\nThe considerations of using pre-trained models can be summarized as the following diagram:\r\n\r\n\r\n\r\nDiagram on Model Tuning Considerations (Gupta 2017)\r\nData similarity: Is the current dataset similar to the dataset used to train pre-trained model?\r\nThese pre-trained models are usually commonly trained by using image data from ImageNet, where ImageNet is an image database with more than 14 million images. As such, this makes the pre-trained models generalized models.\r\nSo, before using the pre-trained models, we should ask ourselves how similar is between our dataset and the dataset used in pre-trained model. If our images are very different from the image data used in training the pre-trained models, the accuracy of our model is likely to be low.\r\nSize of the data set: How big is our dataset?\r\nAnother important consideration is how big is our dataset. The model is unlikely to perform well if the dataset is not large enough to train the models. Therefore, pre-trained models can be used as an alternative if our dataset is not large enough.\r\nHow are we training the model?\r\nThese pre-trained models typically come with the model weights that are derived by fitting the models with images from ImageNet. Hence, when we are using pre-trained models, one of the key questions we should answer is whether we should be using the model weights in the pre-trained models.\r\nThe usual approach is to freeze all the layers in the pre-trained model, except the few top layers. The few top unfreeze layers will be fine-tuned together with the classifier layer.\r\n\r\n\r\n\r\nCNN structure (Saha 2018)\r\nDo note that the more layers we unfreeze, the longer it would take to fit the model.\r\nModel Building Steps\r\nIn general, the model building steps can be summarized as follows:\r\nSplit the dataset into train and validation dataset\r\nImport pre-trained model into the environment and exclude the classifier layer from the pre-trained model\r\nAdd on classifier layer on top of the pre-trained model\r\nFreeze the weights of the pre-trained model, except for a few top layers to be tuned together with the classifier layer\r\nTrain the unfreeze top layers and classifier layer to make the model more relevant for the specific task\r\nDemonstration\r\nI will be using this image dataset from Kaggle. There are about 28k medium-quality images in this data. These photos are from 10 different categories of animals, which consist of dog, cat, horse, spider, butterfly, chicken, sheep, cow, squirrel, and elephant.\r\nBelow are some of the extracted images from the dataset:\r\n\r\n\r\n\r\nI have chosen these as the examples as I thought they looks funny.\r\nSetup the environment\r\nAs usual, I will start by calling all the relevant packages I would need in the analysis later on.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'keras')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport data\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\n\r\nAs mentioned in the post earlier, there are 10 different categories in the dataset. Hence this is a multi-class classification problem. Since the photos are saved under the respective categories, I will use dir function to extract out the list of categories, where train_dir is the link to the folder I stored the image dataset.\r\n\r\n\r\nlabel_list <- dir(train_dir)\r\n\r\n\r\n\r\nNext, length function is used to perform a count on the number of categories.\r\n\r\n\r\noutput_n <- length(label_list)\r\n\r\n\r\n\r\nNext, I will define the input size to be passed into the CNN model later. I have also defined the channel to be 3 as the photos are colored photos.\r\n\r\n\r\nwidth <- 224\r\nheight<- 224\r\nrgb <- 3 \r\n\r\ntarget_size <- c(width, height)\r\n\r\n\r\n\r\nImport the Pre-trained model\r\nAs discussed earlier, I would use the pre-trained model to classify the image. Over here, I will be using VGG16 as shown in the code chunk below. As I won’t be re-trained the entire network, I will indicate the model weights should follow the derived weights when the model is trained by using ImageNet dataset.\r\nAlso, to include our classifier layer, I will indicate the include_top argument to be false so that the pre-trained model is imported without the classifier layer.\r\n\r\n\r\nconv_base <- application_vgg16(\r\n  weights = \"imagenet\",\r\n  include_top = FALSE,\r\n  input_shape = c(width, height, rgb)\r\n)\r\n\r\n\r\n\r\nFor other pre-trained models, do refer to the ‘Applications’ section under either Keras documentation page or TensorFlow documentation page for more information.\r\nNext, summary function can be used to visualize how the pre-trained model looks like.\r\n\r\n\r\nsummary(conv_base)\r\n\r\n\r\nModel: \"vgg16\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\ninput_1 (InputLayer)           [(None, 224, 224, 3)]       0          \r\n______________________________________________________________________\r\nblock1_conv1 (Conv2D)          (None, 224, 224, 64)        1792       \r\n______________________________________________________________________\r\nblock1_conv2 (Conv2D)          (None, 224, 224, 64)        36928      \r\n______________________________________________________________________\r\nblock1_pool (MaxPooling2D)     (None, 112, 112, 64)        0          \r\n______________________________________________________________________\r\nblock2_conv1 (Conv2D)          (None, 112, 112, 128)       73856      \r\n______________________________________________________________________\r\nblock2_conv2 (Conv2D)          (None, 112, 112, 128)       147584     \r\n______________________________________________________________________\r\nblock2_pool (MaxPooling2D)     (None, 56, 56, 128)         0          \r\n______________________________________________________________________\r\nblock3_conv1 (Conv2D)          (None, 56, 56, 256)         295168     \r\n______________________________________________________________________\r\nblock3_conv2 (Conv2D)          (None, 56, 56, 256)         590080     \r\n______________________________________________________________________\r\nblock3_conv3 (Conv2D)          (None, 56, 56, 256)         590080     \r\n______________________________________________________________________\r\nblock3_pool (MaxPooling2D)     (None, 28, 28, 256)         0          \r\n______________________________________________________________________\r\nblock4_conv1 (Conv2D)          (None, 28, 28, 512)         1180160    \r\n______________________________________________________________________\r\nblock4_conv2 (Conv2D)          (None, 28, 28, 512)         2359808    \r\n______________________________________________________________________\r\nblock4_conv3 (Conv2D)          (None, 28, 28, 512)         2359808    \r\n______________________________________________________________________\r\nblock4_pool (MaxPooling2D)     (None, 14, 14, 512)         0          \r\n______________________________________________________________________\r\nblock5_conv1 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_conv2 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_conv3 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_pool (MaxPooling2D)     (None, 7, 7, 512)           0          \r\n======================================================================\r\nTotal params: 14,714,688\r\nTrainable params: 14,714,688\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nAdd on Classifier Layer\r\nNext, I will add on the classifier layer as discussed in the earlier post. As this is a multi-class classification problem, hence softmax is selected to be the last activation function.\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  conv_base %>% \r\n  layer_flatten() %>% \r\n  layer_dense(units = 256, activation = \"relu\") %>% \r\n  layer_dense(units = output_n, activation = \"softmax\")\r\n\r\n\r\n\r\nI will call the summary function to check on the model before fitting the model.\r\n\r\n\r\nsummary(model)\r\n\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nvgg16 (Functional)             (None, 7, 7, 512)           14714688   \r\n______________________________________________________________________\r\nflatten (Flatten)              (None, 25088)               0          \r\n______________________________________________________________________\r\ndense_1 (Dense)                (None, 256)                 6422784    \r\n______________________________________________________________________\r\ndense (Dense)                  (None, 10)                  2570       \r\n======================================================================\r\nTotal params: 21,140,042\r\nTrainable params: 21,140,042\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nImage Augmentation\r\nImage augmentation is a commonly used technique to ensure the model is not overfit.\r\n(Brownlee 2019b) explained this technique allows one to artificially created new training data from existing data. The author further explained that the reason for using such a method is this method will aid the modern deep learning algorithms to learn the different features that are similar, but not entirely the same as the training photo.\r\nFor example, it would be a problem if the objects always appear on the same side of the photo in our dataset. The algorithm may not be able to identify the objects if the objects do not appear on the same side as what is observed under the training dataset in the new photo.\r\nSo, the code chunk below will perform image augmentation on the training data.\r\n\r\n\r\ntrain_datagen = image_data_generator(\r\n  rescale = 1/255,\r\n  rotation_range = 40,\r\n  width_shift_range = 0.2,\r\n  height_shift_range = 0.2,\r\n  shear_range = 0.2,\r\n  zoom_range = 0.2,\r\n  horizontal_flip = TRUE,\r\n  fill_mode = \"nearest\",\r\n  validation_split = 0.2\r\n)\r\n\r\n\r\n\r\nApart from that, 20% of the training dataset is being held back as the validation dataset.\r\nAlso, one important note to take note of while performing image augmentation is the validation & test dataset should not be augmented.\r\n\r\n\r\nvalidation_datagen <- image_data_generator(rescale = 1/255)  \r\n\r\n\r\n\r\nModel Preparation\r\nTo leverage on transfer learning, typically we will freeze the base model and only unfreeze the connecting layers so that we can fine-tune the layers together with the classifier layer.\r\n\r\n\r\nfreeze_weights(conv_base)\r\n\r\nunfreeze_weights(conv_base, from = \"block5_conv1\")\r\n\r\n\r\n\r\nNext, I will generate batches of data in the code chunk below.\r\n\r\n\r\ntrain_generator <- flow_images_from_directory(\r\n  train_dir,                  # Target directory  \r\n  train_datagen,              # Data generator\r\n  target_size = target_size,  # Resizes all images to 150 × 150\r\n  class_mode = \"categorical\"       # binary_crossentropy loss for binary labels\r\n)\r\n\r\nvalidation_generator <- flow_images_from_directory(\r\n  train_dir,\r\n  validation_datagen,\r\n  target_size = target_size,\r\n  class_mode = \"categorical\"\r\n)\r\n\r\n\r\n\r\nLastly, I will define all the different model components before fitting the model.\r\nI will be using optimizer_sgd (ie. stochastic gradient descent optimizer) to fit the model. Do check out the documentation page on the different optimizers.\r\nAs this is a multi-class classification problem, hence I will be using categorical_accuracy as the performance metric.\r\n\r\n\r\nmodel %>% compile(\r\n  loss = \"binary_crossentropy\",\r\n  optimizer = optimizer_sgd(),\r\n  metrics = c(\"categorical_accuracy\")\r\n)\r\n\r\n\r\n\r\nModel Fitting\r\nOnce the different model components are being defined, I will start the model fitting as shown below.\r\n\r\n\r\nhistory <- model %>% fit_generator(\r\n  train_generator,\r\n  steps_per_epoch = 100,\r\n  epochs = 10,\r\n  validation_data = validation_generator,\r\n  validation_steps = 50\r\n)\r\n\r\n\r\n\r\nVisualizing the Model Results\r\nOnce the model is fit, the history from the model fitting is passed into plot function to visualize how the results change when the epochs increase.\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\n\r\nYay! Based on the accuracy shown above, it shows that the built model does have some levels of predictability, ie. the model will perform better than a random guess on the image category. Indeed, as the epoch increases, the model accuracy increases as well.\r\nAlso, as we can see in the graph above, the accuracy increases when the epoch increases. This suggests the accuracy could increase further if we increase the number of epoch.\r\nConclusion\r\nThat’s all for today!\r\nThanks for reading the post until the end.\r\nDo check out on the Keras documentation page or TensorFlow documentation page if you want to find out more on deep learning.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nPhoto by Александар Цветановић from Pexels\r\n\r\n\r\n\r\nBrownlee, Jason. 2019a. “Machine Learning Mastery: A Gentle Introduction to Transfer Learning for Deep Learning.” https://machinelearningmastery.com/transfer-learning-for-deep-learning/.\r\n\r\n\r\n———. 2019b. “Machine Learning Mastery: How to Configure Image Data Augmentation in Keras.” https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/.\r\n\r\n\r\nChollet, François, and J. J. Allaire. 2017. “RStudio AI Blog: Image Classification on Small Datasets with Keras.” https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/.\r\n\r\n\r\nGoogle. 2021. “TensorFlow: Transfer Learning and Fine-Tuning.” https://www.tensorflow.org/tutorials/images/transfer_learning.\r\n\r\n\r\nGupta, Dishashree. 2017. “Analytics Vidhya: Transfer Learning and the Art of Using Pre-Trained Models in Deep Learning.” https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/.\r\n\r\n\r\nSaha, Sumit. 2018. “Towards Data Science: A Comprehensive Guide to Convolutional Neural Networks — the Eli5 Way.” https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.\r\n\r\n\r\nShang, Kailan, ed. 2018. “Applying Image Recognition to Insurance.” Society of Actuaries. https://www.soa.org/globalassets/assets/Files/resources/research-report/2018/applying-image-recognition.pdf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-17-transfer-learning-with-cnn/image/image_on_wall.jpg",
    "last_modified": "2021-09-17T00:01:09+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-31-naive-bayes/",
    "title": "Naive Bayes Classifier",
    "description": "When the \"naive\" one outperform the conventional",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-09-04",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nDiscriminative Model vs Generative Model\r\nNaive Bayes Classifier\r\nDiscrim R Package\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Checking & Wrangling\r\nData Cleaning\r\nModel Building\r\nLogistic Regression\r\nNaive Bayes Classifier\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Shiva Smyth from Pexels\r\nConventionally, logistic regression is often used to solve classification problems.\r\nIn this post, I will explore an alternative popular machine learning algorithms, i.e. naive bayes classifier to solve classification problems.\r\nBefore diving into naive bayes classifier, let’s understand the difference between discriminative model and generative model.\r\nDiscriminative Model vs Generative Model\r\nDiscriminative models directly model the posterior probability \\(Pr(Y|X)\\). One good example of such model is logistic regression models.\r\nFor generative models, instead of directly model the posterior probability, this type of model will model the join distribution of X & Y before predicting the posterior probability given as \\(Pr(Y|X)\\) (Ottesen 2017). Naive bayes classifier is an example of such model.\r\nNaive Bayes Classifier\r\nIn this algorithm, the prior knowledge is being included when making predictions. Essentially the idea for this algorithm is given the features we observed in the predictors, what is likelihood the ‘y’ belong to the particular class?\r\nFollowing is the mathematical expression for naive bayes classifier obtained from Scikit-learn (scikit-learn):\r\n\\[\\hat{y} = arg max_y P(y)\\prod_{i=1}^n P(x_i|y)\\]\r\nOne of the key assumptions in Naive Bayes classifier is all the predictors are assumed to be independent from one another (Kuhn and Johnson 2013). This assumption has effectively simplified the calculations, allowing one to multiply the conditional probabilities together as shown in the mathematical expression above.\r\nAlthough the predictors are unlikely to be independent from one another, this model has a record of decent model performance.\r\nNevertheless, as what Dr Brownlee suggested in his post of tactics to combat imbalanced classes (Brownlee, n.d.), it is probably worthwhile to try different machine learning algorithms, instead of always sticking to our favorite algorithm. This would enable us in selecting the algorithm has a higher accuracy.\r\nDiscrim R Package\r\nIn this demonstration, naive bayes wrapper function from discrim package will be used.\r\nDiscrim packages contains various discriminant analysis models, including linear discriminant models and Naive Bayes Classifier.\r\nTo contrast the model performance, I will be using logistic_reg function from parnsip package to build logistic regression models.\r\nDemonstration\r\nFor the dataset, I will be using a travel insurance dataset from Kaggle.\r\n\r\n\r\n\r\nPhoto by Annie Spratt on Unsplash\r\nSetup the environment\r\nAs usual, I will start by calling all the relevant packages I would need in the analysis later on.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidymodels', 'discrim', \r\n              'naivebayes', 'glmnet', 'tictoc', 'vip', 'shapr', \r\n              'DALEXtra', 'funModeling', 'plotly', 'readxl', 'ggmosaic')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nI will import the dataset obtained from Kaggle website.\r\n\r\n\r\ndf <- read_csv(\"data/travel insurance.csv\") %>%\r\n  rename(\"Commission\" = \"Commision (in value)\")\r\n\r\n\r\n\r\nData Checking & Wrangling\r\nAs usual, I will start with some basic data quality checking.\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n63326\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n7\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nAgency\r\n0\r\n1.00\r\n3\r\n3\r\n0\r\n16\r\n0\r\nAgency Type\r\n0\r\n1.00\r\n8\r\n13\r\n0\r\n2\r\n0\r\nDistribution Channel\r\n0\r\n1.00\r\n6\r\n7\r\n0\r\n2\r\n0\r\nProduct Name\r\n0\r\n1.00\r\n9\r\n36\r\n0\r\n26\r\n0\r\nClaim\r\n0\r\n1.00\r\n2\r\n3\r\n0\r\n2\r\n0\r\nDestination\r\n0\r\n1.00\r\n4\r\n42\r\n0\r\n149\r\n0\r\nGender\r\n45107\r\n0.29\r\n1\r\n1\r\n0\r\n2\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nDuration\r\n0\r\n1\r\n49.32\r\n101.79\r\n-2\r\n9\r\n22.00\r\n53.00\r\n4881.0\r\n▇▁▁▁▁\r\nNet Sales\r\n0\r\n1\r\n40.70\r\n48.85\r\n-389\r\n18\r\n26.53\r\n48.00\r\n810.0\r\n▁▇▁▁▁\r\nCommission\r\n0\r\n1\r\n9.81\r\n19.80\r\n0\r\n0\r\n0.00\r\n11.55\r\n283.5\r\n▇▁▁▁▁\r\nAge\r\n0\r\n1\r\n39.97\r\n14.02\r\n0\r\n35\r\n36.00\r\n43.00\r\n118.0\r\n▁▇▂▁▁\r\n\r\nGender\r\nThere is excessive missing value for Gender, i.e. only about 28.8% of the data contains values in Gender column. Hence, I will drop this columns since this variable is unlikely to be going to be meaningful in explaining the target variable.\r\nDestinations\r\nThere are about 149 unique values under destinations. This could be an issue when we use this feature to build machine learning model as there are too many unique categories.\r\nTo further group the destinations, I have extracted the mapping between destinations and continents from internet and imported the mapping results into the environment. This allows me to join with the existing dataset.\r\nBelow is code chunk I have imported into the environment and performed a left join with the original dataset:\r\n\r\n\r\ndestination_state <- read_excel(\"data/Destination_Continent_Mapping.xlsx\")\r\n\r\ndf <- df %>%\r\n  left_join(destination_state, by = c(\"Destination\" = \"Destination\"))\r\n\r\n\r\n\r\nNext, I use bar chart to plot the claim proportion by continent.\r\n\r\n\r\nggplot(df, aes(Continent, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion by Continents\")\r\n\r\n\r\n\r\n\r\nAs the proportion of claim policies is rather low, it is hard to compare the claim proportion across different continents.\r\n\r\n\r\ndf %>%\r\n  group_by(Continent, Claim) %>%\r\n  summarise(count = n()) %>%\r\n  pivot_wider(names_from = Claim, names_prefix = \"Claim_\", values_from = count) %>%\r\n  mutate(total = Claim_No + Claim_Yes,\r\n         Claim_perc = Claim_Yes / total * 100)\r\n\r\n\r\n# A tibble: 6 x 5\r\n# Groups:   Continent [6]\r\n  Continent     Claim_No Claim_Yes total Claim_perc\r\n  <chr>            <int>     <int> <int>      <dbl>\r\n1 Africa             298         5   303      1.65 \r\n2 Asia             49596       771 50367      1.53 \r\n3 Europe            4991        63  5054      1.25 \r\n4 North America     3041        45  3086      1.46 \r\n5 Oceania           4234        42  4276      0.982\r\n6 South America      239         1   240      0.417\r\n\r\nProduct Name\r\nI will plot the claim proportion by product names.\r\n\r\n\r\nggplot(df, aes(x = `Product Name`, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion by Product Name\")\r\n\r\n\r\n\r\n\r\nThere are too many categories under product name. This would affect the performance of machine learning models if we use this variable to build models.\r\nHence, I will further group the product names so that this feature does not have too many unique categories.\r\nAs I am unable to find any further information on the product, hence I have kept products with top 90% of the sales as their original naming and renamed the remaining products as ‘Others.’\r\nOnce the product names are being recoded, I will join the recoded product names back to the original dataset.\r\n\r\n\r\ndf_prod <- df %>%\r\n  group_by(`Product Name`) %>%\r\n  summarise(count = n(),\r\n            claim_ind = sum(Claim == \"Yes\")) %>%\r\n  mutate(claim_perc = claim_ind/count) %>%\r\n  arrange(desc(count)) %>%\r\n  mutate(claim_cum_perc = cumsum(count)/sum(count),\r\n         product_name_recoded = case_when(claim_cum_perc > 0.9 ~ \"Others\",\r\n                                          TRUE ~ as.character(`Product Name`))) %>%\r\n  select(-c(\"claim_perc\", \"claim_cum_perc\", \"claim_ind\", \"count\")) %>%\r\n  ungroup()\r\n\r\ndf <- df %>%\r\n  left_join(df_prod, by = c(\"Product Name\" = \"Product Name\"))\r\n\r\n\r\n\r\n\r\n\r\ndf %>%\r\n  filter(Duration < 1000) %>%\r\n  ggplot(aes(product_name_recoded, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion of Product Name Recoded\")\r\n\r\n\r\n\r\n\r\nAs shown above, we can see that claim proportion for Bronze Plan and Others are higher than other plans.\r\nDuration\r\nBelow is the duration density plot:\r\n\r\n\r\nduration_mean <- df %>%\r\n  summarise(duration_mean = mean(Duration))\r\n\r\nggplot(df, aes(Duration)) +\r\n  geom_density() +\r\n  geom_vline(data = duration_mean, aes(xintercept = duration_mean), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 800, y = 0.025, label = paste0(\"Mean of Duration: \", round(duration_mean$duration_mean, 2))) +\r\n  labs(title = \"Density of Duration\")\r\n\r\n\r\n\r\n\r\nAs shown in the graph, most of the trips were very short trip.\r\nIf we were removed any data points with duration longer than 1000 and plot duration density between claim and no claim policies, following is the chart:\r\n\r\n\r\nduration_mean_claim <- df %>%\r\n  filter(Duration < 1000, Claim == \"Yes\") %>%\r\n  summarise(mean_dur = mean(Duration),\r\n            mean_dur_log = log(mean_dur + 1))\r\n\r\nduration_mean_Noclaim <- df %>%\r\n  filter(Duration < 1000, Claim == \"No\") %>%\r\n  summarise(mean_dur = mean(Duration),\r\n            mean_dur_log = log(mean_dur + 1))\r\n  \r\n  \r\ndf %>%\r\n  filter(Duration < 1000) %>%\r\n  ggplot(aes(x = log(Duration + 1), color = Claim)) +\r\n  geom_density(alpha = 0.2) + \r\n  geom_vline(data = duration_mean_claim, aes(xintercept = mean_dur_log), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 5.65, y = 0.32, label = \"Average Duration - \") +\r\n  annotate(\"text\", x = 5.65, y = 0.3, label = \"Claim Policies\") +\r\n  geom_vline(data = duration_mean_Noclaim, aes(xintercept = mean_dur_log), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 3, y = 0.03, label = \"Average Duration - \") +\r\n  annotate(\"text\", x = 3, y = 0.01, label = \"No Claim Policies\") +\r\n  labs(title = \"Density of Duration between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nIt seems like the average duration of the policies with no claim is shorter than the average duration of the duration with claim. This is logical since the duration is likely to represent the coverage duration of the travel insurance. Typically the coverage of the travel insurance starts from the day the insurance is purchased till the day the trip is being completed. Therefore, with a higher duration, it is more likely the customers would make a claim during the coverage period.\r\nMeanwhile, I do note that there are negative values within this variable, which the values do not seem to be reasonable. Therefore, I will remove them from the dataset.\r\nSales\r\nIn the earlier data summary, there are some policies with sales with zero or even negative. This is not logical as the sales of insurance policies would be positive. As lack of information, I would remove these policies from the dataset so that it will not affect the model performance.\r\n\r\n\r\nnetSales_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\",\r\n         `Net Sales` > 0) %>%\r\n  summarise(netSales_mean_claim = mean(`Net Sales`))\r\n\r\nnetSales_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\",\r\n         `Net Sales` > 0) %>%\r\n  summarise(netSales_mean_Noclaim = mean(`Net Sales`))\r\n\r\ndf %>%\r\n  filter(`Net Sales` > 0) %>%\r\n  ggplot(aes(`Net Sales`, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = netSales_mean_claim, aes(xintercept = netSales_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 240, y = 0.03, label = \"Average Sales - Claim Policies\") +\r\n  geom_vline(data = netSales_mean_Noclaim, aes(xintercept = netSales_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 200, y = 0.02, label = \"Average Sales - No Claim Policies\") +\r\n  labs(title = \"Density of Sales between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nCommission\r\nSimilarly, I will plot the commission distribution between claim and no claim policies.\r\n\r\n\r\ncomm_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\") %>%\r\n  summarise(comm_mean_claim = mean(Commission))\r\n\r\ncomm_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\") %>%\r\n  summarise(comm_mean_Noclaim = mean(Commission))\r\n\r\ndf %>%\r\n  ggplot(aes(Commission, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = comm_mean_claim, aes(xintercept = comm_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 80, y = 0.1, label = \"Average Commission - Claim Policies\") +\r\n  geom_vline(data = comm_mean_Noclaim, aes(xintercept = comm_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 100, y = 0.15, label = \"Average Commission - No Claim Policies\") +\r\n  labs(title = \"Density of Commission between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nFrom the graph, the average commission for policies that have made a claim is lower than the average commission for policies that does not make a claim.\r\nAge\r\n\r\n\r\nage_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\") %>%\r\n  summarise(age_mean_claim = mean(Age))\r\n\r\nage_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\") %>%\r\n  summarise(age_mean_Noclaim = mean(Age))\r\n\r\ndf %>%\r\n  ggplot(aes(Age, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = age_mean_claim, aes(xintercept = age_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 20, y = 0.1, label = \"Average Age - \") +\r\n  annotate(\"text\", x = 20, y = 0.088, label = \"Claim Policies\") +\r\n  geom_vline(data = age_mean_Noclaim, aes(xintercept = age_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 68, y = 0.15, label = \"Average Age - No Claim Policies\") +\r\n  labs(title = \"Density of Age between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nUnlike the previous numerical variables, it seems like the average age between claim and no claim policies is quite minimal.\r\nAlso, it seems like there are weird ages within the dataset as well. There are 984 policies with age 118, which seem to be what we know as ‘magic value’ in feature engineering. One possible explanation for this is the system will default the age to be 118 if the age is not being input into the system.\r\n\r\n\r\ndf %>%\r\n  filter(Age > 100) %>%\r\n  tally()\r\n\r\n\r\n# A tibble: 1 x 1\r\n      n\r\n  <int>\r\n1   984\r\n\r\nAs I do not have any further information to estimate the ages for these policies, I will remove them from the dataset before building the model.\r\nData Cleaning\r\nBefore building the model, I will proceed and clean the data.\r\n\r\n\r\ndf_1 <- df %>%\r\n  select(-c(Gender, `Product Name`, Destination)) %>%\r\n  filter(`Net Sales` > 0,\r\n         Duration > 0,\r\n         Age < 100) %>%\r\n  rename_with(~gsub(\" \", \"_\", .x, fixed = TRUE))\r\n\r\n\r\n\r\nModel Building\r\nBelow is the generic parameters set for this analysis:\r\n\r\n\r\nset.seed(1234)\r\n\r\nprop_split <- 0.6\r\n\r\ngrid_num <- 5\r\n\r\n\r\n\r\nIn this section, I will split the data into training and testing dataset. Once this is done, I will prepare the dataset for cross validation later.\r\n\r\n\r\ndf_split <- initial_split(df_1, prop = prop_split, strata = Claim)\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\ndf_folds <- vfold_cv(df_train, strata = Claim)\r\n\r\n\r\n\r\nNext, I will define the formula and dataset to be used to train the model.\r\n\r\n\r\ngen_recipe <- recipe(Claim ~ ., data = df_train)\r\n\r\n\r\n\r\nOkay, let’s start the analysis by using our classic machine learning model for classification problem - logistic regression!\r\nLogistic Regression\r\nTo record how long it takes to build a logistic regression model, I use tic & toc functions from tictoc package to capture the time spent in building model.\r\n\r\n\r\ntic(\"Time to Build Logistic Regression\")\r\n\r\n\r\n\r\nFirst, I will define the data pre-processing steps to be performed.\r\n\r\n\r\nlogit_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_zv(all_predictors()) %>%\r\n  step_normalize(all_predictors())\r\n\r\n\r\n\r\nThen, I will define the model to be built. I have also indicated the parameters to be tuned by using tune function to mark the parameters.\r\n\r\n\r\nlogit_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"glmnet\")\r\n\r\n\r\n\r\nOnce this is done, I will chain all the created objects as a workflow.\r\n\r\n\r\nlogit_workflow <- workflow() %>% \r\n  add_recipe(logit_recipe) %>%\r\n  add_model(logit_spec)\r\n\r\n\r\n\r\nThen, I will start performing cross validation to find the best set of parameters.\r\n\r\n\r\nlogit_grid <- tidyr::crossing(penalty = c(1,2,3,4,5,6,7,8,9,10), mixture = c(0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1))\r\n\r\nlogit_tune <- tune_grid(logit_workflow, \r\n                        resample = df_folds, \r\n                        grid = logit_grid)\r\n\r\n\r\n\r\nThe model is then fitted with the best set of parameters.\r\n\r\n\r\nlogit_fit <- logit_workflow %>%\r\n  finalize_workflow(select_best(logit_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n\r\n\r\nThe predicted values are then “collected” so that I could use them to calculate the necessary model performance.\r\n\r\n\r\nlogit_pred <- logit_fit %>%\r\n  collect_predictions()\r\n\r\n\r\n\r\nOnce the predicted values are being “collected,” toc function is used to calculate how long it takes to fit the model and make the necessary predictions.\r\n\r\n\r\ntoc()\r\n\r\n\r\nTime to Build Logistic Regression: 196.01 sec elapsed\r\n\r\nAccuracy is typically used to measure classification model.\r\n\r\n\r\naccuracy(logit_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.984\r\n\r\nBased on the accuracy results, it seems like this logistic model is a pretty decent model given it has such a high accuracy. More than 98% of the policies are being accurately classified.\r\nHowever, it can be very misleading if our decision is just based on accuracy measurement. Accuracy looks the number of policies are being accurately classified whether they have claimed or not. It does not differentiate the true positive and true negative in this case.\r\nHowever, in insurance, we are more interested in the policies that have claimed. One way is to compute the confusion matrix for the relevant model.\r\n\r\n\r\nconf_mat(logit_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n          Truth\r\nPrediction    No   Yes\r\n       No  23539   372\r\n       Yes     0     0\r\n\r\nOh no! According to the results under confusion matrix, our model always predict that the policies will not make a claim. This is bad as this would result in the expected claim severely unstated.\r\nNow let’s us plot the ROC curve.\r\n\r\n\r\nautoplot(roc_curve(logit_pred, \r\n                   truth = Claim, \r\n                   estimate = .pred_No))\r\n\r\n\r\n\r\n\r\nThe ROC curve for this model lie at the diagonal line, suggesting that this fitted model has no predictive power. It is as good as we take a random guess whether the selected policy will make a claim.\r\nIf we were to visualize the claim count by using a histogram, we can see that the proportion of policyholders claimed is very low.\r\n\r\n\r\nggplot(df_1, aes(Claim)) +\r\n  geom_histogram(stat = \"count\")\r\n\r\n\r\n\r\n\r\nNaive Bayes Classifier\r\nOkay, let’s move on and build a naive bayes classifier model.\r\nFirst, I will use tic function to indicate where I should start measuring the time taken to build the model.\r\n\r\n\r\ntic(\"Time to Build Naive Bayes Classifier\")\r\n\r\n\r\n\r\nThen, I will start building the model, same as how I did it for logistic regression.\r\n\r\n\r\nnaive_recipe <- gen_recipe %>%\r\n  step_zv(all_predictors())\r\n\r\nnaive_spec <- naive_Bayes(smoothness = tune(), Laplace = tune()) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"naivebayes\")\r\n\r\nnaive_workflow <- workflow() %>%\r\n  add_recipe(naive_recipe) %>%\r\n  add_model(naive_spec)\r\n\r\nnaive_tune <- tune_grid(naive_workflow, \r\n                        resample = df_folds, \r\n                        grid = grid_num)\r\n\r\nnaive_fit <- naive_workflow %>%\r\n  finalize_workflow(select_best(naive_tune)) %>%\r\n  last_fit(df_split)\r\n\r\nnaive_pred <- naive_fit %>%\r\n  collect_predictions()\r\n\r\n\r\n\r\n\r\n\r\ntoc()\r\n\r\n\r\nTime to Build Naive Bayes Classifier: 36.62 sec elapsed\r\n\r\nNote that the naive bayes classifier takes lesser time to build the model. In fact, it took less time to build naive bayes classifier than build logistic regression.\r\nThis is consistent with what we have discussed in the earlier section of this post.\r\n\r\n\r\naccuracy(naive_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.941\r\n\r\nSimilarly, let’s check the confusion matrix results for our naive bayes classifier.\r\n\r\n\r\nconf_mat(naive_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n          Truth\r\nPrediction    No   Yes\r\n       No  22391   252\r\n       Yes  1148   120\r\n\r\nNote that the model no longer always predict policyholders do not claim from their travel insurance.\r\nWhen we plot the ROC curve, we noted that the ROC curve no longer lie on the diagonal line. This suggests that although the accuracy of this fitted model is lower than logistic regression, this fitted model does have some levels of predictive power.\r\n\r\n\r\nautoplot(roc_curve(naive_pred, \r\n                   truth = Claim, \r\n                   estimate = .pred_No))\r\n\r\n\r\n\r\n\r\nOverall, we can see that naive bayes classifier performs better than logistic regression in this data.\r\nOne of the possible reason why naive bayes classifier outperforms logistic regression is due to the small dataset. (Ng and Jordan) discussed that how the model performance for logistic regression will outperform naive bayes classifier when the training size reaches infinity.\r\nHowever, for this scenario, the dataset seems to be too small for logistic regression to converge and outperform naive bayes classifier. The performance of logistic regression model might improve further when we increase the training data.\r\nConclusion\r\nThat’s all for the day! We have finished “sorting” the policies in whether they are expected to claim or does not claim.\r\n\r\n\r\n\r\nPhoto by emrecan arık on Unsplash\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page if you want to find out more on naive bayes classifier.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nBrownlee, Jason, ed. n.d. “8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset.” https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/.\r\n\r\n\r\nKuhn, Max, and Kjell Johnson, eds. 2013. Applied Predictive Modeling. Springer.\r\n\r\n\r\nNg, Andrew, and Michael Jordan, eds. “On Discriminative Vs. Genererative Classifiers: A Comparison of Logistic Regression and Naive Bayes.” http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf.\r\n\r\n\r\nOttesen, Christopher. 2017. “Comparison Between Naïve Bayes and Logistic Regression.” https://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/.\r\n\r\n\r\nscikit-learn, ed. “1.9 Naive Bayes.” https://scikit-learn.org/stable/modules/naive_bayes.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-31-naive-bayes/image/balance.jpg",
    "last_modified": "2021-09-04T22:45:44+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-23-model-stacking/",
    "title": "Model Stacking",
    "description": "When the sum of all component is greater than individual component",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-05-23",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Ensemble method?\r\nSo, how does stacking work?\r\nWhy is this method popular?\r\nStacks R Package\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Splitting\r\nDefine Recipe\r\nFitting Individual Machine Learning Models\r\nStack the Models!\r\nCompare the Model Performance\r\nPerformance Metric of Individual Models\r\nPerformance Metric of Stack Model\r\nSummary\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Crissy Jarvis on Unsplash\r\nIn this series of modeling, I will be exploring model stacking.\r\nIt has gained popularity especially in data science competition as such method could further improve the accuracy of the machine learning models.\r\nBefore jumping straight into model stacking, let’s understand what is ensemble method.\r\nWhat is Ensemble method?\r\n\r\n\r\n\r\nExtracted from vas3k blog (vas3k)\r\nEnsemble method is one of the machine learning method.\r\n(Geron 2019) described that ensemble method is similar to aggregate the answer of a complex question from thousands of random people. This anchored on the idea of wisdom of the crowd. Often this approach would produce a better answer than an answer from an expert.\r\nSuch method can be either applied on the same machine learning algorithms or aggregate across different machine learning algorithms.\r\nAs the graph shown above, following are the three common ensemble methods:\r\nBagging: Different models are trained by using different dataset randomly drawn with replacement from the training dataset. The final prediction is the average of predictions from all the fitted models. Random forest is one of the algorithm that leverages on bagging to improve the model accuracy.\r\nBoosting: The idea is to a combination of weak learners could form a strong learner. XGBoost is one commonly used model algorithm that uses boosting method.\r\nStacking: As the name suggested, we “stack” the models on one another and create a new model.\r\nIn this post, I will be exploring model stacking and mainly covering the following topics:\r\nHow does model stacking work?\r\nHow to implement model stacking in R?\r\nSo, how does stacking work?\r\nIf we were to visualize the steps for model stacking, this would be how it would look like:\r\n\r\n\r\n\r\nFollowing are the steps to perform model stacking:\r\nFirst, fit different individual machine learning models\r\nNext, use target variable from the training dataset and the predictions from the different machine learning models as input to fit a model\r\nThe conventional way to stack the model is to use linear model. Although in theory we could use other machine learning algorithm to stack the models, this would increase the complexity of the final model, making it even harder to interpret.\r\nWhy is this method popular?\r\n(Funda Gunes and Tan 2017) discussed in their paper that ensemble method enables the users to average out the noise from the diverse models and thereby enhance the generalizable signal.\r\nTherefore, such method has proven to increase model accuracy in Kaggle competitions. Often, even a slight uplift in improvement could affect our model performance rankings in the Kaggle competitions.\r\nWhile this method does not guarantee an improvement in the accuracy, it is worthwhile to fit such models to attempt to improve the accuracy of the predictions.\r\nHowever, by stacking the different models together, this has increased the complexity of the models, making it even harder to understand on how the model reaches the decisions.\r\nThe time required for model fitting would be longer as well since we would need to fit the base model before perform model stacking.\r\nStacks R Package\r\nOkay, let’s perform some model stacking!\r\n\r\n\r\n\r\nPhoto by La-Rel Easter on Unsplash\r\nIn this demonstration, I will be using a package from tidymodels that allows the users to perform model stacks with just a few lines of codes. This package is called stacks.\r\n\r\n\r\n\r\nstacks package\r\nAnd yes, this package is it conforms to the tidy data concept. It is also designed to work together with the various packages under tidymodels.\r\nWHAT A BIG RELIEF!\r\n\r\n\r\n\r\nPhoto by Andrea Piacquadio from Pexels\r\nThis would make it easier for one to use perform model stacking.\r\nDemonstration\r\nSetup the environment\r\nBefore I start the demonstration, I will setup the environment by calling the necessary packages.\r\n\r\n\r\npackages <- c(\"tidyverse\", \"tidymodels\", \"stacks\")\r\n\r\nfor (p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nI will also define the common parameters that will be used in the analysis below.\r\n\r\n\r\n# Define the random seeds for reproducibility\r\nset.seed(1234)\r\n\r\n# Proportion between training and testing dataset\r\nprop_train_test <- 0.6\r\n\r\n# Define the model performance metrics we would like to output later\r\nmodel_metrics <- metric_set(rmse, rsq)\r\n\r\n# The number of grid to be used in the analysis later\r\ngrid_num <- 5\r\n\r\n\r\n\r\nTo stack model, we need to include following as well. According to the documentation page, this is to ensure the predictions and necessary info are output in the necessary format required by stacks package later.\r\n\r\n\r\nctrl_grid <- control_stack_grid()\r\nctrl_res <- control_stack_resamples()\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nFor this demonstration, I will be using a dataset from Actuarial Loss Prediction Kaggle Competition.\r\nAlso, this sharing will be focusing on the model stacking, instead of end to end process. Hence, I will import the data I have previously cleaned.\r\nRefer to my EDA code file on my Github page for the steps taken to clean the data.\r\n\r\n\r\ndf <- read_csv(\"data/data_eda_actLoss_3.csv\") %>%\r\n  dplyr::select(-c(ClaimNumber,\r\n                   num_week_paid_ult,\r\n                   InitialIncurredClaimCost,\r\n                   UltimateIncurredClaimCost)) %>%\r\n  dplyr::select(-starts_with(c(\"acc\", \"report\"))) %>%\r\n  filter(Gender != \"U\") %>%\r\n  drop_na() %>%\r\n  sample_frac(0.3)\r\n\r\n\r\n\r\nWhile importing the data, I have also:\r\nDrop ‘ClaimNumber,’ ‘num_week_paid_ult’ & column headers that starts with ‘acc’ & ‘report’ since I am not using them later\r\nFilter out gender ‘U’ as according to data dictionary, indicator ‘U’ means missing values\r\nDrop the data with missing values\r\nSample 30% of the data to use for the analysis, otherwise it would take forever to run on my laptop 😢\r\nData Splitting\r\nNext, I will split the dataset into training and testing dataset.\r\n\r\n\r\n# Split dataset\r\ndf_split <- initial_split(df,\r\n                            prop = prop_train_test,\r\n                            strata = init_ult_diff)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nThen, I will prepare the dataset for the k-fold validation later.\r\n\r\n\r\n# Cross validation\r\ndf_folds <- vfold_cv(df_train, strata = init_ult_diff)\r\n\r\n\r\n\r\nDefine Recipe\r\nIn this section, I will define the general recipe for the different machine learning models.\r\n\r\n\r\ngen_recipe <- recipe(init_ult_diff ~ ., data = df_train) %>%\r\n  update_role(c(DateTimeOfAccident, DateReported, ClaimDescription), new_role = \"id\") %>% # update the roles of original date variables to \"id\"\r\n  prep()\r\n\r\n\r\n\r\nBasically, I try to perform the following in the code chunk above:\r\nExtract the date information from date variable by using step_date & step_mutate functions\r\nConvert the extracted date features into ordinal variables by using factor function\r\nUpdate the roles for ‘DateTimeOfAccident’ & ‘Date Reported’ to ‘id’ so that they will not be used in fitting the model\r\nFitting Individual Machine Learning Models\r\nBefore fitting a stack model, let’s fit individual models first.\r\nRandom Forest\r\n\r\n\r\nranger_spec <- \r\n  rand_forest() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"ranger\", importance = \"impurity\")\r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(gen_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\nset.seed(51107)\r\nranger_tune <-\r\n  tune_grid(ranger_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\nxgboost_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal())\r\n\r\nxgboost_spec <- \r\n  boost_tree() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"xgboost\") \r\n\r\nxgboost_workflow <- \r\n  workflow() %>% \r\n  \r\n  add_recipe(xgboost_recipe) %>% \r\n  add_model(xgboost_spec) \r\n\r\nset.seed(12071)\r\nxgboost_tune <-\r\n  tune_grid(xgboost_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\nearth_recipe <- gen_recipe %>% \r\n  step_novel(all_nominal(), -all_outcomes()) %>% \r\n  step_dummy(all_nominal(), -all_outcomes()) %>% \r\n  step_zv(all_predictors()) \r\n\r\nearth_spec <- \r\n  mars() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"earth\") \r\n\r\nearth_workflow <- \r\n  workflow() %>% \r\n  add_recipe(earth_recipe) %>% \r\n  add_model(earth_spec) \r\n\r\nearth_grid <- tidyr::crossing(num_terms = 2 * (1:6), prod_degree = 1:2) \r\n\r\nearth_tune <- \r\n  tune_grid(earth_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nStack the Models!\r\nOkay, we have fitted all the individual models. I will move on and fit a stack model.\r\nTo do so, I will use stack function to create an empty stack model and add on the individual fitted models earlier on.\r\n\r\n\r\nstack_model <-\r\n  stacks() %>%\r\n  add_candidates(ranger_tune) %>%\r\n  add_candidates(xgboost_tune) %>%\r\n  add_candidates(earth_tune)\r\n\r\n\r\n\r\nNext, I will use blend_predictions function to find the coefficient of individual models.\r\n\r\n\r\nstack_model_pred <-\r\n  stack_model %>%\r\n  blend_predictions()\r\n\r\n\r\n\r\nTo see the weights of the models, we can call the object we have created in the previous step.\r\n\r\n\r\nstack_model_pred\r\n\r\n\r\n# A tibble: 3 x 3\r\n  member           type        weight\r\n  <chr>            <chr>        <dbl>\r\n1 xgboost_tune_1_1 boost_tree   0.477\r\n2 ranger_tune_1_1  rand_forest  0.475\r\n3 earth_tune_1_1   mars         0.158\r\n\r\nAs the results shown above, random forest, xgboost and mars are used to create the stacked model.\r\nWe can also use the autoplot function to visualize the weight for different models.\r\n\r\n\r\nautoplot(stack_model_pred, type = \"weights\")\r\n\r\n\r\n\r\n\r\nOnce the weights of the different models are determined, I will prepare the fitted stack model by using fit_members function.\r\n\r\n\r\nstack_model_fit <- stack_model_pred %>%\r\n  fit_members()\r\n\r\n\r\n\r\nCompare the Model Performance\r\nPerformance Metric of Individual Models\r\nTo compare the model performance from different models, I will do the following:\r\nFirst, finalize the workflow by selecting the best parameters & perform last_fit on the model\r\nThen, collect the model predictions by using collect_predictions\r\nLastly, calculate the model performance, create a column named ‘model’ and pivot the columns\r\nRanger metrics\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nranger_fit <- ranger_workflow %>%\r\n  finalize_workflow(select_best(ranger_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nranger_pred <- ranger_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nranger_metric <- model_metrics(ranger_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"ranger\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nxgboost_fit <- xgboost_workflow %>%\r\n  finalize_workflow(select_best(xgboost_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nxgboost_pred <- xgboost_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nxgboost_metric <- model_metrics(xgboost_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"xgboost\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nearth_fit <- earth_workflow %>%\r\n  finalize_workflow(select_best(earth_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nearth_pred <- earth_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nearth_metric <- model_metrics(earth_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"earth\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nPerformance Metric of Stack Model\r\nNext, I will calculate the model performance for the stack model.\r\n\r\n\r\nstack_metric <- predict(stack_model_fit, df_test) %>%\r\n  bind_cols(init_ult_diff = df_test$init_ult_diff) %>%\r\n  model_metrics(truth = init_ult_diff, estimate = .pred) %>%\r\n  mutate(model = \"stack\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nSummary\r\nNow I will combine all the results in a tibble table so that its easier to compare the performance metrics from different models.\r\n\r\n\r\ntibble() %>%\r\n  bind_rows(stack_metric) %>%\r\n  bind_rows(ranger_metric) %>%\r\n  bind_rows(xgboost_metric) %>%\r\n  bind_rows(earth_metric)\r\n\r\n\r\n# A tibble: 4 x 4\r\n  .estimator model    rmse   rsq\r\n  <chr>      <chr>   <dbl> <dbl>\r\n1 standard   stack   1547. 0.437\r\n2 standard   ranger  1580. 0.416\r\n3 standard   xgboost 1563. 0.427\r\n4 standard   earth   1662. 0.356\r\n\r\nAs shown above, both model performance metrics improved after we stacked the models.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page if you want to find out more on model stacking.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nFunda Gunes, Russ Wolfinger, and Pei-Yi Tan, eds. 2017. “Stacked Ensemble Models for Improved Prediction Accuracy.” http://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf.\r\n\r\n\r\nGeron, Aurelien, ed. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media.\r\n\r\n\r\nvas3k, ed. “Machine Learning for Everyone - in Simple Words. With Real-World Examples. Yes, Again.” https://vas3k.com/blog/machine_learning/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-23-model-stacking/image/cup_stack.jpg",
    "last_modified": "2021-09-04T21:42:23+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-22-data-wrangling/",
    "title": "Data Wrangling - The Quest to Tame The 'Beast'",
    "description": "Rawwwwwwwwww!",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "Data Wrangling"
    ],
    "contents": "\r\n\r\n\r\n\r\nArtwork on Data Wrangling by Allison Horst\r\nAs mentioned in my last post, it has been a long journey to learn data science pretty much from scratch. I intend to share some of my learning to help others to start their data science journey as well.\r\nTherefore, for my second post, I will share about data wrangling, which is often the essential task before building any machine learning models. Hence this post will serve as a building block of my future machine learning posts.\r\nAlso, I intend to share some of the modern data science R package and how these packages could benefit some of the analysis tasks.\r\nBackground\r\nIn R for Data Science book, Wickham & Grolemund mentioned that a typical data science project would look as following:\r\n\r\n\r\n\r\nExtracted from R for Data Science\r\nAfter importing the data, the very first step usually involves some levels of data wrangling. This is probably one of the most important steps as without a good dataset, the analysis is unlikely to be meaningful.\r\nGolden rule –> Garbage in, garbage out\r\nA good data exploratory will help to answer the following questions (not limited to):  - Does this dataset answer the business questions (eg. does the dataset have the necessary info to predict which customers are likely to purchase in the next promotion?)?  - Are there any issues that exist in the dataset (eg. missing value, excessive categories, value range vary widely)?  - Any data transformation required? As some of the machine learning models can accept certain data types. \r\nThrough understanding the data, it will also give us some insights on whether there is any more “gold” we can squeeze out from data through feature engineering.\r\nIt will also provide us some clues whether the variable is a good predictor even without needing one to fit the model.\r\nThe conventional method of data wrangling is to perform data cleaning and transformation in Excel.\r\n\r\n\r\n\r\nPhoto by Mika Baumeister on Unsplash\r\nHowever, this method is not very sustainable/ideal due to the following reasons:  - Excel has memory limitation and the software would keep on crashing as the data size increases  - Become a hurdle when we want to join different tables together or perform a transformation on the data  - More challenging to extract insights from the unstructured data (eg. perform text mining) \r\nIntroduction to Tidyverse\r\nWhen my professor first introduced this R package to me, I was amazed by what we could perform by using this package. And in fact, the more I use it, the more I love it.\r\ntidyverse is a collection of R packages designed by RStudio.\r\n\r\n\r\n\r\nThe very awesome thing about tidyverse is all of the packages are following the tidy data concept.\r\nSo what the heck is tidy data?\r\nTidy data is a concept on how to store/structure the data/results introduced by Hadley Wickham.\r\n\r\n\r\n\r\nArtwork on Tidy Data by Allison Horst\r\nBelow are the definition of tidy data:  - Each variable must have its column  - Each observation must have its row  - Each value must have its cell \r\n\r\n\r\n\r\nExtraction from R for Data Science book\r\nWhy is tidy data awesome?\r\nThis is as simple as the time spent transforming the dataset/output is now lesser.\r\nWhy less time is required?\r\nThe functions are aligned on the input data formats they require.\r\nOften this allows us to ‘join’ the function together (also often known as “pipe” in tidyverse context. Don’t worry about the meaning of “pipe” for now, will be covering what I mean by “pipe” later in this post).\r\nTherefore, as a user, I find that the learning curve of using tidyverse to perform data wrangling is less steep than other programming languages. This has effectively allowed me to spend more time on the analysis itself.\r\n\r\n\r\n\r\nArtwork by Allison Horst\r\nEnough talking, let’s get our hands dirty.\r\nIllustration\r\nI will be using the motor insurance dataset taken from CASdataset as a demonstration. In this illustration, I am interested to find out how the different profiles of the insured affected the total premium.\r\nPreparation of the “equipment”\r\nOver here, I call the necessary R packages by using a for-loop function.\r\n\r\n\r\npackage <- c('CASdatasets', 'tidyverse', 'skimr', 'funModeling', 'ggplot2', 'plotly', 'ggstatsplot')\r\n\r\nfor (p in package){\r\n  if(!require (p, character.only = TRUE)){\r\n    install(p)\r\n  }\r\n  library(p, character.only = TRUE)\r\n}\r\n\r\n\r\n\r\nThe code chunk will first check whether the relevant packages are installed in the machine.\r\nIf it is not installed, it will first install the required package.\r\nAfter that, the relevant packages will be attached to the environment.\r\nCalling the “beast”\r\nAfter loading the relevant packages, I will indicate to R the dataset I want through data function as there are many datasets within CASdatasets package. I have renamed the dataset to df to shorten the name after attaching the dataset into the environment.\r\n\r\n\r\ndata(fremotor1prem0304a)\r\ndf <- fremotor1prem0304a\r\n\r\n\r\n\r\nNow, tame the “beast”\r\nConventionally, summary function is used to check the quality of the data.\r\n\r\n\r\nsummary(df)\r\n\r\n\r\n    IDpol                Year         DrivAge      DrivGender\r\n Length:51949       Min.   :2003   Min.   :18.00   F:17794   \r\n Class :character   1st Qu.:2003   1st Qu.:31.00   M:34155   \r\n Mode  :character   Median :2003   Median :38.00             \r\n                    Mean   :2003   Mean   :39.84             \r\n                    3rd Qu.:2004   3rd Qu.:47.00             \r\n                    Max.   :2004   Max.   :97.00             \r\n                                                             \r\n    MaritalStatus     BonusMalus       LicenceNb    \r\n Cohabiting:10680   Min.   : 50.00   Min.   :1.000  \r\n Divorced  :  189   1st Qu.: 50.00   1st Qu.:1.000  \r\n Married   : 3554   Median : 57.00   Median :2.000  \r\n Single    : 2037   Mean   : 62.98   Mean   :1.885  \r\n Widowed   :  541   3rd Qu.: 72.00   3rd Qu.:2.000  \r\n NA's      :34948   Max.   :156.00   Max.   :7.000  \r\n                                                    \r\n        PayFreq                  JobCode          VehAge      \r\n Annual     :17579   Private employee: 9147   Min.   : 0.000  \r\n Half-yearly:28978   Public employee : 5045   1st Qu.: 4.000  \r\n Monthly    : 1486   Retiree         : 1035   Median : 7.000  \r\n Quarterly  : 3906   Other           :  856   Mean   : 7.525  \r\n                     Craftsman       :  637   3rd Qu.:10.000  \r\n                     (Other)         :  281   Max.   :89.000  \r\n                     NA's            :34948                   \r\n        VehClass        VehPower        VehGas     \r\n Cheapest   :17894   P10    :9148   Diesel :20615  \r\n Cheaper    :15176   P12    :8351   Regular:31334  \r\n Cheap      : 9027   P11    :8320                  \r\n Medium low : 5566   P9     :6671                  \r\n Medium     : 2021   P13    :6605                  \r\n Medium high: 1385   P8     :5188                  \r\n (Other)    :  880   (Other):7666                  \r\n                   VehUsage                           Garage     \r\n Private+trip to office:50435   Closed collective parking: 9820  \r\n Professional          : 1089   Closed zbox              :26318  \r\n Professional run      :  425   Opened collective parking: 8620  \r\n                                Street                   : 7191  \r\n                                                                 \r\n                                                                 \r\n                                                                 \r\n      Area                Region      Channel   Marketing \r\n A5     :15108   Center      :27486   A:30220   M1:26318  \r\n A3     :12696   Headquarters: 9788   B: 6111   M2: 8620  \r\n A2     : 7414   Paris area  : 7860   L:15618   M3: 9820  \r\n A7     : 6879   South West  : 6815             M4: 7191  \r\n A4     : 3779                                            \r\n A9     : 3397                                            \r\n (Other): 2676                                            \r\n PremWindscreen     PremDamAll         PremFire         PremAcc1    \r\n Min.   :  0.00   Min.   :   0.00   Min.   : 0.000   Min.   : 0.00  \r\n 1st Qu.: 13.00   1st Qu.:   0.00   1st Qu.: 0.000   1st Qu.: 0.00  \r\n Median : 22.00   Median :   0.00   Median : 4.000   Median : 0.00  \r\n Mean   : 25.78   Mean   :  83.06   Mean   : 4.421   Mean   :12.94  \r\n 3rd Qu.: 35.00   3rd Qu.: 144.00   3rd Qu.: 7.000   3rd Qu.:32.00  \r\n Max.   :264.00   Max.   :1429.00   Max.   :50.000   Max.   :77.00  \r\n                                                                    \r\n    PremAcc2       PremLegal        PremTPLM         PremTPLV     \r\n Min.   :  0.0   Min.   : 0.00   Min.   :  38.9   Min.   : 0.000  \r\n 1st Qu.:  0.0   1st Qu.: 8.00   1st Qu.: 102.6   1st Qu.: 5.000  \r\n Median :  0.0   Median :10.00   Median : 141.5   Median : 7.000  \r\n Mean   : 15.4   Mean   :10.43   Mean   : 167.7   Mean   : 8.571  \r\n 3rd Qu.: 45.0   3rd Qu.:12.00   3rd Qu.: 204.8   3rd Qu.:11.000  \r\n Max.   :198.0   Max.   :50.00   Max.   :1432.7   Max.   :68.000  \r\n                                                                  \r\n    PremServ        PremTheft         PremTot      \r\n Min.   :  0.00   Min.   :  0.00   Min.   :  91.0  \r\n 1st Qu.: 51.00   1st Qu.:  0.00   1st Qu.: 269.7  \r\n Median : 53.00   Median : 38.00   Median : 381.4  \r\n Mean   : 53.77   Mean   : 46.58   Mean   : 428.7  \r\n 3rd Qu.: 57.00   3rd Qu.: 68.00   3rd Qu.: 530.4  \r\n Max.   :237.00   Max.   :642.00   Max.   :3163.3  \r\n                                                   \r\n\r\nHowever, the data quality result shown under summary function is often not sufficient. Instead of just looking at the quantile, it’s also important to check other information, such as there is any missing value, what is the distribution of the variable look like, what is coefficient variation, and so on.\r\nBelow are three examples of modern R functions to check the data quality:\r\nskim\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n30\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nfactor\r\n13\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nIDpol\r\n0\r\n1\r\n5\r\n13\r\n0\r\n32117\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nDrivGender\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 34155, F: 17794\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nPayFreq\r\n0\r\n1.00\r\nFALSE\r\n4\r\nHal: 28978, Ann: 17579, Qua: 3906, Mon: 1486\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\nVehClass\r\n0\r\n1.00\r\nFALSE\r\n9\r\nChe: 17894, Che: 15176, Che: 9027, Med: 5566\r\nVehPower\r\n0\r\n1.00\r\nFALSE\r\n15\r\nP10: 9148, P12: 8351, P11: 8320, P9: 6671\r\nVehGas\r\n0\r\n1.00\r\nFALSE\r\n2\r\nReg: 31334, Die: 20615\r\nVehUsage\r\n0\r\n1.00\r\nFALSE\r\n3\r\nPri: 50435, Pro: 1089, Pro: 425\r\nGarage\r\n0\r\n1.00\r\nFALSE\r\n4\r\nClo: 26318, Clo: 9820, Ope: 8620, Str: 7191\r\nArea\r\n0\r\n1.00\r\nFALSE\r\n10\r\nA5: 15108, A3: 12696, A2: 7414, A7: 6879\r\nRegion\r\n0\r\n1.00\r\nFALSE\r\n4\r\nCen: 27486, Hea: 9788, Par: 7860, Sou: 6815\r\nChannel\r\n0\r\n1.00\r\nFALSE\r\n3\r\nA: 30220, L: 15618, B: 6111\r\nMarketing\r\n0\r\n1.00\r\nFALSE\r\n4\r\nM1: 26318, M3: 9820, M2: 8620, M4: 7191\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nprofiling_num\r\n\r\n\r\nprofiling_num(df)\r\n\r\n\r\n         variable        mean     std_dev variation_coef   p_01\r\n1            Year 2003.381759   0.4858226   0.0002425013 2003.0\r\n2         DrivAge   39.835146  11.8652020   0.2978576243   21.0\r\n3      BonusMalus   62.983330  15.3002405   0.2429252393   50.0\r\n4       LicenceNb    1.884733   0.6664325   0.3535951452    1.0\r\n5          VehAge    7.525477   4.7817572   0.6354091975    0.0\r\n6  PremWindscreen   25.779053  20.4951432   0.7950308962    0.0\r\n7      PremDamAll   83.055285 105.7145996   1.2728220676    0.0\r\n8        PremFire    4.421317   4.4846822   1.0143317368    0.0\r\n9        PremAcc1   12.942001  16.7326001   1.2928912922    0.0\r\n10       PremAcc2   15.400219  23.8030155   1.5456283309    0.0\r\n11      PremLegal   10.426399   3.9025924   0.3742991579    4.0\r\n12       PremTPLM  167.737897  96.9041558   0.5777117614   51.8\r\n13       PremTPLV    8.570521   5.2053077   0.6073501958    0.0\r\n14       PremServ   53.770852   5.1332147   0.0954646327   42.0\r\n15      PremTheft   46.584169  48.7219715   1.0458911785    0.0\r\n16        PremTot  428.687713 224.5838408   0.5238868156  129.1\r\n     p_05   p_25   p_50   p_75   p_95     p_99  skewness  kurtosis\r\n1  2003.0 2003.0 2003.0 2004.0 2004.0 2004.000 0.4867707  1.236946\r\n2    24.0   31.0   38.0   47.0   63.0   73.000 0.8090189  3.339693\r\n3    50.0   50.0   57.0   72.0   93.0  106.000 1.1457567  3.756333\r\n4     1.0    1.0    2.0    2.0    3.0    4.000 0.9380943  5.925007\r\n5     1.0    4.0    7.0   10.0   15.0   21.000 1.5128291 15.246811\r\n6     0.0   13.0   22.0   35.0   65.0   92.000 1.5801010  8.575197\r\n7     0.0    0.0    0.0  144.0  283.0  420.000 1.5981568  7.474360\r\n8     0.0    0.0    4.0    7.0   13.0   20.000 1.7369281  9.079772\r\n9     0.0    0.0    0.0   32.0   39.0   44.000 0.5764758  1.461663\r\n10    0.0    0.0    0.0   45.0   57.0   64.000 0.9548824  2.088081\r\n11    5.0    8.0   10.0   12.0   17.0   23.000 1.3283374  6.830506\r\n12   68.0  102.6  141.5  204.8  352.0  512.100 2.1710206 11.739705\r\n13    3.0    5.0    7.0   11.0   18.0   26.000 1.9178789 10.133990\r\n14   46.0   51.0   53.0   57.0   62.0   68.000 1.8713567 51.639566\r\n15    0.0    0.0   38.0   68.0  136.0  216.000 1.9999025 11.271121\r\n16  169.8  269.7  381.4  530.4  852.3 1187.504 1.7016729  8.533766\r\n     iqr          range_98         range_80\r\n1    1.0      [2003, 2004]     [2003, 2004]\r\n2   16.0          [21, 73]         [27, 56]\r\n3   22.0         [50, 106]         [50, 85]\r\n4    1.0            [1, 4]           [1, 3]\r\n5    6.0           [0, 21]          [2, 14]\r\n6   22.0           [0, 92]          [0, 52]\r\n7  144.0          [0, 420]         [0, 221]\r\n8    7.0           [0, 20]          [0, 10]\r\n9   32.0           [0, 44]          [0, 36]\r\n10  45.0           [0, 64]          [0, 53]\r\n11   4.0           [4, 23]          [6, 15]\r\n12 102.2     [51.8, 512.1]    [78.8, 290.8]\r\n13   6.0           [0, 26]          [4, 15]\r\n14   6.0          [42, 68]         [48, 60]\r\n15  68.0          [0, 216]         [0, 106]\r\n16 260.7 [129.1, 1187.504] [199.48, 713.12]\r\n\r\nstatus\r\n\r\n\r\nstatus(df)\r\n\r\n\r\n                     variable q_zeros      p_zeros  q_na      p_na\r\nIDpol                   IDpol       0 0.0000000000     0 0.0000000\r\nYear                     Year       0 0.0000000000     0 0.0000000\r\nDrivAge               DrivAge       0 0.0000000000     0 0.0000000\r\nDrivGender         DrivGender       0 0.0000000000     0 0.0000000\r\nMaritalStatus   MaritalStatus       0 0.0000000000 34948 0.6727367\r\nBonusMalus         BonusMalus       0 0.0000000000     0 0.0000000\r\nLicenceNb           LicenceNb       0 0.0000000000     0 0.0000000\r\nPayFreq               PayFreq       0 0.0000000000     0 0.0000000\r\nJobCode               JobCode       0 0.0000000000 34948 0.6727367\r\nVehAge                 VehAge    1712 0.0329553986     0 0.0000000\r\nVehClass             VehClass       0 0.0000000000     0 0.0000000\r\nVehPower             VehPower       0 0.0000000000     0 0.0000000\r\nVehGas                 VehGas       0 0.0000000000     0 0.0000000\r\nVehUsage             VehUsage       0 0.0000000000     0 0.0000000\r\nGarage                 Garage       0 0.0000000000     0 0.0000000\r\nArea                     Area       0 0.0000000000     0 0.0000000\r\nRegion                 Region       0 0.0000000000     0 0.0000000\r\nChannel               Channel       0 0.0000000000     0 0.0000000\r\nMarketing           Marketing       0 0.0000000000     0 0.0000000\r\nPremWindscreen PremWindscreen    7153 0.1376927371     0 0.0000000\r\nPremDamAll         PremDamAll   26087 0.5021655855     0 0.0000000\r\nPremFire             PremFire   14450 0.2781574236     0 0.0000000\r\nPremAcc1             PremAcc1   32183 0.6195114439     0 0.0000000\r\nPremAcc2             PremAcc2   36379 0.7002829698     0 0.0000000\r\nPremLegal           PremLegal       2 0.0000384993     0 0.0000000\r\nPremTPLM             PremTPLM       0 0.0000000000     0 0.0000000\r\nPremTPLV             PremTPLV    1353 0.0260447747     0 0.0000000\r\nPremServ             PremServ       2 0.0000384993     0 0.0000000\r\nPremTheft           PremTheft   14654 0.2820843520     0 0.0000000\r\nPremTot               PremTot       0 0.0000000000     0 0.0000000\r\n               q_inf p_inf      type unique\r\nIDpol              0     0 character  32117\r\nYear               0     0   numeric      2\r\nDrivAge            0     0   numeric     72\r\nDrivGender         0     0    factor      2\r\nMaritalStatus      0     0    factor      5\r\nBonusMalus         0     0   numeric     68\r\nLicenceNb          0     0   numeric      7\r\nPayFreq            0     0    factor      4\r\nJobCode            0     0    factor      7\r\nVehAge             0     0   numeric     49\r\nVehClass           0     0    factor      9\r\nVehPower           0     0    factor     15\r\nVehGas             0     0    factor      2\r\nVehUsage           0     0    factor      3\r\nGarage             0     0    factor      4\r\nArea               0     0    factor     10\r\nRegion             0     0    factor      4\r\nChannel            0     0    factor      3\r\nMarketing          0     0    factor      4\r\nPremWindscreen     0     0   numeric    177\r\nPremDamAll         0     0   numeric    612\r\nPremFire           0     0   numeric     49\r\nPremAcc1           0     0   numeric     45\r\nPremAcc2           0     0   numeric     63\r\nPremLegal          0     0   numeric     43\r\nPremTPLM           0     0   numeric   1097\r\nPremTPLV           0     0   numeric     58\r\nPremServ           0     0   numeric     70\r\nPremTheft          0     0   numeric    377\r\nPremTot            0     0   numeric   9140\r\n\r\nThese functions show more info than just quantile. They also show info such as:  - Number of unique categories under categorical variables  - The proportion of missing values in the data  - Standard deviation of the numeric variables \r\nBelow are some insights we could draw from the functions above: \r\nExcessive missing value (i.e. more than half of the values are missing) in MaritalStatus & JobCode. More than 65% of the data from these two columns have missing values. So, it doesn’t like the variables will yield meaningful results if they are used to build machine learning models \r\n\r\n\r\ndf %>% dplyr::select(MaritalStatus, JobCode) %>% skim()\r\n\r\n\r\nTable 2: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n2\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\n\r\nAbout 15 unique categories under VehPower. Might not ideal to keep so many unique categories within the variables as it might create noise when fitting machine learning models \r\n\r\n\r\ndf %>% group_by(VehPower) %>% tally()\r\n\r\n\r\n# A tibble: 15 x 2\r\n   VehPower     n\r\n   <fct>    <int>\r\n 1 P10       9148\r\n 2 P11       8320\r\n 3 P12       8351\r\n 4 P13       6605\r\n 5 P14       3773\r\n 6 P15       1510\r\n 7 P16        720\r\n 8 P17         58\r\n 9 P2          15\r\n10 P4          46\r\n11 P5         597\r\n12 P6           2\r\n13 P7         945\r\n14 P8        5188\r\n15 P9        6671\r\n\r\nAlso, note that the count of some categories is relatively low. Perhaps it is better to group these ‘low count categories’ since they are unlikely to have a significant impact on the predicted results.\r\nMaximum BonusMalus can go up to 156. According to the data dictionary, <100 means bonus, >100 means malus. Hence, this is okay. \r\n\r\n\r\ndf %>% dplyr::select(BonusMalus) %>% skim()\r\n\r\n\r\nTable 3: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.3\r\n50\r\n50\r\n57\r\n72\r\n156\r\n▇▂▁▁▁\r\n\r\nInterestingly, the premium for different benefits can range very widely. This is probably due to the difference in the respective burning cost under each benefit. \r\n\r\n\r\ndf %>% dplyr::select(starts_with(\"Prem\")) %>% skim()\r\n\r\n\r\nTable 4: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nI prefer skim function as its output is in a nice & neat format. It also provides most of the crucial info one is looking for in checking the data quality.\r\nIf I am only interested in checking the data quality for the numeric variables, this can be found by piping the variables together.\r\nFirst, I have specified the dataset. Next, I use select_if function to select all the numeric columns before using skim function to skim through the dataset.\r\n\r\n\r\ndf %>% \r\n  select_if(is.numeric) %>%\r\n  skim()\r\n\r\n\r\nTable 5: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nThe piping method is made possible because the functions are following the tidy data concept, which makes it easier to “play” with the data.\r\nOften, visualizing data might provide us unexpected insights. The data can “tell” us underlying/hidden stories by leveraging on the power of data visualization.\r\nFor example, I would like to find out the scatterplot between PremTot against the selected numeric variables.\r\n\r\n\r\nnum_list <- c(\"Year\", \"DrivAge\", \"BonusMalus\", \"LicenceNb\", \"VehAge\")\r\n\r\n\r\nfor (i in num_list){\r\n  print(ggplot(df, aes(x = get(i), y = log(PremTot))) +\r\n      geom_point() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nThe graphs show that the data type for Year and LicenceNb is incorrect. They are supposed to read as factors, instead of numeric variables.\r\nTo fix this, I will use mutate and factor function to transform the data into the correct data type.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(Year = factor(Year),\r\n         LicenceNb = factor(LicenceNb))\r\n\r\n\r\n\r\nAlternatively, sometimes we just want to find out the graphs of a list of variables, instead of a small subset of variables. By typing down all the variables names can be a hassle and prone to human error.\r\nThis is where the different R functions come to the “rescue”.\r\nFor example, I am interested to find out the boxplot of all the factor variables against PremTot. The code chunk below has shown how we could pipe the different functions together to select all the factor columns and extract out the column names as a list.\r\n\r\n\r\ncat_list <- df_1 %>%\r\n  select_if(is.factor) %>%\r\n  names()\r\n\r\n\r\n\r\nSubsequently, I will use for loop to plot the necessary graphs. I will explain the awesome-ness of ggplot function in my future post.\r\n\r\n\r\nfor (i in cat_list){\r\n  print(ggplot(df_1, aes(x = get(i), y = log(PremTot))) +\r\n      geom_boxplot() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nBelow are some of the insights drawn from the graphs above (not limited to):  - Average PremTot for Retailer is higher than the rest  - Average PremTot also varies significantly across different VehPower  - Somehow the premium for a diesel car is higher than a regular car  - PremTot for professional & professional run is higher than private+trip to office \r\nConclusion\r\nOkay, that’s all the sharing for this post!\r\nI have shown the awesome-ness of tidyverse through this post. There are many more functions within tidyverse universe, which are not covered in this post. Do check out their website for many more awesome functions!\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-22-data-wrangling/image/data_cowboy.png",
    "last_modified": "2021-09-02T01:31:08+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-my-data-science-journey/",
    "title": "The Unconventional Path",
    "description": "Journey of Joining the \"Dark\" Side",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nPhoto taken in Taipei\r\nDuring a stormy night in 2018, it was raining cats and dogs outside. Suddenly there was thunder and lightning. Jeng jeng jeng, I suddenly decided to take up a master degree to study data science.\r\nNah, I always wish it was that simple to make such a big decision in life.\r\nIt took me about 5 years to decide that I would go back to university for a data science degree.\r\nIt took me 2.5 years to have the courage in taking the leap to go for a change in job function for the 1st time. \r\nWas I worried or scared when I made these choice? Oh absolutely.\r\nUnconventional Path - Work After working on actuarial pricing for more than 4 years, I have asked for a change in job function, which I have chickened out twice times after the requests. There were a lot of ‘what-if’ in my head.\r\n“What if I don’t like the new function?”\r\n“What if I struggle to understand the new concepts?”\r\n“What if I screwed up?”\r\n“What if I get along with them?”\r\nHowever, on the third time, I came to the conclusion that if I don’t take the leap of faith, I would never step out from my comfort zone.\r\nMeanwhile, unfortunately sometimes life is not a bed of roses. Even if it is a bed of roses, it is also a bed of roses with full of thorns.\r\nInitially, things were not as smooth as what I wanted. Team mates left one after another one and things were pretty messy. It was through these tough times I discovered my strength, how I could play my strength and make a difference.\r\nFast forward to 1.5 years later, I was being offered to change my job scope again at my work. However, this change in job scope was even more drastic. I was being offered to change my job scope from life insurance to general insurance.\r\nAm I scared? Still as worried as usual.\r\nHowever, I decided that I would regret if I did not take the leap of faith again. My industry friends were shocked, they even checked with me to verify the news. Deep down I knew that worry does not solve anything. So, I worked my a** hard to put in the extra effort to understand the fundamental concepts for general insurance.\r\nWithin less than 3 months after joining the team, I was asked to cover group insurance while my colleague was on maternity leave. At the moment, I was worried as my main expertise was never in short term insurance business and I was expecting to guide one of the junior on this piece of work. Despite of the hiccups at the beginning, things turned out better than what I initially imagined.\r\nThese past success have nevertheless given me more confidence in myself.\r\nUnconventional Path - Data Science Journey I first discovered my interest for data science back in my undergraduate studies. I still remember it was generalized linear model project on Titanic dataset that sparked my interest.\r\n\r\n\r\n\r\nScreenshot of my GLM report\r\nAfter I started working, I never forget my interest in this area. However, as life goes on, I was not sure this data science was for me since actuarial science and data science are still different (in terms of concepts and work) although they are somewhat similar.\r\nUntil few years ago, I was once exposed a bit on data science at work again. That re-kindled my interest for data science. I knew that it is not going to work if I rely others for my learning, eg. waiting to learn from work, waiting for others to teach me and so on. Eventually, I decided to go back into university to study data science.\r\nThroughout these processes, I have encountered many “Why” from myself and others.\r\n“Why would you still want to study, given that you have been studying for your actuarial exams after graduating from university?”\r\n“Why are you still studying this when you hold an actuarial science degree?”\r\n“Why are you studying when your working hours can be quite bad?”\r\n“Why are you spending so much money on getting another degree? Are you sure you can earn back the money?”\r\n“Just why??”\r\nI knew it is going to tough and tiring to work full time + study part time, although I was complaining that I did not its going to be this tiring.\r\nLooking back now, did I regret anything? Absolutely no.\r\nIn fact, I met some of my “gui ren” (ie. people who are great help to one’s life) through my this journey. Prof Kam Tin Seong has spent so much time with me guiding me through my capstone project, explaining the concepts of data science techniques (eg. how to do proper clustering, considerations when building a model), making me working very hard for the weekly data visualization makeover and many more. Another of my gui ren has to be Prof Wong Yuet Nan. The valuable advice and experience he shared with me has built up my knowledge and confidence.\r\nWas I stressed during these process? Super duper stressed. I was so so stressed out until there were days my brain couldn’t think or function.\r\nDeep down I knew what are opportunity cost and price I am paying, but I am willing to going through the pain. Sleeping at 2am almost every day, occasionally running back to work after class ended at 10pm and fell asleep at random places due to lack of enough sleep are really nothing when I compared to these valuable experience and confidence I have gained.\r\nWould I trade these experience for anything? Definitely no.\r\nTakeaway Do I have the certainty of what is going to happen next when I was making the above decisions? Nope, often I am not sure how things will work out.\r\nDo I want to have the certainty in these situations? Oh absolutely yes. However, I knew life does not work this way. I would miss out so much fun if I demand on the certainty.\r\nIt was taking these seemed “unconventional paths” that provided me the opportunities to discover amazing things. Just like the the photo shown below, it was taken with no filter during my Japan trip. Instead of walking along the main road like what other tourists were doing, I followed another path that is less taken and discovered this awesome view.\r\n\r\n\r\n\r\nPhoto taken in Hakone, Japan\r\nNevertheless, this is my story of unconventional path.\r\nLife is short, so go & live a life with no regret! :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-my-data-science-journey/image/journey.jpg",
    "last_modified": "2021-01-20T23:23:41+08:00",
    "input_file": {}
  }
]
