[
  {
    "path": "posts/2023-03-26-tweedie-dist/",
    "title": "Tweedie Distribution",
    "description": "Wait, are you referring to Tweety bird?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-03-26",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nParameters\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nModel Building\r\nSmaller Model\r\nModel Performance\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring Tweedie distribution.\r\n\r\n\r\n\r\nPhoto by Dan Cristian Pădureț on Unsplash\r\nWhen I first encountered this distribution many years ago, I have to say the name of the distribution reminds me of a cartoon I used to watch when I was younger…\r\n\r\nTaken from giphy\r\nTweedie distribution is a special case of an exponential distribution (Glen).\r\nIt could generate a distribution with a cluster of data with zero value as shown below.\r\n\r\n\r\n\r\nTaken from this website\r\nThis can be very handy when we want to model insurance claims.\r\nParameters\r\nIn R statmod package, the Tweedie distribution has two arguments:\r\nvar.power\r\nlink.power\r\n\r\n\r\n\r\nTaken from R documentation\r\nAt the point of writing, I can’t seem to find any materials to explain the link.power argument in the function, except link.power = 1 means we are using a log link.\r\nDemonstration\r\nSetup the environment\r\nI will be using the Tweedie distribution in statmod package.\r\n\r\n\r\npacman::p_load(tidyverse, CASdatasets, tidymodels, statmod, ghibli)\r\n\r\n\r\nImport the data\r\nFor this demonstration, I will be using the car insurance dataset from CASdatasets package.\r\n\r\n\r\n\r\nPhoto by Vlad Deep on Unsplash\r\n\r\n\r\ndata(pg17trainpol)\r\ndata(pg17trainclaim)\r\n\r\npol_df <- pg17trainpol\r\nrm(pg17trainpol)\r\n\r\nclm_df <- pg17trainclaim\r\nrm(pg17trainclaim)\r\n\r\n\r\nI will rename the datasets so that the name of the datasets are shorter.\r\nIn this demonstration, I will perform predictions on the claim counts.\r\nTo do so, I will count how many times the insureds have made claims under each policy.\r\n\r\n\r\nclm_cnt <-\r\n  clm_df %>% \r\n  group_by(id_client, id_vehicle) %>% \r\n  summarise(tot_clm_cnt = sum(claim_nb))\r\n\r\n\r\nThen, I will join the claim count with the main dataset before performing any predictions.\r\nFor simplicity, I will also remove any rows with missing values.\r\n\r\n\r\npol_clm_df <-\r\n  pol_df %>% \r\n  left_join(clm_cnt\r\n            ,by = c(\"id_client\", \"id_vehicle\")) %>% \r\n  mutate_at(c(\"tot_clm_cnt\")\r\n            ,function(x) replace_na(x, 0)) %>% \r\n  drop_na()\r\n\r\n\r\nModel Building\r\nNow I will build the GLM model with Tweedie distribution.\r\n\r\n\r\nclm_cnt_glm <-\r\n  glm(tot_clm_cnt ~ \r\n        drv_age1 \r\n      + drv_drv2\r\n      + vh_age\r\n      + pol_bonus\r\n      + pol_coverage\r\n      + pol_usage\r\n      + pol_duration\r\n      + pol_sit_duration\r\n      + pol_pay_freq\r\n      + vh_type\r\n      + vh_speed\r\n      + vh_value\r\n      + vh_weight\r\n      ,family = tweedie(var.power = 1.8\r\n                        ,link.power = 0.8)\r\n      ,data = pol_clm_df\r\n      )\r\n\r\n\r\nI will also run the chi-square test to check the significance of the predictors.\r\n\r\n\r\nanova(clm_cnt_glm, test = \"Chisq\")\r\n\r\nAnalysis of Deviance Table\r\n\r\nModel: Tweedie, link: mu^0.8\r\n\r\nResponse: tot_clm_cnt\r\n\r\nTerms added sequentially (first to last)\r\n\r\n                 Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \r\nNULL                             99998     685490              \r\ndrv_age1          1    316.2     99997     685173 3.591e-13 ***\r\ndrv_drv2          1    201.6     99996     684972 6.414e-09 ***\r\nvh_age            1   6515.3     99995     678457 < 2.2e-16 ***\r\npol_bonus         1    139.0     99994     678318 1.433e-06 ***\r\npol_coverage      3   5380.3     99991     672937 < 2.2e-16 ***\r\npol_usage         2    487.3     99989     672450 < 2.2e-16 ***\r\npol_duration      1     76.4     99988     672374 0.0003506 ***\r\npol_sit_duration  2     68.6     99986     672305 0.0032361 ** \r\npol_pay_freq      3     50.7     99983     672254 0.0371260 *  \r\nvh_type           1      7.4     99982     672247 0.2664458    \r\nvh_speed          1    416.5     99981     671830 < 2.2e-16 ***\r\nvh_value          1    852.7     99980     670978 < 2.2e-16 ***\r\nvh_weight         1    109.0     99979     670869 1.971e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nFrom the result above, it seems like having the vehicle type doesn’t improve the model since the p-value is less than 0.05.\r\nSmaller Model\r\nNext, I will build a smaller model and perform a statistical test to determine which model to use.\r\nAs suggested by (Rodriguez 2023), we will be using F-test to decide which model we should be using.\r\nIn general, to perform an F-test, we will do the following steps (PennState Eberly College of Science):\r\nBuild a full model\r\nThen, build a smaller model\r\nUse F-statistics to guide us on whether we should be accepting the smaller model\r\n\\(H_0\\): Use The Smaller Model\r\n\\(H_\\alpha\\): Use Full Model\r\nWith that, I build a smaller model without the following variables:\r\ndrv_drv2\r\npol_pay_freq\r\nvh_type\r\n\r\n\r\nclm_cnt_glm_smaller <-\r\n  glm(tot_clm_cnt ~ \r\n        drv_age1 \r\n      + vh_age\r\n      + pol_bonus\r\n      + pol_coverage\r\n      + pol_usage\r\n      + pol_duration\r\n      + pol_sit_duration\r\n      + vh_speed\r\n      + vh_value\r\n      + vh_weight\r\n      ,family = tweedie(var.power = 1.8,link.power = 0.8)\r\n      ,data = pol_clm_df\r\n      )\r\n\r\n\r\nNext, I will pass the models into the anova function and indicate I would like to perform a likelihood ratio test.\r\n\r\n\r\nanova(clm_cnt_glm_smaller\r\n      ,clm_cnt_glm\r\n      ,test = 'F')\r\n\r\nAnalysis of Deviance Table\r\n\r\nModel 1: tot_clm_cnt ~ drv_age1 + vh_age + pol_bonus + pol_coverage + \r\n    pol_usage + pol_duration + pol_sit_duration + vh_speed + \r\n    vh_value + vh_weight\r\nModel 2: tot_clm_cnt ~ drv_age1 + drv_drv2 + vh_age + pol_bonus + pol_coverage + \r\n    pol_usage + pol_duration + pol_sit_duration + pol_pay_freq + \r\n    vh_type + vh_speed + vh_value + vh_weight\r\n  Resid. Df Resid. Dev Df Deviance      F    Pr(>F)    \r\n1     99985     671048                                 \r\n2     99979     670869  6   179.16 4.9916 4.025e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThe p-value is less than 0.05 and we will reject the null hypothesis. There is a statistical signification linear association between claim count and additional parameters.\r\nModel Performance\r\nIn this next section, I will check the model performance.\r\nBefore that, I will use augment function to generate the prediction.\r\nThe beauty of this function is that it could help us to append the predictions to the dataset we pass into the function.\r\n\r\n\r\nclm_cnt_glm_pred <-\r\n  clm_cnt_glm %>% \r\n  augment(pol_clm_df\r\n          ,type.predict = \"response\")\r\n\r\n\r\nNote that the default prediction type is link. To get the same scale as the target variable, I will indicate the type.predict would be “response”.\r\nTo double-check, I have also run predictions by using the predict function from base R.\r\n\r\n\r\npredict(clm_cnt_glm\r\n        ,newdata = pol_clm_df\r\n        ,type = \"response\"\r\n        ) %>% \r\n  as_tibble()\r\n\r\n# A tibble: 99,999 x 1\r\n    value\r\n    <dbl>\r\n 1 0.155 \r\n 2 0.192 \r\n 3 0.176 \r\n 4 0.0983\r\n 5 0.153 \r\n 6 0.0813\r\n 7 0.163 \r\n 8 0.154 \r\n 9 0.140 \r\n10 0.163 \r\n# ... with 99,989 more rows\r\n\r\nLastly, I will pass the target variable and fitted values to rmse function to compute the RMSE.\r\n\r\n\r\nrmse(clm_cnt_glm_pred,\r\n     truth = tot_clm_cnt,\r\n     estimate = .fitted)\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard       0.394\r\n\r\nIt seems like on average, our model is predicting 0.4 more or less than claims than actual.\r\nTo further improve the model, we could consider modeling the different coverage types separately.\r\nFollowing is the proportion of different claim counts within each policy converage:\r\n\r\n\r\npol_clm_df %>% \r\n  mutate(tot_clm_cnt = as.factor(tot_clm_cnt)) %>% \r\n  ggplot(aes(pol_coverage, fill = tot_clm_cnt)) +\r\n  geom_bar(position = \"fill\") +\r\n  scale_fill_manual(values = ghibli_palette(name = \"PonyoLight\"\r\n                                             ,n = 7\r\n                                             ,type = \"discrete\")) +  \r\n  scale_y_continuous(labels = scales::percent) +\r\n  xlab(\"\") +\r\n  ylab(\"\") +\r\n  labs(title = \"Proportion of Claim Counts by Different Policy Coverage\") +\r\n  coord_flip() +\r\n  theme_minimal() +\r\n  theme(legend.position = \"bottom\") +\r\n  guides(fill = guide_legend(nrow = 1, byrow = TRUE))\r\n\r\n\r\n\r\nFrom the results, we could observe Maxi has the highest proportion of policies making at least one claim.\r\nWe could improve the overall model performance by making predictions on each policy coverage separately.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Karolina Grabowska\r\n\r\n\r\n\r\nGlen, Stephanie. “Tweedie Distribution: Definition and Examples.” https://www.statisticshowto.com/tweedie-distribution/.\r\n\r\n\r\nPennState Eberly College of Science. “6.2 - the General Linear f-Test.” https://online.stat.psu.edu/stat501/lesson/6/6.2.\r\n\r\n\r\nRodriguez, German. 2023. “2.3 Tests of Hypotheses.” https://www.statisticshowto.com/tweedie-distribution/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-26-tweedie-dist/image/maths.jpg",
    "last_modified": "2023-03-26T21:45:55+08:00",
    "input_file": "tweedie-dist.knit.md"
  },
  {
    "path": "posts/2023-03-21-ale/",
    "title": "Accumulated Local Effect (ALE)",
    "description": {},
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-03-21",
    "categories": [
      "Machine Learning",
      "Model Explanability"
    ],
    "contents": "\r\n\r\nContents\r\nAccumulated local effect\r\nPros and cons\r\nBonus\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nBuild a model\r\nAccumulated Local Effects\r\nCreate explainer plot\r\nUse accumulated_dependence function\r\n\r\nUse model_profile function\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring another model explainability method, i.e. accumulated local effect.\r\n\r\n\r\n\r\nPhoto by Thomas Willmott on Unsplash\r\nAccumulated local effect\r\nThe general idea of why ACE is preferred over partial dependence plot is partial dependence plot cannot be trusted when the features of the machine learning model are correlated (Molnar 2022).\r\nThe author further explained the correlation could greatly bias the estimated feature effect.\r\n\r\n\r\n\r\nIn short, what ALE is trying to do:\r\nDivide the features into intervals\r\nCalculate the difference in the prediction in the prediction when the feature is replaced with the upper & lower limit of the interval\r\nThe differences are accumulated and centered\r\nVoilà! That is how we obtained the ALE curve.\r\nPros and cons\r\nBelow are the pros and cons of using ALE curve:\r\nPros\r\nALE plots are unbiased\r\nALE plots are faster to compute\r\nInterpretation of ALE plots is clear\r\nALE plots are centered at zero\r\nPrediction function can be decomposed\r\nCons\r\nInterpretation of the effect across intervals is not permissible\r\nALE effects may differ from the coefficients specified in a linear regression model when the features interact and are correlated\r\nThe plots can become a bit shaky\r\nNo perfect solution for setting the number of intervals\r\nNot accompanied by ICE curves\r\nBonus\r\n\r\nI happened to come across this comparison table on different model explainability methods.\r\n\r\n\r\n\r\nTaken from this website\r\nI feel the summary is quite well done and easy to understand.\r\nDemonstration\r\nIn this demonstration, I will be using the accumulated_dependence function from ingredients package to explain the model.\r\nSetup the environment\r\nFirst, I will call the relevant packages to set up the environment.\r\n\r\n\r\npacman::p_load(tidyverse, DALEX, DALEXtra, tidymodels, ingredients, themis, iml, ranger)\r\n\r\n\r\nImport the data\r\nI will re-use one of the Kaggle datasets I previously used for model explainability.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-03-12-marketbasket/data/general_data.csv\") %>%\r\n  # drop the columns we don't need\r\n  dplyr::select(-c(EmployeeCount, StandardHours, EmployeeID)) %>%\r\n  # impute the missing values with the mean values\r\n  mutate(\r\n    NumCompaniesWorked = case_when(\r\n      is.na(NumCompaniesWorked) ~ mean(NumCompaniesWorked, na.rm = TRUE),\r\n      TRUE ~ NumCompaniesWorked),\r\n    TotalWorkingYears = case_when(\r\n      is.na(TotalWorkingYears) ~ mean(TotalWorkingYears, na.rm = TRUE),\r\n      TRUE ~ TotalWorkingYears)\r\n    ) %>%\r\n  droplevels()\r\n\r\n\r\nBuild a model\r\nFor simplicity, I will reuse the random forest model building code I wrote in my previous post so that we can focus this post on how we apply PDP to interpret the machine learning model results.\r\nYou can refer to my previous post on the explanations of the model building.\r\nThe only difference I made in this model building is instead of imputing the missing values during recipe stage, I imputed the missing values before building the model.\r\nThis is because the accumulated_dependence function I will be using later is unable to handle missing values.\r\n\r\n\r\nranger_recipe <- \r\n  recipe(formula = Attrition ~ ., \r\n         data = df) %>%\r\n  step_nzv(all_predictors()) %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_upsample(Attrition) %>%\r\n  prep()\r\n\r\nranger_spec <- \r\n  rand_forest(trees = 1000) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\") \r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(ranger_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\nranger_fit <- ranger_workflow %>%\r\n  fit(data = df)\r\n\r\n\r\nAccumulated Local Effects\r\nCreate explainer plot\r\nSimilarly to the last post, I will first create the explainer object by using the explain_tidymodels function.\r\n\r\n\r\nranger_explainer <- explain_tidymodels(ranger_fit,\r\n                   data = dplyr::select(df, -Attrition),\r\n                   y = df$Attrition,\r\n                   verbose = FALSE)\r\n\r\n\r\nUse accumulated_dependence function\r\nNext, I will use accumulated_dependence function to derive ALE.\r\n\r\n\r\nranger_ale_dept <- \r\n  accumulated_dependence(ranger_explainer,\r\n                         N = 100,\r\n                         variables = \"Department\")\r\n\r\n\r\nThen, I will pass the object into plot function to visualize the results.\r\n\r\n\r\nplot(ranger_ale_dept)\r\n\r\n\r\n\r\nThe ALE will be a curve if the selected variable is in numeric form.\r\n\r\n\r\nranger_ale_yearCo <- \r\n  accumulated_dependency(ranger_explainer,\r\n                         N = 1000,\r\n                         variables = \"YearsAtCompany\")\r\n\r\nplot(ranger_ale_yearCo)\r\n\r\n\r\n\r\nSimilarly, we could plot out the ALE curves for all the numeric variables.\r\n\r\n\r\nranger_ale_num <- \r\n  accumulated_dependence(ranger_explainer,\r\n                         N = 1000,\r\n                         variable_type = \"numerical\")\r\n\r\nplot(ranger_ale_num)\r\n\r\n\r\n\r\nTo satisfy my curiosity, I will also generate the partial dependence plot for the numeric variables for this model.\r\nThis will allow me to compare the results side by side.\r\n\r\n\r\nranger_part_num <- \r\n  partial_dependence(ranger_explainer,\r\n                         N = 1000,\r\n                         variable_type = \"numerical\")\r\n\r\nplot(ranger_part_num)\r\n\r\n\r\n\r\nBy comparing the ALE curve and PDP curve, it doesn’t seem like the shape of the curves have changed drastically when we move from ALE to PDP.\r\nBut the effect of each variable does seem to be different under ALE and PDP methods.\r\nUse model_profile function\r\nAlternatively, we could use the model_profile function from DALEX package to show the accumulated local effect plot.\r\nI will also pass accumulated to the type argument to derive ALE result before plotting the result.\r\n\r\n\r\nranger_company_ale <- model_profile(ranger_explainer, \r\n                                    variables = \"YearsAtCompany\", \r\n                                    type = \"accumulated\")\r\n\r\nplot(ranger_company_ale)\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by John Bakator on Unsplash\r\n\r\n\r\n\r\nMolnar, Christoph. 2022. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-21-ale/image/trees.jpg",
    "last_modified": "2023-03-21T19:19:51+08:00",
    "input_file": "ale.knit.md"
  },
  {
    "path": "posts/2023-03-19-local-spatial-autocorrelation/",
    "title": "Local Moran's I",
    "description": "Are the selected locations different from each other?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-03-19",
    "categories": [
      "Geospatial Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is local spatial autocorrelation?\r\nLocal Moran’s I\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nImport shp files\r\nImport Malaysia 2015 - 2021 birth rate dataset\r\n\r\nJoin the datasets together\r\nSpatial weights\r\nLocal Spatial Autocorrelation\r\nLocal Moran’s I\r\nVisualization\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Marek Studzinski on Unsplash\r\nPreviously, we explored how we could check whether there is any spatial autocorrelation within the dataset.\r\nIn this post, I will be exploring how to check the local measures of spatial autocorrelation.\r\nWhat is local spatial autocorrelation?\r\nIn this post, I will use local Moran’s I to perform analysis.\r\nLocal Moran’s I\r\nThe formula can be written as follows:\r\n\\[I_i=\\frac{x_i-\\bar{X}}{m_2}\\sum_jw_{ij}(x_j-\\bar{X})\\]\r\n\\[m_2=\\frac{\\sum_i(x_j-\\bar{X})^2}{n}\\]\r\nwhere\r\nn is the total number of observations (spatial objects)\r\n\\(x_i\\) is the attribute value of feature i\r\n\\(x_j\\) is the attribute value of feature j\r\n\\(\\bar{X}\\) is the mean of this attribute\r\n\\(w_{ij}\\) is the spatial weight between feature i and j\r\n\\(m_2\\) is the constant for all locations. It is a consistent but not unbiased estimate of the variance\r\nThe interpretation of local Moran’s values is similar to global Moran’s I values.\r\nIf the p-value is less than 0.05, we will reject the null hypothesis and conclude that there is statistical evidence that there is spatial autocorrelation.\r\nIf the z-value is negative, it suggests spatial outlier presence\r\nIf the z-value is positive, it indicates intense clustering of either low or high values\r\nDemonstration\r\nI will download Malaysia shape files from this link.\r\nFor more explanations on shape files, please refer to my previous post.\r\nI will also use the total number of newborn babies data between 2015 and 2021.\r\nSetup the environment\r\nFirst, I will set up the environment by calling the necessary packages.\r\n\r\n\r\npacman::p_load(tidyverse, sf, spdep, tmap, janitor, crosstalk)\r\n\r\n\r\n\r\n\r\ntmap_mode('plot')\r\n\r\n\r\n\r\n\r\nset.seed(1234)\r\n\r\n\r\nImport the data\r\nImport shp files\r\nNext, I will import the dataset into the environment.\r\nIn this demonstration, I will focus on the total number of new birth in West Malaysia.\r\n\r\n\r\nmsia_map <- \r\n  st_read(dsn = \"data\", layer = \"MYS_adm2\") %>% \r\n  filter(!NAME_1 %in% c(\"Sabah\", \"Sarawak\", \"Labuan\"))\r\n\r\nReading layer `MYS_adm2' from data source \r\n  `C:\\Users\\jaspe\\OneDrive\\Documents\\2_Data Science\\98_my-blog\\_posts\\2023-03-19-local-spatial-autocorrelation\\data' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 144 features and 14 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 99.64072 ymin: 0.855001 xmax: 119.2697 ymax: 7.380556\r\nGeodetic CRS:  WGS 84\r\n\r\nImport Malaysia 2015 - 2021 birth rate dataset\r\nI will also import the dataset on 2015 - 2021 birth rate by the administrative district into the environment.\r\n\r\n\r\nmsia_birth <- \r\n  read_csv(\"data/live-births-by-state-administrative-district-and-sex-2015-2021.csv\") %>%\r\n  clean_names() %>%\r\n  filter(year != 2021) %>%\r\n  group_by(year, state, administrative_district) %>%\r\n  summarize(total = sum(value)) %>%\r\n  ungroup() %>%\r\n  pivot_wider(names_from = year,\r\n              names_prefix = \"total_birth_\",\r\n              values_from = total,\r\n              values_fill = 0) %>%\r\n  # change the first letter of each word to capital letter\r\n  mutate(administrative_district = str_to_title(administrative_district)) %>%\r\n  # recode the districts\r\n  mutate(administrative_district_recoded = \r\n           case_when(administrative_district == \"Mualim\" ~ \"Batang Padang\",\r\n                     administrative_district == \"Kuala Nerus\" ~ \"Kuala Terengganu\",\r\n                     administrative_district == \"Bagan Datuk\" ~ \"Hilir Perak\",\r\n                     administrative_district == \"Kecil Lojing\" ~ \"Gua Musang\",\r\n                     administrative_district == \"Selama\" ~ \"Larut and Matang\",\r\n                     administrative_district == \"Larut & Matang\" ~ \"Larut and Matang\",\r\n                     administrative_district == \"Johor Bahru\" ~ \"Johor Baharu\",\r\n                     administrative_district == \"Kluang\" ~ \"Keluang\",\r\n                     administrative_district == \"Kulai\" ~ \"Kulaijaya\",\r\n                     administrative_district == \"Tangkak\" ~ \"Ledang\",\r\n                     administrative_district == \"Pasir Puteh\" ~ \"Pasir Putih\",\r\n                     is.na(administrative_district) == TRUE ~ state,\r\n                     TRUE ~ administrative_district)) %>%\r\n  # remove the string so that it can be matched with the naming stated in map data frame\r\n  mutate(administrative_district_recoded = \r\n           str_replace(administrative_district_recoded, \"W.P. \", \"\")) %>%\r\n  # sum the total number of birth by the recoded administrative districts\r\n  group_by(state, administrative_district_recoded) %>%\r\n  summarise_at(c(\"total_birth_2015\",\r\n                 \"total_birth_2016\",\r\n                 \"total_birth_2017\",\r\n                 \"total_birth_2018\",\r\n                 \"total_birth_2019\",\r\n                 \"total_birth_2020\"),\r\n               function(x) sum(x)) %>%\r\n  ungroup() %>%\r\n  mutate(change_in_birth_2015_2020 = total_birth_2020/total_birth_2015 - 1)\r\n\r\n\r\nJoin the datasets together\r\nAs the map is imported as simple features (i.e. one of the data types), this allows us to use the left_join function with the total new birth data file.\r\n\r\n\r\nmsia_map_birth <-\r\n  msia_map %>%\r\n  left_join(msia_birth,\r\n            by = c(\"NAME_2\" = \"administrative_district_recoded\"))\r\n\r\n\r\nI will calculate the longitude and latitude.\r\n\r\n\r\nlongitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[1]])\r\nlatitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[2]])\r\ncoords <- cbind(longitude, latitude)\r\n\r\n\r\nSpatial weights\r\nNext, I will derive the spatial weights by using K nearest neighbors.\r\n\r\n\r\nkneigh <- knearneigh(coords)\r\nknn <- knn2nb(kneigh)\r\nweight_list_knn <- nb2listw(knn, \r\n                   style = \"W\", \r\n                   zero.policy = TRUE)\r\n\r\n\r\nLocal Spatial Autocorrelation\r\nLocal Moran’s I\r\nNow, I will calculate the local Moran I values.\r\n\r\n\r\nlocan_moran <-\r\n  localmoran(msia_map_birth$change_in_birth_2015_2020,\r\n             listw = weight_list_knn)\r\n\r\nlocan_moran\r\n\r\n             Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\r\n1  -0.255918361 -2.928453e-03 2.540293e-01 -0.50195095   0.6157020254\r\n2  -0.407279506 -1.035266e-02 8.913573e-01 -0.42042114   0.6741778248\r\n3  -0.287809568 -3.824506e-03 3.314594e-01 -0.49326500   0.6218253588\r\n4   0.424312241 -2.351376e-03 2.040887e-01  0.94444383   0.3449428924\r\n5  -0.170060580 -2.166387e-03 1.880674e-01 -0.38714971   0.6986453762\r\n6  -0.284783406 -3.870537e-04 3.366064e-02 -1.55011123   0.1211148213\r\n7   0.161828068 -9.258391e-04 8.047342e-02  0.57372686   0.5661526493\r\n8   0.093039617 -3.023902e-03 2.622840e-01  0.18757399   0.8512106135\r\n9  -0.170060580 -1.804988e-03 1.567505e-01 -0.42497660   0.6708537306\r\n10 -0.060985607 -1.299232e-03 1.128863e-01 -0.17764565   0.8590012686\r\n11 -0.788797632 -3.721631e-02 3.117319e+00 -0.42568206   0.6703395377\r\n12  0.088139506 -7.841682e-04 6.816913e-02  0.34058343   0.7334172040\r\n13 -0.026641998 -6.215507e-07 5.407487e-05 -3.62292138   0.0002912944\r\n14 -0.173167407 -1.037058e-03 9.013044e-02 -0.57335250   0.5664060481\r\n15 -0.040130857 -6.175640e-05 5.372475e-03 -0.54666642   0.5846079275\r\n16 -0.006598121 -1.652334e-06 1.437528e-04 -0.55017809   0.5821972275\r\n17  0.766514936 -2.253027e-02 1.915971e+00  0.57004275   0.5686486999\r\n18 -1.667680625 -2.435391e-03 2.113630e-01 -3.62212727   0.0002921903\r\n19  3.760882629 -1.238576e-02 1.064214e+00  3.65765846   0.0002545299\r\n20 -0.026641998 -1.544046e-01 1.135905e+01  0.03790815   0.9697609115\r\n21 -0.788797632 -2.260482e-03 1.962174e-01 -1.77562203   0.0757952361\r\n22  0.598447384 -3.909602e-03 3.388056e-01  1.03485303   0.3007375547\r\n23 -0.035527887 -1.747523e-03 1.517688e-01 -0.08671078   0.9309014054\r\n24  0.873236250 -1.749050e-02 1.495059e+00  0.72847620   0.4663221342\r\n25 -0.804003827 -3.554695e-02 2.982653e+00 -0.44495709   0.6563507667\r\n26 -0.035527887 -9.766053e-05 8.495636e-03 -0.38439345   0.7006868549\r\n27  1.232301772 -5.894736e-03 5.098190e-01  1.73412972   0.0828950693\r\n28  0.606381613 -3.483156e-02 2.924794e+00  0.37493399   0.7077095628\r\n29  0.050419432 -3.519492e-03 3.051181e-01  0.09764911   0.9222109291\r\n30  0.135822268 -1.427323e-03 1.239999e-01  0.38976287   0.6967119037\r\n31 -0.795872240 -2.458764e-03 2.133865e-01 -1.71757636   0.0858739159\r\n32 -0.038743570 -2.078181e-03 1.804260e-01 -0.08631907   0.9312127785\r\n33  0.065261528 -5.396032e-04 4.692015e-02  0.30377615   0.7612984408\r\n34  0.130622067 -1.309666e-02 1.124487e+00  0.13553022   0.8921926711\r\n35  0.192117491 -2.833096e-02 2.394963e+00  0.14244841   0.8867258262\r\n36  0.130622067 -1.761472e-04 1.532210e-02  1.05667789   0.2906586210\r\n37  0.721971960 -1.035466e-02 8.915276e-01  0.77559932   0.4379855996\r\n38  0.218924028 -1.225608e-02 1.053211e+00  0.22526453   0.8217734914\r\n39 -0.146205454 -5.287353e-04 4.597565e-02 -0.67940108   0.4968837647\r\n40  0.406607235 -4.089416e-03 3.543243e-01  0.68995489   0.4902225537\r\n41 -0.146205454 -5.466281e-03 4.729669e-01 -0.20464429   0.8378500527\r\n42  0.449539545 -6.681555e-03 5.774114e-01  0.60038897   0.5482470388\r\n43 -0.954340169 -4.346586e-03 3.765093e-01 -1.54822056   0.1215692028\r\n44  0.217213428 -3.839666e-03 3.327683e-01  0.38320012   0.7015713913\r\n45  1.463297207 -2.362199e-02 2.006567e+00  1.04968865   0.2938612844\r\n46 -0.657191913 -8.717226e-03 7.517875e-01 -0.74790335   0.4545184578\r\n47 -0.757911282 -1.546074e-02 1.324288e+00 -0.64517307   0.5188150335\r\n48  0.370166726 -4.219181e-03 3.655200e-01  0.61924702   0.5357536377\r\n49  0.569104956 -5.023536e-03 4.348521e-01  0.87063936   0.3839510967\r\n50  0.664443090 -3.592822e-02 3.013452e+00  0.40345602   0.6866127839\r\n51 -0.244022451 -3.744820e-03 3.245793e-01 -0.42174791   0.6732090302\r\n52 -0.074495922 -8.125827e-04 7.063725e-02 -0.27723777   0.7815975396\r\n53  0.143733369 -2.149967e-03 1.866449e-01  0.33767396   0.7356089035\r\n54  0.217213428 -1.661434e-03 1.443046e-01  0.57617688   0.5644956276\r\n55  0.245342111 -6.002743e-04 5.219252e-02  1.07653771   0.2816868434\r\n56  0.274277547 -1.355806e-02 1.163559e+00  0.26683966   0.7895926201\r\n57 -1.918144851 -1.336696e-02 1.147381e+00 -1.77824085   0.0753643057\r\n58  0.514070773 -4.556588e-02 3.783598e+00  0.28770916   0.7735693814\r\n59 -0.221212465 -8.437486e-03 7.278677e-01 -0.24939879   0.8030523230\r\n60  0.128876654 -6.698988e-03 5.789077e-01  0.17818740   0.8585758050\r\n61  0.128876654 -3.352296e-04 2.915520e-02  0.75673610   0.4492079836\r\n62  0.289988711 -1.347573e-03 1.170808e-01  0.85143534   0.3945275696\r\n63 -1.390469117 -2.163763e-02 1.841742e+00 -1.00863865   0.3131479660\r\n64 -1.390469117 -1.208137e-02 1.038380e+00 -1.35267412   0.1761597633\r\n65 -0.040130857 -3.525965e-03 3.056773e-01 -0.06620755   0.9472125879\r\n66 -1.540161952 -8.074053e-03 6.967710e-01 -1.83543344   0.0664414932\r\n67  0.088139506 -1.339477e-03 1.163784e-01  0.26229192   0.7930963915\r\n68  0.052810130 -3.562425e-03 3.088269e-01  0.10144022   0.9192010139\r\n69  0.052810130 -1.058502e-04 9.207995e-03  0.55144767   0.5813268334\r\n70 -1.540161952 -3.972321e-02 3.318639e+00 -0.82364161   0.4101432249\r\n71  2.678019169 -6.179080e-02 5.043625e+00  1.21996974   0.2224763477\r\n72  0.414484487 -4.304722e-02 3.583892e+00  0.24168170   0.8090268139\r\n73  0.164812300 -6.806248e-03 5.881133e-01  0.22378641   0.8229235038\r\n74  0.542214663 -9.234226e-04 8.026358e-02  1.91712748   0.0552217384\r\n75  0.336404225 -1.433781e-02 1.229505e+00  0.31631710   0.7517618306\r\n76 -1.847440570 -2.940610e-02 2.483100e+00 -1.15373216   0.2486099955\r\n77 -0.022188928 -7.209004e-05 6.271381e-03 -0.27928103   0.7800291595\r\n78  0.065261528 -1.067193e-03 9.274672e-02  0.21779723   0.8275871054\r\n79  0.274277547 -7.502156e-04 6.521979e-02  1.07692839   0.2815122583\r\n80  2.678019169 -1.569302e-02 1.343867e+00  2.32366207   0.0201436128\r\n81  0.290569697 -7.998001e-03 6.902609e-01  0.35936534   0.7193218036\r\n82  1.540231871 -8.617752e-03 7.432833e-01  1.79652001   0.0724118528\r\n83  0.193665731 -2.844657e-04 2.474148e-02  1.23304074   0.2175605636\r\n84  0.529030022 -4.391064e-03 3.803451e-01  0.86493066   0.3870768486\r\n85  1.591226637 -1.782702e-02 1.523302e+00  1.30369960   0.1923360265\r\n86  2.215279271 -3.722042e-02 3.117651e+00  1.27570725   0.2020590233\r\n87  1.591226637 -1.920386e-02 1.638651e+00  1.25805252   0.2083727627\r\nattr(,\"call\")\r\nlocalmoran(x = msia_map_birth$change_in_birth_2015_2020, listw = weight_list_knn)\r\nattr(,\"class\")\r\n[1] \"localmoran\" \"matrix\"     \"array\"     \r\nattr(,\"quadr\")\r\n        mean    median     pysal\r\n1   High-Low  High-Low  High-Low\r\n2   High-Low  High-Low  High-Low\r\n3   Low-High  Low-High  Low-High\r\n4  High-High High-High High-High\r\n5   Low-High  Low-High  Low-High\r\n6   Low-High  Low-High  Low-High\r\n7    Low-Low   Low-Low   Low-Low\r\n8    Low-Low   Low-Low   Low-Low\r\n9   High-Low  High-Low  High-Low\r\n10  High-Low  High-Low  High-Low\r\n11  High-Low  High-Low  High-Low\r\n12 High-High High-High High-High\r\n13  High-Low   Low-Low  High-Low\r\n14  High-Low  High-Low  High-Low\r\n15  Low-High  Low-High  Low-High\r\n16  High-Low  High-Low  High-Low\r\n17 High-High High-High High-High\r\n18  High-Low  High-Low  High-Low\r\n19   Low-Low   Low-Low   Low-Low\r\n20  Low-High  Low-High  Low-High\r\n21  Low-High  Low-High  Low-High\r\n22   Low-Low   Low-Low   Low-Low\r\n23  High-Low  High-Low  High-Low\r\n24 High-High High-High High-High\r\n25  High-Low  High-Low  High-Low\r\n26  Low-High  Low-High  Low-High\r\n27 High-High High-High High-High\r\n28 High-High High-High High-High\r\n29   Low-Low   Low-Low   Low-Low\r\n30 High-High High-High High-High\r\n31  Low-High  Low-High  Low-High\r\n32  High-Low  High-Low  High-Low\r\n33   Low-Low   Low-Low   Low-Low\r\n34 High-High High-High High-High\r\n35 High-High High-High High-High\r\n36 High-High High-High High-High\r\n37   Low-Low   Low-Low   Low-Low\r\n38   Low-Low   Low-Low   Low-Low\r\n39  Low-High  Low-High  Low-High\r\n40 High-High High-High High-High\r\n41  High-Low  High-Low  High-Low\r\n42 High-High High-High High-High\r\n43  Low-High  Low-High  Low-High\r\n44   Low-Low   Low-Low   Low-Low\r\n45   Low-Low   Low-Low   Low-Low\r\n46  High-Low  High-Low  High-Low\r\n47  Low-High  Low-High  Low-High\r\n48 High-High High-High High-High\r\n49 High-High High-High High-High\r\n50   Low-Low   Low-Low   Low-Low\r\n51  Low-High  Low-High  Low-High\r\n52  High-Low  High-Low  High-Low\r\n53 High-High High-High High-High\r\n54   Low-Low   Low-Low   Low-Low\r\n55   Low-Low   Low-Low   Low-Low\r\n56   Low-Low   Low-Low   Low-Low\r\n57  Low-High  Low-High  Low-High\r\n58 High-High High-High High-High\r\n59  Low-High  Low-High  Low-High\r\n60   Low-Low   Low-Low   Low-Low\r\n61   Low-Low   Low-Low   Low-Low\r\n62   Low-Low   Low-Low   Low-Low\r\n63  Low-High  Low-High  Low-High\r\n64  High-Low  High-Low  High-Low\r\n65  High-Low High-High  High-Low\r\n66  High-Low  High-Low  High-Low\r\n67 High-High High-High High-High\r\n68   Low-Low   Low-Low   Low-Low\r\n69   Low-Low   Low-Low   Low-Low\r\n70  Low-High  Low-High  Low-High\r\n71   Low-Low   Low-Low   Low-Low\r\n72   Low-Low   Low-Low   Low-Low\r\n73   Low-Low   Low-Low   Low-Low\r\n74   Low-Low   Low-Low   Low-Low\r\n75   Low-Low   Low-Low   Low-Low\r\n76  High-Low  High-Low  High-Low\r\n77  High-Low  High-Low  High-Low\r\n78   Low-Low   Low-Low   Low-Low\r\n79   Low-Low   Low-Low   Low-Low\r\n80   Low-Low   Low-Low   Low-Low\r\n81 High-High High-High High-High\r\n82 High-High High-High High-High\r\n83 High-High High-High High-High\r\n84 High-High High-High High-High\r\n85 High-High High-High High-High\r\n86 High-High High-High High-High\r\n87 High-High High-High High-High\r\n\r\nVisualization\r\nTo visualize the result, I will merge it with the map data.\r\n\r\n\r\nlocan_moran_maps <-\r\n  bind_cols(msia_map_birth, locan_moran)\r\n\r\n\r\nFirst I will create the graph objects.\r\n\r\n\r\nplot_local_moran <- tm_shape(locan_moran_maps) +\r\n  tm_polygons(\"Ii\")\r\n\r\nplot_local_moran_pvalue <- tm_shape(locan_moran_maps) +\r\n  tm_polygons(\"Pr(z != E(Ii))\",\r\n              breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf))\r\n\r\n\r\nThen I will visualize both graphs by using tmap_arrange function.\r\n\r\n\r\ntmap_arrange(plot_local_moran, \r\n             plot_local_moran_pvalue,\r\n             ncol = 2)\r\n\r\n\r\n\r\nFrom the results above, we observe the following:\r\nThree of the states in the north have values that statistically different from the neighbors\r\nAmong the three states, the one in the middle seems to have a higher new birth rate than the surrounding\r\nThe state on the right seems to have a lower new birth rate than the surrounding\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by James Yarema on Unsplash\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-19-local-spatial-autocorrelation/image/apples.jpg",
    "last_modified": "2023-03-19T18:14:51+08:00",
    "input_file": "local-spatial-autocorrelation.knit.md"
  },
  {
    "path": "posts/2023-02-28-linear-mixed-effect/",
    "title": "Linear Mixed Effect Model",
    "description": "When all are being \"mixed\" together",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-02-28",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is a mixed effect model?\r\nFixed effect vs random effect\r\nIntraclass Correlation Coefficient (ICC)\r\nModel Building Strategy\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\n\r\nModel Building\r\nNull Model\r\nModel Performance\r\n\r\nPrediction\r\nBigger model\r\nOther considerations\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by RhondaK Native Florida Folk Artist on Unsplash\r\nIn this post, I will be exploring the linear mixed effect model.\r\nWhat is a mixed effect model?\r\nIn real life, data is often messy. The data might have some “grouping”, “clustering” or “hierarchical” within the dataset.\r\nThis is where we could use mixed effect model to take into account the “grouping”, “clustering” or “hierarchical” within the dataset while fitting the model.\r\nIt seems like the mixed effect model has different names under different disciplines (Clark):\r\nVariance components\r\nRandom intercepts and slopes\r\nRandom effects\r\nRandom coefficients\r\nVarying coefficients\r\nIntercepts- and/or slopes-as-outcomes\r\nHierarchical linear models\r\nMultilevel models (implies multiple levels of hierarchically clustered data)\r\nGrowth curve models (possibly Latent GCM)\r\nMixed effects models\r\nFixed effect vs random effect\r\nBelow is the difference between fixed and random effect (Shrikanth):\r\nFixed effect - the predictor variable, i.e. the effect we are trying to measure after accounting for random variability\r\nRandom effect - best defined as a noise in the data. It arises from uncontrollable variability within the sample\r\nIntraclass Correlation Coefficient (ICC)\r\nAnother important concept for mixed effect models is the intraclass correlation coefficient.\r\nAs suggested by the author(Kumar 2023), it is recommended to calculate the ICC before fitting a random effect model.\r\nThe ICC formula can be written as follows:\r\n\\[\\rho=\\frac{group\\  variance}{group\\  variance + residual\\ variance}\\]\r\nEffectively, ICC tells us how “strongly” clustered the data is.\r\nAccording to the documentation page of the ICC function, The ICC is a value between 0 and 1, where values below 0.5 indicate poor reliability, between 0.5 and 0.75 moderate reliability, between 0.75 and 0.9 good reliability, and any value above 0.9 indicates excellent reliability.\r\nThe author also suggested that the ICC should be calculated on an “empty” or “null” model.\r\nModel Building Strategy\r\nTo build the model, the author suggested building some simple, preliminary models so that they can be used to evaluate the larger models (Roback and Legler 2021).\r\nWith that, we will start by performing extensive exploratory data analysis at each level.\r\nDemonstration\r\nIn this demonstration, I will be using lmer function from lme4 package to explain the model.\r\nI will be using multilevelmod package to assist me in building the models by using functions from tidymodels package.\r\nSetup the environment\r\nFirst, I will call the relevant packages to set up the environment.\r\n\r\n\r\npacman::p_load(tidyverse, tidymodels, ingredients, multilevelmod, lme4, janitor, broom.mixed, tidyr)\r\n\r\n\r\nImport the data\r\nI will use this chocolate rating data from tidytuesday for this demonstration.\r\n\r\n\r\n\r\nPhoto by Pushpak Dsilva on Unsplash\r\n\r\n\r\ndf <- \r\n  read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv\") %>% \r\n  clean_names()\r\n\r\n\r\nAs there are too many company locations, I will group the locations with lesser count as “others”.\r\n\r\n\r\nrecode_company_location <- \r\n  tabyl(df$company_location) %>% \r\n  as_tibble() %>% \r\n  arrange(desc(percent)) %>% \r\n  rename(company_location = `df$company_location`) %>% \r\n  mutate(cumsum = cumsum(percent),\r\n         company_location_recode = case_when(cumsum >= 0.8 ~ \"others\",\r\n                                      TRUE ~ company_location)) %>% \r\n  select(c(company_location, company_location_recode))\r\n\r\n\r\nOnce that is done, I will wrangle & clean up the data before building the model.\r\n\r\n\r\ndf_1 <-\r\n  df %>% \r\n  # for simplicity, drop the missing value\r\n  drop_na() %>% \r\n  # mutate the cocoa percent column so that the info is in numeric format\r\n  mutate(cocoa_percent = str_replace(cocoa_percent, \"%\", \"\"),\r\n         cocoa_percent = as.numeric(cocoa_percent)) %>%\r\n  # join with the recode data and rename the recoded column\r\n  left_join(recode_company_location) %>% \r\n  select(-company_location) %>% \r\n  rename(company_location = company_location_recode) %>% \r\n  # split the ingredient columns\r\n  separate_wider_delim(ingredients,\r\n                       delim = \"-\",\r\n                       names = c(\"num_ingredients\", \"components\")) %>%\r\n  # create dummy variables to capture the ingredients in the chocolate\r\n  mutate(contain_salt = if_else(str_detect(components, \"\\\\Sa\"),1,0),\r\n         contain_lecithin = if_else(str_detect(components, \"\\\\L\"),1,0),\r\n         contain_vanilla = if_else(str_detect(components, \"\\\\V\"),1,0),\r\n         contain_cocoa = if_else(str_detect(components, \"\\\\C\"),1,0),\r\n         contain_other_sweet = if_else(str_detect(components, \"\\\\S*\"),1,0),\r\n         contain_bean = if_else(str_detect(components, \"\\\\B\"),1,0)) %>%\r\n  # create dummy variables to capture tastes\r\n  mutate(taste_fruity = if_else(\r\n    str_detect(most_memorable_characteristics, \r\n               paste(c(\"orange\",\"lemon\",\"fruit\",\"banana\",\"strawberry\",\"melon\",\"berry\",\"cherry\"),\r\n                     collapse = \"|\")),\r\n               1,0),\r\n    taste_sweet = if_else(str_detect(most_memorable_characteristics, \"sweet\"),1,0),\r\n    taste_earthy = if_else(str_detect(most_memorable_characteristics, \"earthy\"),1,0),\r\n    taste_cocoa = if_else(str_detect(most_memorable_characteristics, \"cocoa\"),1,0),\r\n    taste_nutty = \r\n      if_else(str_detect(most_memorable_characteristics,\r\n                         paste(c(\"nutty\",\"nut\",\"nuts\"),\r\n                               collapse = \"|\")),\r\n              1,0),\r\n    taste_sour = if_else(str_detect(most_memorable_characteristics, \"sour\"),1,0),\r\n    taste_intense = if_else(str_detect(most_memorable_characteristics, \"intense\"),1,0),\r\n    taste_rich = if_else(str_detect(most_memorable_characteristics, \"rich\"),1,0),\r\n    taste_mild = if_else(str_detect(most_memorable_characteristics, \"mild\"),1,0),\r\n    taste_fatty = if_else(str_detect(most_memorable_characteristics, \"fatty\"),1,0),\r\n    taste_creamy = if_else(str_detect(most_memorable_characteristics, \"creamy\"),1,0),\r\n    taste_roast = if_else(str_detect(most_memorable_characteristics, \"roasty\"),1,0),\r\n    taste_bitter = if_else(str_detect(most_memorable_characteristics, \"bitter\"),1,0),\r\n    taste_burnt = if_else(str_detect(most_memorable_characteristics, \"burnt\"),1,0),\r\n    taste_spicy = if_else(\r\n    str_detect(most_memorable_characteristics, \r\n               paste(c(\"spicy\",\"pepper\",\"spice\"),\r\n                     collapse = \"|\")),\r\n    1,0),\r\n    taste_sandy = if_else(str_detect(most_memorable_characteristics, \"sandy\"),1,0)\r\n         )\r\n\r\n\r\nModel Building\r\nNull Model\r\nFirst, I will first fit a random intercept model.\r\nI will first define the model I want to build.\r\n\r\n\r\nlmer_specs <- \r\n  linear_reg() %>% \r\n  set_engine(\"lmer\")\r\n\r\n\r\nThen, I will fit the model by indicating the formula and dataset.\r\n\r\n\r\nlmer_fit <- \r\n  lmer_specs %>% \r\n  fit(rating ~ 1 + (1|company_manufacturer), data = df_1)\r\n\r\n\r\nWe could call the fitted model results by indicating fit object.\r\n\r\n\r\nlmer_fit$fit\r\n\r\nLinear mixed model fit by REML ['lmerMod']\r\nFormula: rating ~ 1 + (1 | company_manufacturer)\r\n   Data: data\r\nREML criterion at convergence: 2502.819\r\nRandom effects:\r\n Groups               Name        Std.Dev.\r\n company_manufacturer (Intercept) 0.2180  \r\n Residual                         0.3688  \r\nNumber of obs: 2443, groups:  company_manufacturer, 542\r\nFixed Effects:\r\n(Intercept)  \r\n      3.146  \r\n\r\nAlternatively, we could pass the fit object to the summary function.\r\n\r\n\r\nsummary(lmer_fit$fit)\r\n\r\nLinear mixed model fit by REML ['lmerMod']\r\nFormula: rating ~ 1 + (1 | company_manufacturer)\r\n   Data: data\r\n\r\nREML criterion at convergence: 2502.8\r\n\r\nScaled residuals: \r\n    Min      1Q  Median      3Q     Max \r\n-4.8763 -0.6462  0.0697  0.6994  2.8232 \r\n\r\nRandom effects:\r\n Groups               Name        Variance Std.Dev.\r\n company_manufacturer (Intercept) 0.04751  0.2180  \r\n Residual                         0.13602  0.3688  \r\nNumber of obs: 2443, groups:  company_manufacturer, 542\r\n\r\nFixed effects:\r\n            Estimate Std. Error t value\r\n(Intercept)  3.14640    0.01311   239.9\r\n\r\nBased on the results above, we noted the following:\r\nOn average, the chocolate rating is 3.14\r\nGroup variance is 0.047\r\nResidual variance is 0.136\r\nWe could calculate the ICC by using the info above:\r\n\\(\\rho=\\frac{0.047}{0.047 + 0.136}\\)\r\n= 0.2588678\r\nAnother method is to use icc function from performance package to calculate the necessary figure.\r\n\r\n\r\nicc(lmer_fit$fit)\r\n\r\n# Intraclass Correlation Coefficient\r\n\r\n    Adjusted ICC: 0.259\r\n  Unadjusted ICC: 0.259\r\n\r\nAccording to the documentation page, following are the explanations:\r\nAdjusted ICC: Only looks at the random effects\r\nUnadjusted ICC: Also takes the fixed effects variances into account\r\n\r\n\r\nicc(lmer_fit$fit, by_group = TRUE)\r\n\r\n# ICC by Group\r\n\r\nGroup                |   ICC\r\n----------------------------\r\ncompany_manufacturer | 0.259\r\n\r\nModel Performance\r\nNext, I will check the model performance.\r\n\r\n\r\nperformance::model_performance(lmer_fit$fit)\r\n\r\n# Indices of model performance\r\n\r\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\r\n--------------------------------------------------------------------------------\r\n2508.819 | 2508.829 | 2526.222 |      0.259 |      0.000 | 0.259 | 0.347 | 0.369\r\n\r\nFrom the results above, we noted that there are R2 (cond.) and R2 (marg.).\r\nBelow is the explanation on both R2 extracted from this article:\r\nMarginal R2 is concerned with variance explained by fixed factors\r\nConditional R2 is concerned with variance explained by both fixed and random factors.\r\nOne important note is the definitions of the R2 above are slightly different from the traditional R2.\r\nThis article discussed the difference in R2 definitions.\r\nNevertheless, we could call the model performance by using glance function, summ function or report function as shown below.\r\n\r\n\r\nlmer_fit %>% \r\n  glance()\r\n\r\n# A tibble: 1 x 7\r\n   nobs sigma logLik   AIC   BIC REMLcrit df.residual\r\n  <int> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\r\n1  2443 0.369 -1251. 2509. 2526.    2503.        2440\r\n\r\nThis method I found online is also pretty cool. I love how it “tidies” the results so that it is easier to read.\r\n\r\n\r\nsumm(lmer_fit$fit)\r\n\r\n\r\nObservations\r\n\r\n\r\n2443\r\n\r\n\r\nDependent variable\r\n\r\n\r\nrating\r\n\r\n\r\nType\r\n\r\n\r\nMixed effects linear regression\r\n\r\n\r\nAIC\r\n\r\n\r\n2508.82\r\n\r\n\r\nBIC\r\n\r\n\r\n2526.22\r\n\r\n\r\nPseudo-R² (fixed effects)\r\n\r\n\r\n0.00\r\n\r\n\r\nPseudo-R² (total)\r\n\r\n\r\n0.26\r\n\r\n\r\n\r\nFixed Effects\r\n\r\n\r\n\r\n\r\n\r\nEst.\r\n\r\n\r\nS.E.\r\n\r\n\r\nt val.\r\n\r\n\r\nd.f.\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n3.15\r\n\r\n\r\n0.01\r\n\r\n\r\n239.81\r\n\r\n\r\n476.37\r\n\r\n\r\n0.00\r\n\r\n\r\n p values calculated using Kenward-Roger standard errors and d.f.\r\n\r\n\r\n\r\nRandom Effects\r\n\r\n\r\n\r\nGroup\r\n\r\n\r\nParameter\r\n\r\n\r\nStd. Dev.\r\n\r\n\r\ncompany_manufacturer\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n0.22\r\n\r\n\r\nResidual\r\n\r\n\r\n\r\n\r\n0.37\r\n\r\n\r\n\r\nGrouping Variables\r\n\r\n\r\n\r\nGroup\r\n\r\n\r\n# groups\r\n\r\n\r\nICC\r\n\r\n\r\ncompany_manufacturer\r\n\r\n\r\n542\r\n\r\n\r\n0.26\r\n\r\n\r\nOr use report function to show the summarised model results.\r\n\r\n\r\nreport(lmer_fit$fit) %>% \r\n  as.data.frame()\r\n\r\nParameter        | Coefficient |       95% CI | t(2440) |      p | Effects |                Group | Std. Coef. | Std. Coef. 95% CI |     Fit\r\n--------------------------------------------------------------------------------------------------------------------------------------------\r\n(Intercept)      |        3.15 | [3.12, 3.17] |  239.93 | < .001 |   fixed |                      |      -0.15 |    [-0.21, -0.09] |        \r\n                 |        0.22 |              |         |        |  random | company_manufacturer |            |                   |        \r\n                 |        0.37 |              |         |        |  random |             Residual |            |                   |        \r\n                 |             |              |         |        |         |                      |            |                   |        \r\nAIC              |             |              |         |        |         |                      |            |                   | 2508.82\r\nAICc             |             |              |         |        |         |                      |            |                   | 2508.83\r\nBIC              |             |              |         |        |         |                      |            |                   | 2526.22\r\nR2 (conditional) |             |              |         |        |         |                      |            |                   |    0.26\r\nR2 (marginal)    |             |              |         |        |         |                      |            |                   |    0.00\r\nSigma            |             |              |         |        |         |                      |            |                   |    0.37\r\n\r\nPrediction\r\nWe could extract the prediction by using augment function from tidymodels package.\r\n\r\n\r\nprediction_df <-\r\n  lmer_fit %>% \r\n  augment(df_1)\r\n\r\nprediction_df\r\n\r\n# A tibble: 2,443 x 35\r\n     ref company_man~1 revie~2 count~3 speci~4 cocoa~5 num_i~6 compo~7\r\n   <dbl> <chr>           <dbl> <chr>   <chr>     <dbl> <chr>   <chr>  \r\n 1  2454 5150             2019 Tanzan~ Kokoa ~      76 3       \" B,S,~\r\n 2  2458 5150             2019 Domini~ Zorzal~      76 3       \" B,S,~\r\n 3  2454 5150             2019 Madaga~ Bejofo~      76 3       \" B,S,~\r\n 4  2542 5150             2021 Fiji    Matasa~      68 3       \" B,S,~\r\n 5  2546 5150             2021 Venezu~ Sur de~      72 3       \" B,S,~\r\n 6  2546 5150             2021 Uganda  Semuli~      80 3       \" B,S,~\r\n 7  2542 5150             2021 India   Anamal~      68 3       \" B,S,~\r\n 8   797 A. Morin         2012 Bolivia Bolivia      70 4       \" B,S,~\r\n 9   797 A. Morin         2012 Peru    Peru         63 4       \" B,S,~\r\n10  1011 A. Morin         2013 Panama  Panama       70 4       \" B,S,~\r\n# ... with 2,433 more rows, 27 more variables:\r\n#   most_memorable_characteristics <chr>, rating <dbl>,\r\n#   company_location <chr>, contain_salt <dbl>,\r\n#   contain_lecithin <dbl>, contain_vanilla <dbl>,\r\n#   contain_cocoa <dbl>, contain_other_sweet <dbl>,\r\n#   contain_bean <dbl>, taste_fruity <dbl>, taste_sweet <dbl>,\r\n#   taste_earthy <dbl>, taste_cocoa <dbl>, taste_nutty <dbl>, ...\r\n\r\n\r\n\r\nggplot(prediction_df, \r\n       aes(x = rating, y = .pred)) +\r\n  geom_point() +\r\n  geom_abline(slope = 1, intercept = 0) +\r\n  xlab(\"Actual Value\") +\r\n  ylab(\"Predicted Value\") +\r\n  labs(title = \"Graph - Actual vs Predicted\",\r\n       subtitle = \"Linear Mixed Effect Model - Null\")\r\n\r\n\r\n\r\n\r\n\r\nBigger model\r\nNext, I will build a bigger model.\r\n\r\n\r\nlmer_fit_bigger <- \r\n  linear_reg() %>%\r\n  set_engine(\"lmer\") %>% \r\n  fit(rating ~ \r\n        ref \r\n      + company_location\r\n      + cocoa_percent \r\n      + review_date \r\n      + num_ingredients\r\n      + contain_salt \r\n      + contain_lecithin \r\n      + contain_cocoa \r\n      + taste_fruity \r\n      + taste_sweet\r\n      + taste_earthy\r\n      + taste_cocoa\r\n      + taste_nutty\r\n      + taste_sour\r\n      + taste_intense\r\n      + taste_rich\r\n      + taste_mild\r\n      + taste_fatty\r\n      + taste_creamy\r\n      + taste_roast\r\n      + taste_bitter\r\n      + taste_burnt\r\n      + taste_spicy\r\n      + taste_sandy\r\n      + (1|company_manufacturer), \r\n      data = df_1)\r\n\r\n\r\nNext, I will use summ function to summarise the model result.\r\n\r\n\r\nsumm(lmer_fit_bigger$fit)\r\n\r\n\r\nObservations\r\n\r\n\r\n2443\r\n\r\n\r\nDependent variable\r\n\r\n\r\nrating\r\n\r\n\r\nType\r\n\r\n\r\nMixed effects linear regression\r\n\r\n\r\nAIC\r\n\r\n\r\n2133.21\r\n\r\n\r\nBIC\r\n\r\n\r\n2371.05\r\n\r\n\r\nPseudo-R² (fixed effects)\r\n\r\n\r\n0.24\r\n\r\n\r\nPseudo-R² (total)\r\n\r\n\r\n0.39\r\n\r\n\r\n\r\nFixed Effects\r\n\r\n\r\n\r\n\r\n\r\nEst.\r\n\r\n\r\nS.E.\r\n\r\n\r\nt val.\r\n\r\n\r\nd.f.\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n-59.84\r\n\r\n\r\n35.26\r\n\r\n\r\n-1.70\r\n\r\n\r\n2291.43\r\n\r\n\r\n0.09\r\n\r\n\r\nref\r\n\r\n\r\n-0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n-1.15\r\n\r\n\r\n2263.32\r\n\r\n\r\n0.25\r\n\r\n\r\ncompany_locationBelgium\r\n\r\n\r\n-0.12\r\n\r\n\r\n0.10\r\n\r\n\r\n-1.23\r\n\r\n\r\n441.20\r\n\r\n\r\n0.22\r\n\r\n\r\ncompany_locationCanada\r\n\r\n\r\n-0.12\r\n\r\n\r\n0.09\r\n\r\n\r\n-1.44\r\n\r\n\r\n466.26\r\n\r\n\r\n0.15\r\n\r\n\r\ncompany_locationEcuador\r\n\r\n\r\n-0.14\r\n\r\n\r\n0.10\r\n\r\n\r\n-1.33\r\n\r\n\r\n503.62\r\n\r\n\r\n0.18\r\n\r\n\r\ncompany_locationFrance\r\n\r\n\r\n-0.07\r\n\r\n\r\n0.09\r\n\r\n\r\n-0.75\r\n\r\n\r\n406.06\r\n\r\n\r\n0.46\r\n\r\n\r\ncompany_locationGermany\r\n\r\n\r\n-0.08\r\n\r\n\r\n0.11\r\n\r\n\r\n-0.66\r\n\r\n\r\n483.12\r\n\r\n\r\n0.51\r\n\r\n\r\ncompany_locationItaly\r\n\r\n\r\n-0.08\r\n\r\n\r\n0.10\r\n\r\n\r\n-0.78\r\n\r\n\r\n402.30\r\n\r\n\r\n0.44\r\n\r\n\r\ncompany_locationothers\r\n\r\n\r\n-0.11\r\n\r\n\r\n0.08\r\n\r\n\r\n-1.45\r\n\r\n\r\n494.33\r\n\r\n\r\n0.15\r\n\r\n\r\ncompany_locationSpain\r\n\r\n\r\n-0.02\r\n\r\n\r\n0.12\r\n\r\n\r\n-0.17\r\n\r\n\r\n427.03\r\n\r\n\r\n0.86\r\n\r\n\r\ncompany_locationSwitzerland\r\n\r\n\r\n-0.12\r\n\r\n\r\n0.11\r\n\r\n\r\n-1.07\r\n\r\n\r\n447.13\r\n\r\n\r\n0.28\r\n\r\n\r\ncompany_locationU.K.\r\n\r\n\r\n-0.25\r\n\r\n\r\n0.09\r\n\r\n\r\n-2.89\r\n\r\n\r\n462.93\r\n\r\n\r\n0.00\r\n\r\n\r\ncompany_locationU.S.A.\r\n\r\n\r\n-0.16\r\n\r\n\r\n0.08\r\n\r\n\r\n-2.07\r\n\r\n\r\n488.72\r\n\r\n\r\n0.04\r\n\r\n\r\ncocoa_percent\r\n\r\n\r\n-0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n-2.17\r\n\r\n\r\n2401.07\r\n\r\n\r\n0.03\r\n\r\n\r\nreview_date\r\n\r\n\r\n0.03\r\n\r\n\r\n0.02\r\n\r\n\r\n1.79\r\n\r\n\r\n2291.30\r\n\r\n\r\n0.07\r\n\r\n\r\nnum_ingredients2\r\n\r\n\r\n0.05\r\n\r\n\r\n0.15\r\n\r\n\r\n0.34\r\n\r\n\r\n2170.10\r\n\r\n\r\n0.73\r\n\r\n\r\nnum_ingredients3\r\n\r\n\r\n-0.26\r\n\r\n\r\n0.17\r\n\r\n\r\n-1.52\r\n\r\n\r\n2393.48\r\n\r\n\r\n0.13\r\n\r\n\r\nnum_ingredients4\r\n\r\n\r\n-0.45\r\n\r\n\r\n0.18\r\n\r\n\r\n-2.46\r\n\r\n\r\n2400.34\r\n\r\n\r\n0.01\r\n\r\n\r\nnum_ingredients5\r\n\r\n\r\n-0.53\r\n\r\n\r\n0.19\r\n\r\n\r\n-2.72\r\n\r\n\r\n2332.21\r\n\r\n\r\n0.01\r\n\r\n\r\nnum_ingredients6\r\n\r\n\r\n-0.55\r\n\r\n\r\n0.29\r\n\r\n\r\n-1.87\r\n\r\n\r\n2012.93\r\n\r\n\r\n0.06\r\n\r\n\r\ncontain_salt\r\n\r\n\r\n-0.10\r\n\r\n\r\n0.09\r\n\r\n\r\n-1.02\r\n\r\n\r\n1102.38\r\n\r\n\r\n0.31\r\n\r\n\r\ncontain_lecithin\r\n\r\n\r\n0.12\r\n\r\n\r\n0.05\r\n\r\n\r\n2.56\r\n\r\n\r\n881.64\r\n\r\n\r\n0.01\r\n\r\n\r\ncontain_cocoa\r\n\r\n\r\n0.34\r\n\r\n\r\n0.09\r\n\r\n\r\n3.80\r\n\r\n\r\n1774.29\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_fruity\r\n\r\n\r\n0.20\r\n\r\n\r\n0.02\r\n\r\n\r\n12.10\r\n\r\n\r\n2320.76\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_sweet\r\n\r\n\r\n-0.11\r\n\r\n\r\n0.02\r\n\r\n\r\n-4.57\r\n\r\n\r\n2402.82\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_earthy\r\n\r\n\r\n-0.13\r\n\r\n\r\n0.03\r\n\r\n\r\n-4.80\r\n\r\n\r\n2351.91\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_cocoa\r\n\r\n\r\n0.15\r\n\r\n\r\n0.02\r\n\r\n\r\n7.54\r\n\r\n\r\n2330.62\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_nutty\r\n\r\n\r\n0.10\r\n\r\n\r\n0.02\r\n\r\n\r\n5.19\r\n\r\n\r\n2334.65\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_sour\r\n\r\n\r\n-0.17\r\n\r\n\r\n0.03\r\n\r\n\r\n-6.38\r\n\r\n\r\n2333.30\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_intense\r\n\r\n\r\n0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n0.66\r\n\r\n\r\n2314.37\r\n\r\n\r\n0.51\r\n\r\n\r\ntaste_rich\r\n\r\n\r\n0.16\r\n\r\n\r\n0.03\r\n\r\n\r\n5.11\r\n\r\n\r\n2291.11\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_mild\r\n\r\n\r\n0.07\r\n\r\n\r\n0.03\r\n\r\n\r\n2.87\r\n\r\n\r\n2337.86\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_fatty\r\n\r\n\r\n-0.11\r\n\r\n\r\n0.03\r\n\r\n\r\n-3.54\r\n\r\n\r\n2394.49\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_creamy\r\n\r\n\r\n0.21\r\n\r\n\r\n0.03\r\n\r\n\r\n7.50\r\n\r\n\r\n2325.85\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_roast\r\n\r\n\r\n-0.02\r\n\r\n\r\n0.03\r\n\r\n\r\n-0.80\r\n\r\n\r\n2314.03\r\n\r\n\r\n0.42\r\n\r\n\r\ntaste_bitter\r\n\r\n\r\n-0.23\r\n\r\n\r\n0.04\r\n\r\n\r\n-5.67\r\n\r\n\r\n2334.66\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_burnt\r\n\r\n\r\n-0.25\r\n\r\n\r\n0.04\r\n\r\n\r\n-5.88\r\n\r\n\r\n2310.41\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_spicy\r\n\r\n\r\n0.14\r\n\r\n\r\n0.02\r\n\r\n\r\n6.05\r\n\r\n\r\n2278.74\r\n\r\n\r\n0.00\r\n\r\n\r\ntaste_sandy\r\n\r\n\r\n-0.07\r\n\r\n\r\n0.03\r\n\r\n\r\n-2.25\r\n\r\n\r\n2397.80\r\n\r\n\r\n0.02\r\n\r\n\r\n p values calculated using Kenward-Roger standard errors and d.f.\r\n\r\n\r\n\r\nRandom Effects\r\n\r\n\r\n\r\nGroup\r\n\r\n\r\nParameter\r\n\r\n\r\nStd. Dev.\r\n\r\n\r\ncompany_manufacturer\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n0.17\r\n\r\n\r\nResidual\r\n\r\n\r\n\r\n\r\n0.33\r\n\r\n\r\n\r\nGrouping Variables\r\n\r\n\r\n\r\nGroup\r\n\r\n\r\n# groups\r\n\r\n\r\nICC\r\n\r\n\r\ncompany_manufacturer\r\n\r\n\r\n542\r\n\r\n\r\n0.20\r\n\r\n\r\nWe could also use anova function to check whether there is an improvement in model fit.\r\n\r\n\r\nanova(lmer_fit$fit, lmer_fit_bigger$fit)\r\n\r\nData: data\r\nModels:\r\nlmer_fit$fit: rating ~ 1 + (1 | company_manufacturer)\r\nlmer_fit_bigger$fit: rating ~ ref + company_location + cocoa_percent + review_date + num_ingredients + contain_salt + contain_lecithin + contain_cocoa + taste_fruity + taste_sweet + taste_earthy + taste_cocoa + taste_nutty + taste_sour + taste_intense + taste_rich + taste_mild + taste_fatty + taste_creamy + taste_roast + taste_bitter + taste_burnt + taste_spicy + taste_sandy + (1 | company_manufacturer)\r\n                    npar    AIC    BIC  logLik deviance Chisq Df\r\nlmer_fit$fit           3 2502.0 2519.4 -1248.0   2496.0         \r\nlmer_fit_bigger$fit   41 1929.6 2167.4  -923.8   1847.6 648.4 38\r\n                    Pr(>Chisq)    \r\nlmer_fit$fit                      \r\nlmer_fit_bigger$fit  < 2.2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nAs the p-value is less than 0.05, we reject the null hypothesis and conclude that the additional variables improve the model fit.\r\nOther considerations\r\nIf the model fails to converge, this post offers some suggestions on how we can fix the issue.\r\nAlternatively, this author offers suggestions on how we could fix singularity and convergence issues.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Danny Lines on Unsplash\r\n\r\n\r\n\r\nClark, Michael. “Mixed Models.” https://m-clark.github.io/mixed-models-with-R/random_intercepts.html.\r\n\r\n\r\nKumar, Anshul. 2023. https://bookdown.org/anshul302/HE902-MGHIHP-Spring2020/Random.html#ICCintro.\r\n\r\n\r\nRoback, Paul, and Julie Legler. 2021. https://bookdown.org/roback/bookdown-BeyondMLR/ch-multilevelintro.html#sec:buildmodel.\r\n\r\n\r\nShrikanth, Sushmita. https://ademos.people.uic.edu/Chapter17.html#122_fixed_v_random_effects.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-28-linear-mixed-effect/image/mixed_colors.jpg",
    "last_modified": "2023-03-09T22:18:41+08:00",
    "input_file": "linear-mixed-effect.knit.md"
  },
  {
    "path": "posts/2023-02-20-global-spatial-autocorrelation/",
    "title": "Global Spatial Autocorrelation",
    "description": {},
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-02-20",
    "categories": [
      "Geospatial Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is spatial autocorrelation?\r\nMoran’s I test\r\nHow to interpret the result\r\nGeary’s C test\r\nGeneral G statistics\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nImport shp files\r\nImport Malaysia 2015 - 2021 birth rate dataset\r\n\r\nJoin the datasets together\r\nGlobal Spatial Autocorrelation\r\nDerive the longitude and latitude of each polygon\r\nDerive spatial weights\r\nMoran’s I\r\nGeary’s C test\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Ralph (Ravi) Kayden on Unsplash\r\nWhat is spatial autocorrelation?\r\nSpatial autocorrelation is the degree of spatial dependency, association or\r\ncorrelation between the value of an observation of a spatial entity and the\r\nvalues of neighboring observations of the same variable (Grekousis 2020).\r\nIn this post, I will be exploring the different approaches to measure global spatial autocorrelation.\r\nMoran’s I test\r\nMoran’s I index computes global spatial autocorrelation by taking into account\r\nfeature locations and attribute values (of a single attribute) simultaneously.\r\nThe formula can be written as:\r\n\\[I = \\frac{n}{\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}}\\frac{\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\sum^{n}_{i}(x_i-\\bar{x})^2}\\]\r\nwhere\r\nn is the number of spatial features\r\n\\(x_i\\) is the attribute value of feature i\r\n\\(x_j\\) is the attribute value of feature j\r\n\\(\\bar{x}\\) is the mean of this attribute\r\n\\(w_{ij}\\) is the spatial weight between feature i and j\r\n\\(\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}\\) is the aggregation of all spatial weights\r\nHow to interpret the result\r\n(Grekousis 2020) has a very good explanation of how to interpret the results from the different tests and what to look out for when doing the spatial analysis.\r\nWe will reject the null hypothesis of zero spatial autocorrelation when the p-value is less than 0.05.\r\nPositive z value indicates there is positive spatial autocorrelation, i.e. clustering of high or low values\r\nNegative z value indicates there is negative spatial autocorrelation, i.e. a dispersed pattern of values\r\n\r\n\r\n\r\nTaken from this github\r\nAnother important note when using this statistical test is even if the statistical test shows that there is no autocorrelation on the global level, it doesn’t imply there are no clusters on the local level (Grekousis 2020).\r\nThe authors also mentioned that global statistics are more effective when there is a consistent trend across the study area.\r\nThe following are other practical guidelines recommended by the authors:\r\nResults are reliable if we have at least 30 spatial objects\r\nRow standardization should be applied if necessary. Row standardization\r\nis common when we have polygons\r\nGeary’s C test\r\nThis is another method to measure global spatial autocorrelation.\r\nNote that this approach is more sensitive to the differences in small neighborhoods.\r\nHence when we are searching for global spatial autocorrelation, Moran’s I is usually preferred.\r\nThe formula can be written as follows:\r\n\\[C=\\frac{(n-1)}{2\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}}\\frac{\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}(x_i-x_j)^2}{\\sum^{n}_{i}(x_i-\\bar{x})^2}\\]\r\nwhere\r\nn is the total number of spatial objects\r\n\\(x_i\\) is the attribute value of feature i\r\n\\(x_j\\) is the attribute value of feature j\r\n\\(\\bar{x}\\) is the mean of this attribute\r\n\\(w_{ij}\\) is the spatial weight between feature i and j\r\n\\(\\sum^{n}_{i}\\sum^{n}_{j}w_{ij}\\) is the aggregation of all spatial weights\r\nGeneral G statistics\r\nIn this demonstration, I will focus on Moran’s I and Geary’s C test.\r\nBut for completeness, I will just put a brief description of general G statistics below.\r\nSimilar to Moran’s I, we will reject the null hypothesis when the p-value is less than 0.05.\r\nPositive z value indicates a clustering of high values\r\nNegative z value suggests there are cluster of low values\r\nNote that if the clusters of low values coexist with the clusters with high values in the study area, they tend to counterbalance each other.\r\nHence, Moran’s I is more suitable to trace this association on a global scale.\r\nThe author suggests the following while using this statistics test:\r\nThis statistic works only with positive values\r\nA binary weights matrix (e.g. fixed distance band, polygon contiguity, k-nearest\r\nneighbors or Delaunay triangulation) is more appropriate for this statistic\r\nWhen we use binary weighting and fixed distance, the size of the polygons\r\nmight matter\r\nMore common to use the local version of the General G-Statistic\r\nindex, as it provides the exact locations of the clusters\r\nDemonstration\r\nI will download Malaysia shape files from this link.\r\nFor more explanations on shape files, please refer to my previous post.\r\nTo make the demonstration, I will also use the total number of newborn babies data between 2015 and 2021.\r\n\r\n\r\n\r\nSetup the environment\r\nFirst, I will setup the environment by calling the necessary packages.\r\n\r\n\r\npacman::p_load(tidyverse, sf, spdep, tmap, janitor)\r\n\r\n\r\nI will also set the tmap_mode to view so that I can interact with the graphs.\r\n\r\n\r\ntmap_mode('view')\r\n\r\n\r\n\r\n\r\n\r\nImport the data\r\nImport shp files\r\nNext, I will import the dataset into the environment.\r\nFor simplicity, I will focus on spatial analysis of West Malaysia.\r\n\r\n\r\nmsia_map <- \r\n  st_read(dsn = \"data\", layer = \"MYS_adm2\") %>% \r\n  filter(!NAME_1 %in% c(\"Sabah\", \"Sarawak\", \"Labuan\"))\r\n\r\nReading layer `MYS_adm2' from data source \r\n  `C:\\Users\\jaspe\\OneDrive\\Documents\\2_Data Science\\98_my-blog\\_posts\\2023-02-20-global-spatial-autocorrelation\\data' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 144 features and 14 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 99.64072 ymin: 0.855001 xmax: 119.2697 ymax: 7.380556\r\nGeodetic CRS:  WGS 84\r\n\r\nImport Malaysia 2015 - 2021 birth rate dataset\r\nI will also import the dataset on 2015 - 2021 birth rate by administrative district into the environment.\r\n\r\n\r\nmsia_birth <- \r\n  read_csv(\"data/live-births-by-state-administrative-district-and-sex-2015-2021.csv\") %>%\r\n  clean_names() \r\n\r\n\r\nAs there are some districts that only have a total number of births for 2021 and I am unable to find online whether these districts have split out from other existing districts, I will focus on the demonstration of the number of birth between 2015 and 2020.\r\nI will also reshape the dataset for analysis later.\r\n\r\n\r\nmsia_birth <-\r\n  msia_birth %>%\r\n  filter(year != 2021) %>%\r\n  group_by(year, state, administrative_district) %>%\r\n  summarize(total = sum(value)) %>%\r\n  ungroup() %>%\r\n  pivot_wider(names_from = year,\r\n              names_prefix = \"total_birth_\",\r\n              values_from = total,\r\n              values_fill = 0)\r\n\r\n\r\nAs some of the namings of the administrative districts are different between the map data frame and the dataset on the number of birth, hence I will perform some data cleaning before joining the datasets.\r\n\r\n\r\nmsia_birth <-\r\n  msia_birth %>%\r\n  # change the first letter of each word to capital letter\r\n  mutate(administrative_district = str_to_title(administrative_district)) %>%\r\n  # recode the districts\r\n  mutate(administrative_district_recoded = \r\n           case_when(administrative_district == \"Mualim\" ~ \"Batang Padang\",\r\n                     administrative_district == \"Kuala Nerus\" ~ \"Kuala Terengganu\",\r\n                     administrative_district == \"Bagan Datuk\" ~ \"Hilir Perak\",\r\n                     administrative_district == \"Kecil Lojing\" ~ \"Gua Musang\",\r\n                     administrative_district == \"Selama\" ~ \"Larut and Matang\",\r\n                     administrative_district == \"Larut & Matang\" ~ \"Larut and Matang\",\r\n                     administrative_district == \"Johor Bahru\" ~ \"Johor Baharu\",\r\n                     administrative_district == \"Kluang\" ~ \"Keluang\",\r\n                     administrative_district == \"Kulai\" ~ \"Kulaijaya\",\r\n                     administrative_district == \"Tangkak\" ~ \"Ledang\",\r\n                     administrative_district == \"Pasir Puteh\" ~ \"Pasir Putih\",\r\n                     is.na(administrative_district) == TRUE ~ state,\r\n                     TRUE ~ administrative_district)) %>%\r\n  # remove the string so that it can be matched with the naming stated in the map data frame\r\n  mutate(administrative_district_recoded = \r\n           str_replace(administrative_district_recoded, \"W.P. \", \"\")) %>%\r\n  # sum the total number of birth by the recoded administrative districts\r\n  group_by(state, administrative_district_recoded) %>%\r\n  summarise_at(c(\"total_birth_2015\",\r\n                 \"total_birth_2016\",\r\n                 \"total_birth_2017\",\r\n                 \"total_birth_2018\",\r\n                 \"total_birth_2019\",\r\n                 \"total_birth_2020\"),\r\n               function(x) sum(x)) %>%\r\n  ungroup()\r\n\r\n\r\nI will also calculate the change in a number of birth over 6 years period.\r\n\r\n\r\nmsia_birth <-\r\n  msia_birth %>%\r\n  mutate(change_in_birth_2015_2020 = total_birth_2020/total_birth_2015 - 1)\r\n\r\n\r\nJoin the datasets together\r\nAs the map is imported as simple features (i.e. one of the data types), this allows us to use left_join function to\r\n\r\n\r\nmsia_map_birth <-\r\n  msia_map %>%\r\n  left_join(msia_birth,\r\n            by = c(\"NAME_2\" = \"administrative_district_recoded\"))\r\n\r\n\r\nNext, I will visualize the results by using functions from tmap package.\r\n\r\n\r\nchange <- tmap_mode('view') + \r\n  tm_shape(msia_map_birth) +\r\n  tm_polygons(\"change_in_birth_2015_2020\",\r\n              id = \"NAME_2\",\r\n              popup.vars = c(\"District: \" = \"NAME_2\",\r\n                            \"Total Birth in 2015: \" = \"total_birth_2015\",\r\n                            \"Total Birth in 2020: \" = \"total_birth_2020\",\r\n                            \"Change in Birth from 2015 to 2020: \" = \"change_in_birth_2015_2020\"), \r\n             breaks = c(-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75))\r\n\r\nbirth <- tmap_mode('view') + \r\n  tm_shape(msia_map_birth) +\r\n  tm_polygons(\"total_birth_2020\", \r\n              id = \"NAME_2\",\r\n              popup.vars = c(\"District: \" = \"NAME_2\",\r\n                            \"Total Birth in 2015: \" = \"total_birth_2015\",\r\n                            \"Total Birth in 2020: \" = \"total_birth_2020\",\r\n                            \"Change in Birth from 2015 to 2020: \" = \"change_in_birth_2015_2020\"))\r\n\r\ntmap_arrange(change, \r\n             birth,\r\n             ncol = 2,\r\n             nrow = 1,\r\n             sync = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nInteresting.\r\nMost of the administrative districts have experienced a drop in total newborn babies from 2015 to 2020.\r\nThe decrease in total newborn babies seems to be quite uniform across different administrative districts.\r\nGlobal Spatial Autocorrelation\r\nDerive the longitude and latitude of each polygon\r\nI will first derive the longitude and latitude of each districts from the data.\r\n\r\n\r\nlongitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[1]])\r\nlatitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[2]])\r\ncoords <- cbind(longitude, latitude)\r\n\r\n\r\nDerive spatial weights\r\nOver here, I have used k nearest neighbors to derive the spatial weights.\r\n\r\n\r\nkneigh <- knearneigh(coords, k = 3)\r\nknn <- knn2nb(kneigh)\r\n\r\n\r\nRefer to my previous post on how to derive distance-based neighbors.\r\nThen, I will use nb2listw function to find the neighbor lists.\r\nNote that I have indicated the style to be “W” so that the output is row standardized.\r\n\r\n\r\nweight_list_knn <- nb2listw(knn, \r\n                   style = \"W\", \r\n                   zero.policy = TRUE)\r\n\r\n\r\nMoran’s I\r\nGreat!\r\nOnce we derived all the necessary info, I will perform the global Moran’s I test.\r\n\r\n\r\nmoran.test(msia_map_birth$change_in_birth_2015_2020,\r\n           listw = weight_list_knn,\r\n           zero.policy = TRUE,\r\n           na.action = na.omit)\r\n\r\n\r\n    Moran I test under randomisation\r\n\r\ndata:  msia_map_birth$change_in_birth_2015_2020  \r\nweights: weight_list_knn    \r\n\r\nMoran I statistic standard deviate = 1.5283, p-value = 0.06322\r\nalternative hypothesis: greater\r\nsample estimates:\r\nMoran I statistic       Expectation          Variance \r\n      0.108917530      -0.011627907       0.006221649 \r\n\r\nWe fail to reject the null hypothesis as the p-value is more than 0.05.\r\nWe could take one step further by running the random permutations of the global Moran I test by using moran.mc function.\r\n\r\n\r\nmoran_sim <- \r\n  moran.mc(msia_map_birth$change_in_birth_2015_2020, \r\n           listw = weight_list_knn, \r\n           nsim = 999,\r\n           zero.policy = TRUE,\r\n           na.action = na.omit)\r\n\r\nmoran_sim\r\n\r\n\r\n    Monte-Carlo simulation of Moran I\r\n\r\ndata:  msia_map_birth$change_in_birth_2015_2020 \r\nweights: weight_list_knn  \r\nnumber of simulations + 1: 1000 \r\n\r\nstatistic = 0.10892, observed rank = 932, p-value = 0.068\r\nalternative hypothesis: greater\r\n\r\nAgain, we saw the p-value is more than 0.05, i.e. fail to reject the null hypothesis.\r\nBelow is the simulated Moran’s I result:\r\n\r\n\r\nmoran_sim_value <- \r\n  moran_sim$res %>%\r\n  as_tibble()\r\n\r\n\r\nggplot(moran_sim_value, aes(value)) +\r\n  geom_histogram() +\r\n  geom_vline(xintercept = moran_sim$statistic, color = \"red\", size = 1.5, linetype = 2) +\r\n  xlab(\"\") +\r\n  ylab(\"Frequency\") +\r\n  labs(title = \"Histogram of Simulated Moran's I\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\nAs mentioned earlier, even if the statistical test for global spatial autocorrelation fails, it does not mean that there is any spatial autocorrelation on the local level.\r\nGeary’s C test\r\nNext, I will perform Geary’s C test by using geary.mc function as shown below.\r\n\r\n\r\ngeary.mc(msia_map_birth$change_in_birth_2015_2020, \r\n           listw = weight_list_knn, \r\n           nsim = 999,\r\n           zero.policy = TRUE)\r\n\r\n\r\n    Monte-Carlo simulation of Geary C\r\n\r\ndata:  msia_map_birth$change_in_birth_2015_2020 \r\nweights: weight_list_knn \r\nnumber of simulations + 1: 1000 \r\n\r\nstatistic = 0.93034, observed rank = 218, p-value = 0.218\r\nalternative hypothesis: greater\r\n\r\nSimilarly, we fail to reject the null hypothesis as the p-value is more than 0.05.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Ruddy Corporan on Unsplash\r\n\r\n\r\n\r\nGrekousis, George. 2020. Spatial Analysis Methods and Practice: Describe-Explore-Explain Through GIS. Cambridge University Press.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-20-global-spatial-autocorrelation/image/housing zone.jpg",
    "last_modified": "2023-02-20T23:05:27+08:00",
    "input_file": "global-spatial-autocorrelation.knit.md"
  },
  {
    "path": "posts/2023-02-17-cox-assumptions/",
    "title": "Proportion Hazard Assumption under Cox Model",
    "description": "Have you checked the assumption?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-02-17",
    "categories": [
      "Machine Learning",
      "Survival Model"
    ],
    "contents": "\r\n\r\nContents\r\nTest Cox proportional hazard\r\nWhen proportional hazard assumption is inappropriate\r\n\r\nStrata\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nTest Cox Proportional assumption\r\nStratify the variable\r\n\r\nConclusion\r\n\r\nIn previous post, I have explored on how to build a Cox proportional hazard model.\r\n\r\n\r\n\r\nPhoto by Robert Horvick on Unsplash\r\nIn this post, I will be exploring how to check the proportional hazard assumption in the model.\r\nTest Cox proportional hazard\r\nOne of the main assumptions is the hazard functions are proportional to one another over the period.\r\nTo check whether the proportional hazard is appropriate, we will perform statistical test based on scaled Schoenfeld residuals.\r\nIf the p-value is significant, the proportional hazard assumption doesn’t hold (StackExchange 2019).\r\nWhen proportional hazard assumption is inappropriate\r\nWhen the proportional hazard assumption does not hold, there are several approaches to overcome this (Sestelo 2017).\r\nStratify the variable\r\nPartition of the time axis\r\nCheck on the nonlinear effect\r\nIn this post, I will focus on how to stratify the variable.\r\nStrata\r\n(D 2017) explained that in a Cox model, stratification allows for as many different hazard functions as there are strata.\r\nBeta coefficients (hazard ratios) optimized for all strata are then fitted.\r\nDemonstration\r\nIn this demonstration, I will be using this bank dataset from Kaggle.\r\nSetup the environment\r\nFirst, I will load the necessary packages into the environment.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, janitor, survival, survminer)\r\n\r\n\r\nImport Data\r\nFirst I will import the dataset into the environment.\r\nI will also clean the column names, drop the columns I don’t need and transform the columns to be the right format.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-09-10-kaplan-meier/data/Churn_Modelling.csv\") %>%\r\n  clean_names() %>%\r\n  select(-c(row_number, customer_id, surname)) %>%\r\n  mutate(has_cr_card = factor(has_cr_card),\r\n         is_active_member = factor(is_active_member),\r\n         credit_score = credit_score/100,\r\n         balance = balance/10000,\r\n         estimated_salary = estimated_salary/10000) %>%\r\n  filter(tenure > 0)\r\n\r\n\r\nTest Cox Proportional assumption\r\nBefore we could check the proportional hazard assumption, I will build a simple model.\r\n\r\n\r\ncox_model <- \r\n  coxph(Surv(tenure, exited) ~ estimated_salary, \r\n        data = df, \r\n        x = TRUE, \r\n        y = TRUE)\r\n\r\n\r\nTo check the assumption, I will pass the fitted model to cox.zph function as follows:\r\n\r\n\r\ncox.zph(cox_model)\r\n\r\n                 chisq df    p\r\nestimated_salary  1.01  1 0.31\r\nGLOBAL            1.01  1 0.31\r\n\r\nFrom the result, the p-value is greater than 0.05, suggesting the proportional hazard assumption is appropriate for this simple model.\r\nTo visualize the result, we could pass the result from cox.zph function to plot function.\r\n\r\n\r\nplot(cox.zph(cox_model))\r\n\r\n\r\n\r\nAlternatively, we can pass the result from cox.zph function to ggcoxzph function to visualize the result.\r\nAccording to R documentation, ggcoxzph function is a wrapper around plot.cox.zph function.\r\n\r\n\r\nggcoxzph(\r\n  cox.zph(cox_model)\r\n)\r\n\r\n\r\n\r\nBelow are some of the interpretations of the result (Dessai and Patil 2019):\r\nSolid line represents the smoothing spline fit for the plot\r\nDashed lines represent +/- 2 standard error bands around the fit\r\nIf the p-value is less than 0.05, then the assumption is violated\r\nAssumption: the hazard ratio is not constant over time\r\nAssume a non-significant relationship betweeen residuals and time\r\n\r\nNext, I will check the proportional hazard assumption under a multivariate model.\r\nSimilarly, I will build a multivariate model first.\r\n\r\n\r\ncox_model_all <- \r\n  coxph(Surv(tenure, exited) ~ credit_score + gender + balance + age + num_of_products + has_cr_card + is_active_member + estimated_salary, \r\n        data = df, \r\n        x = TRUE, \r\n        y = TRUE)\r\n\r\n\r\nThen, I will pass the result to cox.zph function.\r\n\r\n\r\ncox.zph(cox_model_all)\r\n\r\n                    chisq df     p\r\ncredit_score      0.20167  1 0.653\r\ngender            0.32393  1 0.569\r\nbalance           4.25168  1 0.039\r\nage               1.71430  1 0.190\r\nnum_of_products   0.78555  1 0.375\r\nhas_cr_card       0.00105  1 0.974\r\nis_active_member  1.89779  1 0.168\r\nestimated_salary  1.30608  1 0.253\r\nGLOBAL           11.01238  8 0.201\r\n\r\nFrom the result, we note that the overall p-value is still greater than 0.05, although the p-value for balance is less than 0.05.\r\nBased on this discussion post, the author suggested since the overall test has a near-perfect multiplicity adjustment, individual tests probably is not so crucial if global p-value is greater than 0.2 unless the individual \\(\\rho\\) value (i.e. correlation between Schoenfeld residual and time) is very high.\r\nNext, I will visualize the result by using ggcoxzph function.\r\nGiven the number of variables used to build the model, the graph will be hard to read.\r\nHence, I will make some modifications to the graphs produced.\r\n\r\n\r\nggcoxzph(\r\n    cox.zph(cox_model_all)\r\n    ,font.x = 8\r\n    ,font.y = 7\r\n    ,font.tickslab = 8\r\n    ,font.main = 8)\r\n\r\n\r\n\r\nSimilar results can be observed.\r\nIf I were to convert the balance into categorical variable and visualize the survival curve, we will note that one of the survival curve interacts other curves.\r\n\r\n\r\ndf_grp <-\r\n  df %>%\r\n  mutate(balance_grp = cut(balance, breaks = c(-Inf, 5, 10, 15, 20, 25, 30)))\r\n\r\nggsurvplot(survfit(Surv(tenure, exited) ~ balance_grp, data = df_grp),\r\n           data = df_grp)\r\n\r\n\r\n\r\nStratify the variable\r\nIn this demonstration, I will explore using stratification.\r\nTo do so, I will re-build the model by stratifying the balance variable.\r\n\r\n\r\ncox_model_all_balance_strata <- \r\n  coxph(Surv(tenure, exited) ~ credit_score + gender + age + num_of_products + has_cr_card + is_active_member + estimated_salary + strata(balance), \r\n        data = df, \r\n        x = TRUE, \r\n        y = TRUE)\r\n\r\n\r\n\r\n\r\ncox_model_all_balance_strata\r\n\r\nCall:\r\ncoxph(formula = Surv(tenure, exited) ~ credit_score + gender + \r\n    age + num_of_products + has_cr_card + is_active_member + \r\n    estimated_salary + strata(balance), data = df, x = TRUE, \r\n    y = TRUE)\r\n\r\n                       coef exp(coef)  se(coef)       z        p\r\ncredit_score      -0.043836  0.957111  0.045962  -0.954    0.340\r\ngenderMale        -0.403308  0.668106  0.092768  -4.347 1.38e-05\r\nage                0.054010  1.055495  0.003697  14.609  < 2e-16\r\nnum_of_products   -0.963456  0.381572  0.089148 -10.807  < 2e-16\r\nhas_cr_card1      -0.157441  0.854327  0.099791  -1.578    0.115\r\nis_active_member1 -0.910944  0.402145  0.100939  -9.025  < 2e-16\r\nestimated_salary   0.010022  1.010073  0.008082   1.240    0.215\r\n\r\nLikelihood ratio test=432.2  on 7 df, p=< 2.2e-16\r\nn= 9587, number of events= 1942 \r\n\r\nAfter stratifying the variable, the overall p-value increases.\r\n\r\n\r\ncox.zph(cox_model_all_balance_strata)\r\n\r\n                  chisq df    p\r\ncredit_score     0.0149  1 0.90\r\ngender           0.0841  1 0.77\r\nage              0.4012  1 0.53\r\nnum_of_products  0.5101  1 0.48\r\nhas_cr_card      0.0205  1 0.89\r\nis_active_member 0.6870  1 0.41\r\nestimated_salary 0.0767  1 0.78\r\nGLOBAL           1.4938  7 0.98\r\n\r\nSimilarly, I will visualize the result by using ggcoxzph function.\r\n\r\n\r\nggcoxzph(\r\n    cox.zph(cox_model_all_balance_strata)\r\n    ,font.x = 8\r\n    ,font.y = 7\r\n    ,font.tickslab = 8\r\n    ,font.main = 8)\r\n\r\n\r\n\r\nLastly, I will extract the concordance results from the original & stratified models for comparison.\r\n\r\n\r\ncox_model_all$concordance[\"concordance\"]\r\n\r\nconcordance \r\n  0.7110915 \r\n\r\ncox_model_all_balance_strata$concordance[\"concordance\"]\r\n\r\nconcordance \r\n  0.7478856 \r\n\r\nThe concordance result improves slightly after we stratify balance.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Abbilyn Rurenko on Unsplash\r\n\r\n\r\n\r\nD, Todd. 2017. “Stratification in Cox Model.” https://stats.stackexchange.com/questions/256148/stratification-in-cox-model.\r\n\r\n\r\nDessai, Sampada, and Vijay Patil. 2019. “Testing and Interpreting Assumptions of COX Regression Analysis.” https://journals.lww.com/crst/Fulltext/2019/02010/Testing_and_interpreting_assumptions_of_COX.26.aspx.\r\n\r\n\r\nSestelo, Marta. 2017. A Short Course on Survival Analysis. https://bookdown.org/sestelo/sa_financial/non-proportional-hazards-and-now-what.html.\r\n\r\n\r\nStackExchange. 2019. “How to Correctly Interpret Schoenfeld Residuals p-Value.” https://stats.stackexchange.com/questions/433971/how-to-correctly-interpret-schoenfeld-residuals-p-value#:~:text=Schoenfeld%20is%20like%20a%20Shapiro,the%20individual%20feature%20p%2Dvalues.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-17-cox-assumptions/image/triangle.jpg",
    "last_modified": "2023-02-19T21:57:44+08:00",
    "input_file": "cox-assumptions.knit.md"
  },
  {
    "path": "posts/2023-02-12-distance-spatial-weights/",
    "title": "Distance-based Spatial Weights",
    "description": "Part 2 - Journey to find the \"neighbours\"",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-02-13",
    "categories": [
      "Geospatial Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is distance-based spatial weight?\r\nTypes of distance-based spatial weights\r\nBest practice on how to select the appropriate spatial weighting method\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nImport shp files\r\n\r\nDistance-based spatial weight\r\nK Nearest Neighbours\r\nFixed distance\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Nadine Shaabana on Unsplash\r\nPreviously we discussed contiguity-based spatial weight.\r\nIn this post, we will focus on another spatial weighting method, which is distance-based spatial weights.\r\nWhat is distance-based spatial weight?\r\n(Rey, Arribas-Bel, and Wolf 2020) explained that distance-based weight can be defined as neighbour relations as a function of the distance separating spatial observations.\r\nIn other words, this approach determines who are the neighbours by using the distance, instead of which areas are connected.\r\nTypes of distance-based spatial weights\r\nBelow are two common distance-based spatial weights approaches:\r\nK nearest neighbours weights\r\n(Rey, Arribas-Bel, and Wolf 2020) defines this type of distance-based weight as the neighbour set of a particular observation containing its nearest k observations, where the user specifies the value of k.\r\nFixed distance\r\nAs the name suggested, this approach defines to be the neighbour of the selected district, the distance between them must be within the specified range.\r\nWe will see more about how this fixed distance spatial weight works in the demonstration.\r\nBest practice on how to select the appropriate spatial weighting method\r\nThis was discussed in my previous post.\r\nPlease refer to the previous post for more info.\r\nDemonstration\r\nI will download Malaysia shape files from this link.\r\nFor more explanations on shape files, please refer to my previous post.\r\nSetup the environment\r\nFirst, I will setup the environment by calling the necessary packages.\r\n\r\n\r\npacman::p_load(tidyverse, sf, spdep, tmap, janitor)\r\n\r\n\r\nI will also set the tmap_mode to view so that I can interact with the graphs.\r\n\r\n\r\ntmap_mode('view')\r\n\r\n\r\nImport the data\r\nImport shp files\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\nmsia_map <- st_read(dsn = \"data\", layer = \"MYS_adm2\")\r\n\r\nReading layer `MYS_adm2' from data source \r\n  `C:\\Users\\jaspe\\OneDrive\\Documents\\2_Data Science\\98_my-blog\\_posts\\2023-02-12-distance-spatial-weights\\data' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 144 features and 14 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 99.64072 ymin: 0.855001 xmax: 119.2697 ymax: 7.380556\r\nGeodetic CRS:  WGS 84\r\n\r\nNext, I will visualize the Malaysia map.\r\n\r\n\r\ntm_shape(msia_map) +\r\n  tm_polygons()\r\n\r\n\r\n\r\nGood! Now, we can proceed and find the “neighbours”.\r\nDistance-based spatial weight\r\nIn this sub-section, I will derive the distance-based spatial weights.\r\nTo better visualize the spatial weights results later, I will find the centroids of different administrative districts.\r\n\r\n\r\nlongitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[1]])\r\nlatitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[2]])\r\n\r\n\r\nOnce the latitude and longitude are derived, I will use cbind function to bind the columns together.\r\n\r\n\r\ncoords <- cbind(longitude, latitude)\r\n\r\n\r\nK Nearest Neighbours\r\nI will pass the created coords object into knearneigh function.\r\n\r\n\r\nkneigh <- knearneigh(coords, k = 4)\r\n\r\n\r\nAccording to the documentation, the function returns a matrix with the indices of points belonging to the set of k nearest neighbours of each other.\r\nI have sliced the first 10 rows from the data to have a look at who are their neighbours.\r\n\r\n\r\nkneigh$nn[1:10, 1:4]\r\n\r\n      [,1] [,2] [,3] [,4]\r\n [1,]    8    3    6    9\r\n [2,]    5    4    9    3\r\n [3,]    1    5    7    9\r\n [4,]    2    5    9    7\r\n [5,]    9    2    4    3\r\n [6,]   36    8   44   10\r\n [7,]    3    4    5   10\r\n [8,]    6    1   36   10\r\n [9,]    5    2    3    1\r\n[10,]    6    8   54   44\r\n\r\nOnce we find the neighbours, we will use knn2nb function to convert the objects into nb so that we can use the plot function to visualize the results.\r\n\r\n\r\nknn <- knn2nb(kneigh)\r\n\r\n\r\nFantastic!\r\nI will then pass the objects to the plotting function to visualize the results.\r\n\r\n\r\nplot(msia_map$geometry, border = \"lightgrey\")\r\nplot(knn, coords, add = TRUE, col = \"blue\")\r\n\r\n\r\n\r\nNoted that under this approach, everyone has the same number of neighbours.\r\nFixed distance\r\nNow, let’s move on to another distance-based spatial weight approach, which is fixed distance.\r\nTo do so, I will use dnearneigh function.\r\nI will need to specify the lower and upper distance bounds in the function.\r\nOver here, I have specified the lower and upper distance bounds are 0 and 100 respectively.\r\n\r\n\r\nfixed_d <- dnearneigh(coords, 0, 100, longlat = TRUE)\r\n\r\n\r\nspdep package offers a function to extract the number of neighbours of each area.\r\n\r\n\r\ncard(fixed_d)\r\n\r\n  [1] 11  6 10  5  7 15  8 14  7 15 18 18 15 19 11 19  6 11 18 15 18\r\n [22] 19 10 10 11 10 14 13 10 12 12  9 20  6 17 16 15 24 22 23 19 22\r\n [43] 22 18 20 18 11  8  7  8 11  6 16 11 16 12 10 16 18 17 12 15 15\r\n [64] 11  9  9 16 16 18 19 16 20 11  5 13  3 11 11  9 10  3  4  5  4\r\n [85] 12 11  5 10 12  3  3  8 12  4  9  5  9  8  6  0 11  2  9  9 14\r\n[106] 11  0  6  6  3  7  4  1 11 13  1  6 12  8 12 11  9  5 12  8  6\r\n[127]  8  3 19 21 18 16 16 16 18 13 20 14  7 11  6  8  6 12\r\n\r\nOh no! Some of the districts (i.e. the two colored districts in the graph below) don’t have any neighbours as they seem to be far from everyone.\r\n\r\n\r\n\r\nSo if we were to visualize the result by using our usual plotting function, we will see that there are two nodes within the graph that don’t have any edges.\r\n\r\n\r\nplot(msia_map$geometry, border = \"lightgrey\")\r\nplot(fixed_d, coords, add = TRUE, col = \"blue\")\r\n\r\n\r\n\r\nTo overcome this issue, we could increase the upper distance bound to a higher number.\r\nAs such, we will find the largest distance between the first nearest neighbours and\r\nIn this example, I will take the max distance from K nearest neighbours as the upper distance bound.\r\nThen I will use nbdists function to calculate the Euclidean distances along the links.\r\n\r\n\r\ndist_new <- nbdists(knn, coords, longlat = TRUE)\r\n\r\n\r\nThen I will unlist the object to convert it into a list before passing it to summary function to find the summary statistics.\r\n\r\n\r\ndist_new_unlist <- unlist(dist_new)\r\n\r\n\r\n\r\n\r\nsummary(dist_new_unlist)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  8.261  29.384  40.714  45.795  56.039 163.055 \r\n\r\nGreat!\r\nThe largest distance between the first nearest neighbours is 163.055km from the summary statistics shown above.\r\n\r\nTaken from giphy\r\nNow, we can find the neighbours by using the updated upper distance bound.\r\n\r\n\r\nfixed_d_new <- dnearneigh(coords, 0, 163.1, longlat = TRUE)\r\n\r\nplot(msia_map$geometry, border = \"lightgrey\")\r\nplot(fixed_d_new, coords, add = TRUE, col = \"blue\")\r\n\r\n\r\n\r\nNow all the districts have “neighbours”.\r\nWe can also call the created object to see the summary result.\r\n\r\n\r\nfixed_d_new\r\n\r\nNeighbour list object:\r\nNumber of regions: 144 \r\nNumber of nonzero links: 3120 \r\nPercentage nonzero weights: 15.0463 \r\nAverage number of links: 21.66667 \r\n\r\nFrom the result, we noted the following:\r\nThere are about 3,120 edges in total\r\nOn average, each district has 21.6667 neighbours\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Dariusz Sankowski on Unsplash\r\n\r\n\r\n\r\nRey, Sergio J., Dani Arribas-Bel, and Levi J. Wolf. 2020. Spatial Weights. https://geographicdata.science/book/notebooks/04_spatial_weights.html#introduction.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-12-distance-spatial-weights/image/hands.jpg",
    "last_modified": "2023-02-12T09:20:40+08:00",
    "input_file": "distance-spatial-weights.knit.md"
  },
  {
    "path": "posts/2023-02-11-k-proto/",
    "title": "K-proto Clustering",
    "description": "Sorting numeric and categorical data at one go",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2023-02-11",
    "categories": [
      "Machine Learning",
      "Unsupervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nLimitation of K-means clustering\r\nK-Prototypes\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nKproto clustering\r\nVisualize the result\r\nElbow Curve\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Karolina Grabowska from Pexels\r\nRecently, I happened to come across a paper on how clustering can be used in actuarial valuation.\r\nIn the paper, the author used clustering algorithm to find the representative model points for a group of variable annuities before calculating the market value and Greeks of every contract in the portfolio.\r\nThis has reduced the number of model points, which reduced the computing time significantly. In the paper, the method is estimated about 66 times faster than the conventional method.\r\nHence, in this post, I will be exploring cluster algorithm, k-prototypes.\r\nAs the discussion will be building on my previous post on clustering, this would be a rather short post.\r\nBelow are the links to the previous posts:\r\nPurpose & considerations of clustering\r\nK-means clustering\r\nLimitation of K-means clustering\r\nOne of the issue of using K-means clustering algorithm is this method could only take in numeric variables.\r\nTo overcome this, one could convert the non-numeric variables into dummy variables.\r\nHowever, it is not ideal to convert the non-numeric variables to dummy variables and perform the clustering (IBM, n.d.).\r\nK-Prototypes\r\nK-Prototypes is a clustering method that allows one to perform clustering on mixed data types.\r\nThe author explained that the k-prototypes algorithm combines the “means” of the numerical part and the “modes” of the categorical part to build a new hybrid Cluster Center “prototype”(Soliya, n.d.).\r\nApplication of data clustering and machine learning in variable annuity valuation\r\nThe distance between two records x and y can be defined as:\r\n\\(D(x,y,\\lambda)=\\sqrt{\\sum_{h=1}^{d_1}(x_h-y_h)^2+\\lambda\\sum_{h=d_1+1}^{d}\\delta({x_h,y_h})}\\)\r\nwhere:\r\nthe first \\(d_1\\) attributes are numeric and the last \\(d_2=d-d_1\\) attribures are categorical\r\n\\({x_h}\\) and \\({y_h}\\) are the hth component of x and y, respectively \\(\\lambda\\)\r\nsimple matching distance is defined as:\r\n\\(\\delta({x_h,y_h})=\\{_{1,\\ if\\ x_h\\ \\neq\\ y_h}^{0,\\ if\\ x_h\\ =\\ y_h}\\)\r\nThe objective function for k-prototypes algorithm is to minimize the following function:\r\n\\(P_{\\lambda}=\\sum_{j=i}^{k}\\sum_{x\\in C_{j}}D^2(x, \\mu_j, \\lambda)\\)\r\nwhere:\r\nk is the number of clusters\r\n\\(C_j\\) is the jth cluster\r\n\\(\\mu_j\\) is the center or prototype of cluster \\(C_j\\)\r\nOkay, that’s all the discussions on the algorithm.\r\nLet’s start the demonstration!\r\nDemonstration\r\nFor this demonstration, I will be using bank marketing data from UCI Machine Learning Repository.\r\nThis dataset contains data points from past direct marketing campaigns of a Portuguese banking institution.\r\n\r\n\r\n\r\nPhoto by Monstera from Pexels\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I need for the analysis later.\r\n\r\n\r\n\r\n\r\n\r\npacman::p_load(tidyverse, readr, clustMixType)\r\n\r\n\r\nImport the data\r\nNext, I will import the data into the environment.\r\n\r\n\r\ndf <- read_delim(\"data/bank-full.csv\", delim = \";\") %>%\r\n  mutate(across(where(is.character), as.factor)) %>%\r\n  mutate(month = ordered(month, levels = c(\"jan\",\r\n                                          \"feb\",\r\n                                          \"mar\",\r\n                                          \"apr\",\r\n                                          \"may\",\r\n                                          \"jun\",\r\n                                          \"jul\",\r\n                                          \"aug\",\r\n                                          \"sep\",\r\n                                          \"oct\",\r\n                                          \"nov\",\r\n                                          \"dec\"))) %>%\r\n  sample_frac(0.5)\r\n\r\n\r\nKproto clustering\r\nOkay, let’s start the demonstration on clustering by using kproto function.\r\nNote that similar to other clustering algorithms, we will need to specify the number of clusters upfront before running the clustering algorithm.\r\n\r\n\r\nkpres <- kproto(df, 5)\r\n\r\n# NAs in variables:\r\n      age       job   marital education   default   balance   housing \r\n        0         0         0         0         0         0         0 \r\n     loan   contact       day     month  duration  campaign     pdays \r\n        0         0         0         0         0         0         0 \r\n previous  poutcome         y \r\n        0         0         0 \r\n0 observation(s) with NAs.\r\n\r\nEstimated lambda: 2716532 \r\n\r\n0 observation(s) with NAs.\r\n\r\nTo understand the characteristics of the different clusters, we could call the clustering results.\r\n\r\n\r\nkpres\r\n\r\nDistance type: standard \r\n\r\nNumeric predictors: 7 \r\nCategorical predictors: 10 \r\nLambda: 2716532 \r\n\r\nNumber of Clusters: 5 \r\nCluster sizes: 1748 7508 6827 147 6376 \r\nWithin cluster error: 31206981321 65081637216 52424519586 19105775396 57499226991 \r\n\r\nCluster prototypes:\r\n       age         job marital education default    balance housing\r\n1 43.71110  management married  tertiary      no  6965.3312      no\r\n2 43.61015  management married  tertiary      no   799.6534      no\r\n3 41.39065 blue-collar married secondary      no   856.1601     yes\r\n4 45.80272  management married  tertiary      no 25419.6599      no\r\n5 36.61841      admin.  single secondary      no   446.6633     yes\r\n  loan  contact      day month duration campaign    pdays  previous\r\n1   no cellular 16.33753   nov 275.6150 2.609840 39.76945 0.7500000\r\n2   no cellular 16.23455   aug 253.3195 2.924747 31.28663 0.5575386\r\n3   no  unknown 15.50051   may 257.5398 2.741907 29.51164 0.3644353\r\n4   no cellular 16.46939   nov 253.0000 2.931973 37.84354 0.7142857\r\n5   no cellular 15.37171   may 260.1371 2.532465 63.33140 0.8105395\r\n  poutcome  y\r\n1  unknown no\r\n2  unknown no\r\n3  unknown no\r\n4  unknown no\r\n5  unknown no\r\n\r\nIn the results, the numeric value represents the mean value of the cluster and mode for the categorical variables.\r\nFollowing are some of the insights gathered from the clustering:\r\nAmong the groups, the customers under group 4 have the highest average balance, while group 3 has the lowest average balance\r\nDespite that the customers under group 4 & 6 are rather similar, i.e. both groups are mostly married and have a tertiary education, the average balance under group 6 is somewhat much lower than group 4\r\nSimilarly, the customer characteristics under group 3 & 5 are also quite similar, the average balance of the customers under group 5 are more than 3 times of the average balance of group 3\r\nAs mentioned earlier, the algorithm requires us to upfront indicate the number of clusters. To find the optimal number of clusters, we will loop through the clustering algorithm through different numbers.\r\nAlternatively, we could pass the results into summary function.\r\n\r\n\r\nsummary(kpres)\r\n\r\nage \r\n  Min. 1st Qu. Median     Mean 3rd Qu. Max.\r\n1   20      34     42 43.71110      53   86\r\n2   18      34     42 43.61015      52   93\r\n3   21      34     40 41.39065      48   86\r\n4   24      35     45 45.80272      55   84\r\n5   18      30     34 36.61841      42   90\r\n\r\n-----------------------------------------------------------------\r\njob \r\n       \r\ncluster admin. blue-collar entrepreneur housemaid management retired\r\n      1  0.083       0.147        0.033     0.032      0.291   0.075\r\n      2  0.038       0.076        0.037     0.041      0.418   0.083\r\n      3  0.059       0.488        0.030     0.022      0.074   0.038\r\n      4  0.061       0.068        0.102     0.027      0.422   0.095\r\n      5  0.275       0.104        0.030     0.019      0.086   0.019\r\n       \r\ncluster self-employed services student technician unemployed unknown\r\n      1         0.045    0.067   0.018      0.163      0.034   0.012\r\n      2         0.046    0.046   0.015      0.161      0.031   0.009\r\n      3         0.024    0.104   0.005      0.132      0.021   0.004\r\n      4         0.061    0.014   0.014      0.082      0.041   0.014\r\n      5         0.032    0.145   0.042      0.218      0.029   0.003\r\n\r\n-----------------------------------------------------------------\r\nmarital \r\n       \r\ncluster divorced married single\r\n      1    0.098   0.643  0.259\r\n      2    0.104   0.709  0.187\r\n      3    0.097   0.800  0.104\r\n      4    0.116   0.667  0.218\r\n      5    0.151   0.252  0.597\r\n\r\n-----------------------------------------------------------------\r\neducation \r\n       \r\ncluster primary secondary tertiary unknown\r\n      1   0.148     0.399    0.404   0.049\r\n      2   0.137     0.245    0.573   0.045\r\n      3   0.243     0.624    0.091   0.043\r\n      4   0.075     0.286    0.605   0.034\r\n      5   0.072     0.752    0.145   0.030\r\n\r\n-----------------------------------------------------------------\r\ndefault \r\n       \r\ncluster    no   yes\r\n      1 0.999 0.001\r\n      2 0.983 0.017\r\n      3 0.983 0.017\r\n      4 1.000 0.000\r\n      5 0.975 0.025\r\n\r\n-----------------------------------------------------------------\r\nbalance \r\n   Min. 1st Qu.  Median       Mean  3rd Qu.  Max.\r\n1  3679  4849.5  5978.0  6965.3312  8281.50 16119\r\n2 -6847    73.0   447.5   799.6534  1228.00  4089\r\n3 -3313    87.0   530.0   856.1601  1335.00  4746\r\n4 16264 18189.5 22196.0 25419.6599 26770.00 81204\r\n5 -2122    11.0   221.0   446.6633   544.25  4464\r\n\r\n-----------------------------------------------------------------\r\nhousing \r\n       \r\ncluster    no   yes\r\n      1 0.537 0.463\r\n      2 0.809 0.191\r\n      3 0.181 0.819\r\n      4 0.578 0.422\r\n      5 0.279 0.721\r\n\r\n-----------------------------------------------------------------\r\nloan \r\n       \r\ncluster    no   yes\r\n      1 0.921 0.079\r\n      2 0.855 0.145\r\n      3 0.827 0.173\r\n      4 0.952 0.048\r\n      5 0.819 0.181\r\n\r\n-----------------------------------------------------------------\r\ncontact \r\n       \r\ncluster cellular telephone unknown\r\n      1    0.697     0.089   0.215\r\n      2    0.853     0.071   0.076\r\n      3    0.254     0.074   0.672\r\n      4    0.721     0.088   0.190\r\n      5    0.817     0.044   0.139\r\n\r\n-----------------------------------------------------------------\r\nday \r\n  Min. 1st Qu. Median     Mean 3rd Qu. Max.\r\n1    1      10     18 16.33753      21   31\r\n2    1       8     17 16.23455      23   31\r\n3    1       8     15 15.50051      21   31\r\n4    1      11     18 16.46939      21   31\r\n5    1       8     15 15.37171      21   31\r\n\r\n-----------------------------------------------------------------\r\nmonth \r\n       \r\ncluster   jan   feb   mar   apr   may   jun   jul   aug   sep   oct\r\n      1 0.017 0.054 0.017 0.078 0.186 0.141 0.106 0.132 0.016 0.028\r\n      2 0.043 0.078 0.018 0.055 0.050 0.085 0.189 0.323 0.022 0.026\r\n      3 0.011 0.024 0.002 0.051 0.560 0.210 0.085 0.012 0.003 0.004\r\n      4 0.014 0.068 0.007 0.061 0.184 0.122 0.054 0.177 0.020 0.027\r\n      5 0.045 0.081 0.008 0.085 0.359 0.052 0.204 0.051 0.010 0.014\r\n       \r\ncluster   nov   dec\r\n      1 0.217 0.009\r\n      2 0.102 0.008\r\n      3 0.038 0.001\r\n      4 0.259 0.007\r\n      5 0.086 0.004\r\n\r\n-----------------------------------------------------------------\r\nduration \r\n  Min. 1st Qu. Median     Mean 3rd Qu. Max.\r\n1    0     107    185 275.6150  344.25 2372\r\n2    4     102    173 253.3195  311.00 3422\r\n3    3     102    181 257.5398  319.00 3785\r\n4    8      95    175 253.0000  323.50 1598\r\n5    0     104    183 260.1371  323.00 3253\r\n\r\n-----------------------------------------------------------------\r\ncampaign \r\n  Min. 1st Qu. Median     Mean 3rd Qu. Max.\r\n1    1       1      2 2.609840       3   29\r\n2    1       1      2 2.924747       3   38\r\n3    1       1      2 2.741907       3   55\r\n4    1       1      2 2.931973       3   31\r\n5    1       1      2 2.532465       3   38\r\n\r\n-----------------------------------------------------------------\r\npdays \r\n  Min. 1st Qu. Median     Mean 3rd Qu. Max.\r\n1   -1      -1     -1 39.76945      -1  779\r\n2   -1      -1     -1 31.28663      -1  854\r\n3   -1      -1     -1 29.51164      -1  808\r\n4   -1      -1     -1 37.84354      -1  589\r\n5   -1      -1     -1 63.33140      79  842\r\n\r\n-----------------------------------------------------------------\r\nprevious \r\n  Min. 1st Qu. Median      Mean 3rd Qu. Max.\r\n1    0       0      0 0.7500000       0   35\r\n2    0       0      0 0.5575386       0   38\r\n3    0       0      0 0.3644353       0   30\r\n4    0       0      0 0.7142857       0   23\r\n5    0       0      0 0.8105395       1   55\r\n\r\n-----------------------------------------------------------------\r\npoutcome \r\n       \r\ncluster failure other success unknown\r\n      1   0.130 0.043   0.048   0.779\r\n      2   0.097 0.034   0.050   0.819\r\n      3   0.077 0.025   0.011   0.887\r\n      4   0.095 0.041   0.054   0.810\r\n      5   0.162 0.063   0.035   0.740\r\n\r\n-----------------------------------------------------------------\r\ny \r\n       \r\ncluster    no   yes\r\n      1 0.839 0.161\r\n      2 0.842 0.158\r\n      3 0.942 0.058\r\n      4 0.850 0.150\r\n      5 0.875 0.125\r\n\r\n-----------------------------------------------------------------\r\n\r\nVisualize the result\r\nTo visualize the clustering result, we could use plot function.\r\n\r\n\r\nplot(kpres, vars = \"job\")\r\n\r\n\r\n\r\nIf a numeric variable is being passed to the function, boxplot will be used to show the distribution.\r\n\r\n\r\nplot(kpres, vars = \"age\")\r\n\r\n\r\n\r\nAlternatively, we can use ggplot to visualize the result.\r\n\r\n\r\nfor (i in 1:5){\r\n  graph <- df %>%\r\n    bind_cols(tibble(cluster = kpres$cluster)) %>%\r\n    filter(cluster == i) %>%\r\n    group_by(job) %>%\r\n    tally() %>%\r\n    rename(count = n) %>%\r\n    ggplot(aes(x = reorder(job, -count), \r\n               y = count/sum(count),\r\n               alpha = 0.8)) +\r\n    geom_col() +\r\n    scale_y_continuous(labels = scales::percent) +\r\n    xlab(\"Jobs\") +\r\n    ylab(\"Proportion\") +\r\n    labs(title = paste0(\"Cluster \", i, \" - Proportion of Jobs\")) +\r\n    theme(axis.text.x = element_text(angle = 90), \r\n          legend.position=\"none\")\r\n  \r\n  print(graph)\r\n  \r\n}\r\n\r\n\r\n\r\nElbow Curve\r\n\r\n\r\nk_num <- 8\r\ntot_wss <- sapply(1:k_num, \r\n                  function(k){kproto(df, k)$tot.withinss})\r\n\r\n\r\nNext, I will convert the list to a tibble table and include the respective number of clusters.\r\n\r\n\r\nwithness_cluster <- \r\n  as_tibble(tot_wss) %>%\r\n  rename(tot_withness = value) %>%\r\n  mutate(cluster = row_number())\r\n\r\n\r\nThis will allow us to visualize the results by using ggplot function.\r\n\r\n\r\nggplot(withness_cluster, \r\n       aes(x = cluster, y = tot_withness)) +\r\n  geom_point() +\r\n  geom_line() +\r\n  labs(title = \"Elbow Curve\")\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\n*Took this when I went to Lang Tengah, Malaysia\r\n\r\n\r\n\r\nIBM. n.d. “Clustering Binary Data with k-Means (Should Be Avoided).” https://www.ibm.com/support/pages/clustering-binary-data-k-means-should-be-avoided.\r\n\r\n\r\nSoliya, Shivam. n.d. “Customer Segmentation Using k-Prototypes Algorithm in Python.” https://medium.com/analytics-vidhya/customer-segmentation-using-k-prototypes-algorithm-in-python-aad4acbaaede.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-11-k-proto/image/mix.jpg",
    "last_modified": "2023-02-04T15:56:33+08:00",
    "input_file": "Clustering_Kproto.knit.md"
  },
  {
    "path": "posts/2023-02-08-survival-decision-tree/",
    "title": "Survival Decision Tree",
    "description": "Treeeeeeeeeeeee",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-02-08",
    "categories": [
      "Machine Learning",
      "Survival Model"
    ],
    "contents": "\r\n\r\nContents\r\nSurvival analysis with decision tree\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nBuild decision trees\r\nMethod 1 - Use rpart package\r\nPrune the tree\r\nMethod 1: Choose minimum xerror\r\nMethod 2: Choose the xerror < minimum(xerror) + xstd\r\nVisualize the tree\r\nVisualize survival curve\r\n\r\nMethod 2 - Use ctree package\r\nMethod 3 - Use tidymodels packages to build the trees\r\n\r\n\r\nConclusion\r\n\r\nIn previous posts, I have explored using cox proportional hazards model to perform survival analysis.\r\n(Gepp and Kumar 2015) found out decision trees could have a better classification accuracy than other techniques (eg. cox proportional hazards model).\r\n\r\n\r\n\r\nPhoto by Anders Nord on Unsplash\r\nThis makes me curious on how to build a decision tree to perform survival analysis.\r\nI have read through different examples, posts and articles to satisfy my own curiousity, otherwise I will not be able to sleep at night.\r\nTaken from giphy\r\nFor the explanations on survival analysis, you may refer to my previous post.\r\nAlso, in this post, I will assume the readers have some basic understanding on how decision trees work.\r\nSurvival analysis with decision tree\r\nThe different decision tree R packages support survival analysis.\r\nDemonstration\r\nIn this demonstration, I will be using this bank dataset from Kaggle.\r\nAlso, there are many ways we could build a survival decision tree.\r\nTherefore, I will be using 3 different methods to build survival decision trees.\r\nSetup the environment\r\nFirst, I will load the necessary packages into the environment.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, janitor, survival, survminer, partykit, rpart, censored)\r\n\r\n\r\nImport Data\r\nFirst I will import the dataset into the environment.\r\nI will also clean the column names, drop the columns I don’t need and transform the columns to be the right format.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-09-10-kaplan-meier/data/Churn_Modelling.csv\") %>%\r\n  clean_names() %>%\r\n  select(-c(row_number, customer_id, surname)) %>%\r\n  mutate(has_cr_card = factor(has_cr_card),\r\n         is_active_member = factor(is_active_member)) %>%\r\n  filter(tenure > 0)\r\n\r\n\r\nBuild decision trees\r\nNow, let’s build the decision tree!\r\nMethod 1 - Use rpart package\r\nIn this first method, I will use rpart package to build the tree.\r\nAs usual, the target variable will be our surv object, which contains the duration and status.\r\n\r\n\r\nrpart_fit <-\r\n  rpart(Surv(tenure, exited) ~ ., data = df)\r\n\r\n\r\nWe could call the fitted object to output the results.\r\n\r\n\r\nrpart_fit\r\n\r\nn= 9587 \r\n\r\nnode), split, n, deviance, yval\r\n      * denotes terminal node\r\n\r\n 1) root 9587 7780.7980 1.0000000  \r\n   2) age< 41.5 6510 3683.8010 0.5459338  \r\n     4) num_of_products< 2.5 6365 3295.1530 0.4751260  \r\n       8) num_of_products>=1.5 3237  915.7341 0.1968239 *\r\n       9) num_of_products< 1.5 3128 2144.6720 0.7674072 *\r\n     5) num_of_products>=2.5 145  147.4370 3.6416170 *\r\n   3) age>=41.5 3077 3322.9470 1.9579020  \r\n     6) is_active_member=1 1660 1540.7610 1.3032990 *\r\n     7) is_active_member=0 1417 1629.8980 2.6766400  \r\n      14) age< 47.5 785  836.4027 1.8475160 *\r\n      15) age>=47.5 632  694.6653 3.7496700 *\r\n\r\nFrom the result, we can observe the following:\r\nThe split condition at each node\r\nNumber of data points fall under each node\r\nDeviance at each node\r\nFitted value of the response at the node\r\nNote the root node value is always fixed to 1\r\nThe yval of the subsequent nodes represent the relative riskness compared to the overall\r\nEg. The exited rate at node 3 is about 1.96 times the overall rate\r\n\r\nAlternatively, we could pass the model into summary function, which gives us more info on the model.\r\n\r\n\r\nsummary(rpart_fit)\r\n\r\nCall:\r\nrpart(formula = Surv(tenure, exited) ~ ., data = df)\r\n  n= 9587 \r\n\r\n          CP nsplit rel error    xerror       xstd\r\n1 0.09948214      0 1.0000000 1.0001186 0.01418356\r\n2 0.03100081      1 0.9005179 0.9021411 0.01447699\r\n3 0.03017004      2 0.8695171 0.8769640 0.01458997\r\n4 0.01957221      3 0.8393470 0.8411863 0.01427720\r\n5 0.01270177      4 0.8197748 0.8221423 0.01436621\r\n6 0.01000000      5 0.8070730 0.8132424 0.01433770\r\n\r\nVariable importance\r\n             age  num_of_products is_active_member          balance \r\n              54               29                9                6 \r\n    credit_score        geography \r\n               1                1 \r\n\r\nNode number 1: 9587 observations,    complexity param=0.09948214\r\n  events=1942,  estimated rate=1 , mean deviance=0.8115989 \r\n  left son=2 (6510 obs) right son=3 (3077 obs)\r\n  Primary splits:\r\n      age              < 41.5     to the left,  improve=774.0515, (0 missing)\r\n      num_of_products  < 2.5      to the left,  improve=392.6679, (0 missing)\r\n      geography        splits as  LRL,          improve=200.8115, (0 missing)\r\n      is_active_member splits as  RL,           improve=169.5380, (0 missing)\r\n      balance          < 6229.595 to the left,  improve=124.5847, (0 missing)\r\n  Surrogate splits:\r\n      num_of_products  < 2.5      to the left,  agree=0.682, adj=0.010, (0 split)\r\n      credit_score     < 404.5    to the right, agree=0.680, adj=0.004, (0 split)\r\n      balance          < 212694.6 to the left,  agree=0.679, adj=0.000, (0 split)\r\n      estimated_salary < 94.01    to the right, agree=0.679, adj=0.000, (0 split)\r\n\r\nNode number 2: 6510 observations,    complexity param=0.03100081\r\n  events=719,  estimated rate=0.5459338 , mean deviance=0.5658681 \r\n  left son=4 (6365 obs) right son=5 (145 obs)\r\n  Primary splits:\r\n      num_of_products < 2.5      to the left,  improve=241.27750, (0 missing)\r\n      geography       splits as  LRL,          improve= 88.13687, (0 missing)\r\n      age             < 34.5     to the left,  improve= 79.68383, (0 missing)\r\n      balance         < 21009.38 to the left,  improve= 66.77857, (0 missing)\r\n      gender          splits as  RL,           improve= 45.01281, (0 missing)\r\n  Surrogate splits:\r\n      credit_score < 385.5    to the right, agree=0.978, adj=0.014, (0 split)\r\n\r\nNode number 3: 3077 observations,    complexity param=0.01957221\r\n  events=1223,  estimated rate=1.957902 , mean deviance=1.079931 \r\n  left son=6 (1660 obs) right son=7 (1417 obs)\r\n  Primary splits:\r\n      is_active_member splits as  RL,           improve=152.29040, (0 missing)\r\n      num_of_products  < 1.5      to the right, improve=105.33000, (0 missing)\r\n      age              < 65.5     to the right, improve= 68.64285, (0 missing)\r\n      geography        splits as  LRL,          improve= 67.56701, (0 missing)\r\n      balance          < 87573.12 to the left,  improve= 34.90691, (0 missing)\r\n  Surrogate splits:\r\n      age             < 49.5     to the right, agree=0.593, adj=0.116, (0 split)\r\n      geography       splits as  LRL,          agree=0.550, adj=0.023, (0 split)\r\n      credit_score    < 485.5    to the right, agree=0.548, adj=0.018, (0 split)\r\n      num_of_products < 2.5      to the left,  agree=0.547, adj=0.016, (0 split)\r\n      balance         < 161847.5 to the left,  agree=0.543, adj=0.007, (0 split)\r\n\r\nNode number 4: 6365 observations,    complexity param=0.03017004\r\n  events=612,  estimated rate=0.475126 , mean deviance=0.5176988 \r\n  left son=8 (3237 obs) right son=9 (3128 obs)\r\n  Primary splits:\r\n      num_of_products  < 1.5      to the right, improve=234.75170, (0 missing)\r\n      geography        splits as  LRL,          improve= 74.62815, (0 missing)\r\n      age              < 34.5     to the left,  improve= 64.71067, (0 missing)\r\n      balance          < 21009.38 to the left,  improve= 58.77733, (0 missing)\r\n      is_active_member splits as  RL,           improve= 42.02675, (0 missing)\r\n  Surrogate splits:\r\n      balance          < 21009.38 to the left,  agree=0.706, adj=0.401, (0 split)\r\n      age              < 39.5     to the left,  agree=0.522, adj=0.028, (0 split)\r\n      geography        splits as  LRL,          agree=0.517, adj=0.018, (0 split)\r\n      credit_score     < 572.5    to the right, agree=0.515, adj=0.013, (0 split)\r\n      estimated_salary < 186897.4 to the left,  agree=0.510, adj=0.004, (0 split)\r\n\r\nNode number 5: 145 observations\r\n  events=107,  estimated rate=3.641617 , mean deviance=1.016807 \r\n\r\nNode number 6: 1660 observations\r\n  events=427,  estimated rate=1.303299 , mean deviance=0.9281695 \r\n\r\nNode number 7: 1417 observations,    complexity param=0.01270177\r\n  events=796,  estimated rate=2.67664 , mean deviance=1.150246 \r\n  left son=14 (785 obs) right son=15 (632 obs)\r\n  Primary splits:\r\n      age             < 47.5     to the left,  improve=98.844360, (0 missing)\r\n      num_of_products < 1.5      to the right, improve=54.817880, (0 missing)\r\n      geography       splits as  LRL,          improve=18.576390, (0 missing)\r\n      gender          splits as  RL,           improve=12.347840, (0 missing)\r\n      balance         < 6229.595 to the left,  improve= 7.494066, (0 missing)\r\n  Surrogate splits:\r\n      geography        splits as  LRL,          agree=0.572, adj=0.040, (0 split)\r\n      credit_score     < 768.5    to the left,  agree=0.565, adj=0.025, (0 split)\r\n      num_of_products  < 2.5      to the left,  agree=0.560, adj=0.014, (0 split)\r\n      estimated_salary < 14041.56 to the right, agree=0.559, adj=0.011, (0 split)\r\n      balance          < 166745.5 to the left,  agree=0.555, adj=0.003, (0 split)\r\n\r\nNode number 8: 3237 observations\r\n  events=129,  estimated rate=0.1968239 , mean deviance=0.2828959 \r\n\r\nNode number 9: 3128 observations\r\n  events=483,  estimated rate=0.7674072 , mean deviance=0.6856368 \r\n\r\nNode number 14: 785 observations\r\n  events=312,  estimated rate=1.847516 , mean deviance=1.065481 \r\n\r\nNode number 15: 632 observations\r\n  events=484,  estimated rate=3.74967 , mean deviance=1.099154 \r\n\r\nWoh, there are a lot of info within the summary function.\r\nThe results can be split into different parts.\r\n\r\n\r\n\r\nSummary\r\nFirst part: Results under different complexity parameters\r\nSecond part: Variable importance\r\nNote that according to the documentation, the variable importance is being scaled so that they will add up to 100\r\n\r\nThird part: Tree results under each split\r\nPrune the tree\r\nTo prune the tree, plotcp function can be very handy.\r\nIt plots the cross validation results based on the rpart object.\r\nForm this post, it seems like there are different methods in choosing where to prune the tree.\r\nBased on the explanation in the post, below are the three common methods in selecting the optimal levels for pruning:\r\nMethod 1: Choose minimum xerror\r\nMethod 2: Choose the xerror < minimum(xerror) + xstd\r\nThis method accounts for the variability of xerror from cross-validation\r\n\r\nMethod 3: Choose xerror +/- xstd overlaps with minimum(xerror) +/- xstd\r\nSince the author mentioned the third method is rarely used, I will focus on method 1 and 2 over here.\r\nMethod 1: Choose minimum xerror\r\nFirst, I will print out the cp by using printcp function.\r\n\r\n\r\nprintcp(rpart_fit)\r\n\r\n\r\nSurvival regression tree:\r\nrpart(formula = Surv(tenure, exited) ~ ., data = df)\r\n\r\nVariables actually used in tree construction:\r\n[1] age              is_active_member num_of_products \r\n\r\nRoot node error: 7780.8/9587 = 0.8116\r\n\r\nn= 9587 \r\n\r\n        CP nsplit rel error  xerror     xstd\r\n1 0.099482      0   1.00000 1.00012 0.014184\r\n2 0.031001      1   0.90052 0.90214 0.014477\r\n3 0.030170      2   0.86952 0.87696 0.014590\r\n4 0.019572      3   0.83935 0.84119 0.014277\r\n5 0.012702      4   0.81977 0.82214 0.014366\r\n6 0.010000      5   0.80707 0.81324 0.014338\r\n\r\nFrom the result above, we will choose nsplit = 5 as it has the lowest xerror.\r\nMethod 2: Choose the xerror < minimum(xerror) + xstd\r\nrpart package has a nice function (i.e. plotcp function) to help us to decide which cp we should use in pruning the tree.\r\n\r\n\r\nplotcp(rpart_fit)\r\n\r\n\r\n\r\nBased on the documentation, the horizontal line in the graph is drawn on 1 standard error above the minimum xerror.\r\nAlternatively, we could illustrate by using ggplot.\r\n\r\n\r\nrpart_fit_cp <- \r\n  printcp(rpart_fit) %>%\r\n  as_tibble() %>%\r\n  mutate(xerror_plus_std = xerror + xstd)\r\n\r\n\r\nSurvival regression tree:\r\nrpart(formula = Surv(tenure, exited) ~ ., data = df)\r\n\r\nVariables actually used in tree construction:\r\n[1] age              is_active_member num_of_products \r\n\r\nRoot node error: 7780.8/9587 = 0.8116\r\n\r\nn= 9587 \r\n\r\n        CP nsplit rel error  xerror     xstd\r\n1 0.099482      0   1.00000 1.00012 0.014184\r\n2 0.031001      1   0.90052 0.90214 0.014477\r\n3 0.030170      2   0.86952 0.87696 0.014590\r\n4 0.019572      3   0.83935 0.84119 0.014277\r\n5 0.012702      4   0.81977 0.82214 0.014366\r\n6 0.010000      5   0.80707 0.81324 0.014338\r\n\r\nggplot(rpart_fit_cp, \r\n       aes(x = as.character(nsplit), y = xerror)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = min(rpart_fit_cp$xerror_plus_std),\r\n             linetype = 2,\r\n             size = 1) +\r\n  xlab(\"nsplit\")\r\n\r\n\r\n\r\nBased on the result above, we could choose cp = 0.016 (or nsplit = 4).\r\nTo prune the tree, we will use prune function.\r\n\r\n\r\nprune(rpart_fit, cp = 0.016)\r\n\r\nn= 9587 \r\n\r\nnode), split, n, deviance, yval\r\n      * denotes terminal node\r\n\r\n1) root 9587 7780.7980 1.0000000  \r\n  2) age< 41.5 6510 3683.8010 0.5459338  \r\n    4) num_of_products< 2.5 6365 3295.1530 0.4751260  \r\n      8) num_of_products>=1.5 3237  915.7341 0.1968239 *\r\n      9) num_of_products< 1.5 3128 2144.6720 0.7674072 *\r\n    5) num_of_products>=2.5 145  147.4370 3.6416170 *\r\n  3) age>=41.5 3077 3322.9470 1.9579020  \r\n    6) is_active_member=1 1660 1540.7610 1.3032990 *\r\n    7) is_active_member=0 1417 1629.8980 2.6766400 *\r\n\r\nThe result of the pruned tree will be shown after pruning the tree.\r\nVisualize the tree\r\nWe could use the plotting functions from rpart package in visualizing the tree.\r\nThis would be easier to visualize how the tree looks like, instead of reading a list of splitting rules from the results.\r\nTo do so, we need to use plot and text functions to visualize the tree.\r\n\r\n\r\nplot(rpart_fit, \r\n     margin = 0.05)\r\ntext(rpart_fit)\r\n\r\n\r\n\r\nThe split condition is written on top of the branch. The data points will “go” to left branch if they met the condition.\r\nNote the values at the end of the node are the y-val, which can be found in the fitted object as well.\r\nWe could also trace the paths a node took by using path.rpart function.\r\n\r\n\r\npath.rpart(rpart_fit, nodes = 14)\r\n\r\n\r\n node number: 14 \r\n   root\r\n   age>=41.5\r\n   is_active_member=0\r\n   age< 47.5\r\n\r\nThis is a good way to summarize the characteristics of the data points under different nodes.\r\nFor example, we can see the data points fall under node 14 are age between 41.5 and 47.5 and they are not active members.\r\nAlternatively, we could pipe the fitted tree from rpart package to a party object before passing into plot function to visualize the tree.\r\n\r\n\r\nrpart_fit %>%\r\n  as.party()\r\n\r\n\r\nModel formula:\r\nSurv(tenure, exited) ~ credit_score + geography + gender + age + \r\n    balance + num_of_products + has_cr_card + is_active_member + \r\n    estimated_salary\r\n\r\nFitted party:\r\n[1] root\r\n|   [2] age < 41.5\r\n|   |   [3] num_of_products < 2.5\r\n|   |   |   [4] num_of_products >= 1.5: Inf (n = 3237)\r\n|   |   |   [5] num_of_products < 1.5: Inf (n = 3128)\r\n|   |   [6] num_of_products >= 2.5: 6.000 (n = 145)\r\n|   [7] age >= 41.5\r\n|   |   [8] is_active_member in 1: 10.000 (n = 1660)\r\n|   |   [9] is_active_member in 0\r\n|   |   |   [10] age < 47.5: 9.000 (n = 785)\r\n|   |   |   [11] age >= 47.5: 6.000 (n = 632)\r\n\r\nNumber of inner nodes:    5\r\nNumber of terminal nodes: 6\r\n\r\nThe fitted model result will look similar to the result produced from ctree package.\r\n\r\n\r\nrpart_fit %>%\r\n  as.party() %>%\r\n  plot()\r\n\r\n\r\n\r\nThe cool thing is the result will show us the survival curves under the different groups of customers.\r\nVisualize survival curve\r\nAlternatively, we could visualize the survival curves of the different groups by using ggplot function.\r\nFirst, I will pull the group info from the fitted object. It is stored under where in the fitted object.\r\nThen, I will convert into a tibble data before joining back to the original dataset.\r\n\r\n\r\ndf_grp <-\r\n  rpart_fit$where %>%\r\n  as_tibble() %>%\r\n  rename(group = value) %>%\r\n  bind_cols(df)\r\n\r\n\r\nOnce that is done, I will visualize the surival curve by using survfit and ggsurvplot functions as shown below.\r\n\r\n\r\nsurvfit(Surv(tenure, exited) ~ group,\r\n        data = df_grp) %>%\r\n  ggsurvplot(data = df_grp)\r\n\r\n\r\n\r\nMethod 2 - Use ctree package\r\nNext, I will use ctree package to build survival decision tree.\r\nAs ctree package is unable to accept characters, so I will convert the characters columns into factors.\r\n\r\n\r\ndf_factor <- df %>%\r\n  mutate(geography = factor(geography),\r\n         gender = factor(gender))\r\n\r\n\r\nLet’s build the tree!\r\n\r\n\r\nctree_fit <-\r\n  ctree(Surv(tenure, exited) ~ ., data = df_factor)\r\n\r\n\r\nSimilarly, we could call the fitted object to see the result.\r\n\r\n\r\nctree_fit\r\n\r\n\r\nModel formula:\r\nSurv(tenure, exited) ~ credit_score + geography + gender + age + \r\n    balance + num_of_products + has_cr_card + is_active_member + \r\n    estimated_salary\r\n\r\nFitted party:\r\n[1] root\r\n|   [2] age <= 42\r\n|   |   [3] age <= 34\r\n|   |   |   [4] balance <= 102073.67\r\n|   |   |   |   [5] num_of_products <= 2\r\n|   |   |   |   |   [6] num_of_products <= 1\r\n|   |   |   |   |   |   [7] balance <= 60280.62: Inf (n = 294)\r\n|   |   |   |   |   |   [8] balance > 60280.62: Inf (n = 341)\r\n|   |   |   |   |   [9] num_of_products > 1: Inf (n = 1295)\r\n|   |   |   |   [10] num_of_products > 2: 10.000 (n = 28)\r\n|   |   |   [11] balance > 102073.67\r\n|   |   |   |   [12] geography in France\r\n|   |   |   |   |   [13] balance <= 191775.65: Inf (n = 679)\r\n|   |   |   |   |   [14] balance > 191775.65: 9.000 (n = 9)\r\n|   |   |   |   [15] geography in Germany, Spain\r\n|   |   |   |   |   [16] gender in Female: Inf (n = 382)\r\n|   |   |   |   |   [17] gender in Male: Inf (n = 507)\r\n|   |   [18] age > 34\r\n|   |   |   [19] geography in France, Spain\r\n|   |   |   |   [20] is_active_member in 0\r\n|   |   |   |   |   [21] age <= 40\r\n|   |   |   |   |   |   [22] gender in Female: 10.000 (n = 447)\r\n|   |   |   |   |   |   [23] gender in Male: Inf (n = 577)\r\n|   |   |   |   |   [24] age > 40: 10.000 (n = 240)\r\n|   |   |   |   [25] is_active_member in 1: Inf (n = 1222)\r\n|   |   |   [26] geography in Germany\r\n|   |   |   |   [27] is_active_member in 0: 9.000 (n = 413)\r\n|   |   |   |   [28] is_active_member in 1\r\n|   |   |   |   |   [29] gender in Female: 10.000 (n = 154)\r\n|   |   |   |   |   [30] gender in Male: Inf (n = 227)\r\n|   [31] age > 42\r\n|   |   [32] is_active_member in 0\r\n|   |   |   [33] age <= 47\r\n|   |   |   |   [34] geography in France, Spain: 9.000 (n = 445)\r\n|   |   |   |   [35] geography in Germany: 8.000 (n = 176)\r\n|   |   |   [36] age > 47\r\n|   |   |   |   [37] age <= 51: 7.000 (n = 278)\r\n|   |   |   |   [38] age > 51: 6.000 (n = 354)\r\n|   |   [39] is_active_member in 1\r\n|   |   |   [40] geography in France, Spain\r\n|   |   |   |   [41] age <= 61\r\n|   |   |   |   |   [42] gender in Female: 9.000 (n = 405)\r\n|   |   |   |   |   [43] gender in Male: Inf (n = 470)\r\n|   |   |   |   [44] age > 61: Inf (n = 251)\r\n|   |   |   [45] geography in Germany\r\n|   |   |   |   [46] age <= 67: 8.000 (n = 355)\r\n|   |   |   |   [47] age > 67: Inf (n = 38)\r\n\r\nNumber of inner nodes:    23\r\nNumber of terminal nodes: 24\r\n\r\nNote as rpart and partykit packages are using different methods in splitting the tree, hence the trees might look different between both packages.\r\nAs the fitted object is a party object, so we could pass the fitted object into plot function to visualize the tree.\r\n\r\n\r\nctree_fit_sub <-\r\n  ctree(Surv(tenure, exited) ~ age + geography, data = df_factor)\r\n\r\nplot(ctree_fit_sub)\r\n\r\n\r\n\r\nVoila!\r\nMethod 3 - Use tidymodels packages to build the trees\r\nNext, I will be exploring how to build a survival decision tree by using my all times favourite R package, tidymodel.\r\nI will also need to load another additional R package, censored in order to perform survival analysis.\r\nTaken from giphy\r\nTo do so, I will first indicate what type of model I am building, which R package I am wrapping and what type of problem are we solving here.\r\n\r\n\r\ntdmodel_specs <- \r\n  decision_tree() %>%\r\n  set_engine(\"rpart\") %>%\r\n  set_mode(\"censored regression\")\r\n\r\n\r\nThen I will fit the model by calling fit function.\r\n\r\n\r\ntdmodel_fit <- \r\n  tdmodel_specs %>%\r\n  fit(Surv(tenure, exited) ~ .,\r\n      data = df)\r\n\r\n\r\nWe can access the rpart object by calling the object as follow:\r\n\r\n\r\ntdmodel_fit$fit$rpart\r\n\r\nn= 9587 \r\n\r\nnode), split, n, deviance, yval\r\n      * denotes terminal node\r\n\r\n 1) root 9587 7780.7980 1.0000000  \r\n   2) age< 41.5 6510 3683.8010 0.5459338  \r\n     4) num_of_products< 2.5 6365 3295.1530 0.4751260  \r\n       8) num_of_products>=1.5 3237  915.7341 0.1968239 *\r\n       9) num_of_products< 1.5 3128 2144.6720 0.7674072 *\r\n     5) num_of_products>=2.5 145  147.4370 3.6416170 *\r\n   3) age>=41.5 3077 3322.9470 1.9579020  \r\n     6) is_active_member=1 1660 1540.7610 1.3032990 *\r\n     7) is_active_member=0 1417 1629.8980 2.6766400  \r\n      14) age< 47.5 785  836.4027 1.8475160 *\r\n      15) age>=47.5 632  694.6653 3.7496700 *\r\n\r\nSimilarly, we could convert the object into party object and visualize the tree.\r\nAnd here you go!\r\n\r\n\r\ntdmodel_fit$fit$rpart %>%\r\n  as.party() %>%\r\n  plot()\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Sugar Bee on Unsplash\r\n\r\n\r\n\r\nGepp, Adrian, and Kuldeep Kumar. 2015. “Predicting Financial Distress: A Comparison of Survival Analysis and Decision Tree Techniques.” https://www.sciencedirect.com/science/article/pii/S1877050915013708?ref=pdf_download&fr=RR-2&rr=7933bfb87b5a4dd4.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-08-survival-decision-tree/image/tree.jpg",
    "last_modified": "2023-02-09T00:00:50+08:00",
    "input_file": "survival_decision_tree.knit.md"
  },
  {
    "path": "posts/2023-02-05-contiguity-spatial-weights/",
    "title": "Contiguity-based Spatial Weights",
    "description": "Are you my \"neighbours\"?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-02-05",
    "categories": [
      "Geospatial Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nSpatial weights\r\nWhat is contiguity-based spatial weight?\r\nTypes of contiguity-based spatial weights\r\nBest practice on how to select the appropriate spatial weighting method\r\nDemonstration\r\nSetup the environment\r\nImport the data\r\nImport shp files\r\n\r\nContiguity-based spatial weight\r\nQueen Contiguity-based Neighbours\r\nRook Contiguity-based Neighbours\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Mateo on Unsplash\r\nSpatial weights\r\nOne of the analyses in geospatial is to find out how strong is the spatial relationship between different interest objects.\r\nThis is an important step before we could perform spatial autocorrelation.\r\nBut how do we know who are the “neighbours”?\r\nSometimes it can be not very obvious from the map.\r\n\r\n\r\n\r\nGenerated from Imgflip\r\nThis is where we will use spatial weighting method to find the list of neighbours.\r\nIn this post, we will focus on one of the spatial weighting methods, which is contiguity-based spatial weights.\r\nWhat is contiguity-based spatial weight?\r\nContiguity means that two spatial units share a common border of non-zero length (Anselin 2020).\r\nIn other words, we are interested in finding whether the two areas “touch” each other.\r\nTypes of contiguity-based spatial weights\r\nThere are two types of contiguity-based spatial weights.\r\nThey are queen contiguity and rook contiguity.\r\nAs the names suggested, the definition of “neighbours” under each method matches the respective movements on the chess board.\r\nBelow is the illustration of the definition of “neighbours” under each method:\r\n\r\n\r\n\r\nTaken from (De Bellefon, Loonis, and Le Gleut 2018)\r\nBest practice on how to select the appropriate spatial weighting method\r\nThere are different spatial weighting techniques.\r\nThen how would we know which one is more appropriate since different spatial weighting might give us different results?\r\n(Kam 2020) discussed the considerations in one of his lectures.\r\nIn summary, below are different spatial weighting methods and when they are more appropriate:\r\nPolygon contiguity\r\nWhen the polygons are similar in size and distribution, and when spatial relationships are a function of polygon proximity\r\n\r\nFixed distance method\r\nWork well for point data\r\nAlternative for polygon data when there is a large variation in polygon size\r\n\r\nInverse distance method\r\nAppropriate with continuous data or to model processes where the closer two features are in space, the more likely they are to interact/influence each other\r\n\r\nk-nearest neighbours method\r\nWhen want to ensure we have a minimum number of neighbours\r\n\r\nI will cover the other weighting methods in my future post.\r\nDemonstration\r\nI will download Malaysia shape files from this link.\r\nFor more explanations on shape files, please refer to my previous post.\r\nSetup the environment\r\nFirst, I will set up the environment by calling the necessary packages.\r\n\r\n\r\npacman::p_load(tidyverse, sf, spdep, tmap, janitor)\r\n\r\n\r\nI will also set the tmap_mode to view so that I can interact with the graphs.\r\n\r\n\r\ntmap_mode('view')\r\n\r\n\r\nImport the data\r\nImport shp files\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\nmsia_map <- st_read(dsn = \"data\", layer = \"MYS_adm2\")\r\n\r\nReading layer `MYS_adm2' from data source \r\n  `C:\\Users\\jaspe\\OneDrive\\Documents\\2_Data Science\\98_my-blog\\_posts\\2023-02-05-contiguity-spatial-weights\\data' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 144 features and 14 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 99.64072 ymin: 0.855001 xmax: 119.2697 ymax: 7.380556\r\nGeodetic CRS:  WGS 84\r\n\r\nNext, I will visualize the Malaysia map.\r\n\r\n\r\ntm_shape(msia_map) +\r\n  tm_polygons()\r\n\r\n\r\n\r\nGood! Now, we can proceed and find the “neighbours”.\r\nContiguity-based spatial weight\r\nIn this sub-section, I will derive the contiguity-based spatial weights.\r\nTo better visualize the spatial weights results later, I will find the centroids of different administrative districts.\r\n\r\n\r\nlongitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[1]])\r\nlatitude <- map_dbl(msia_map$geometry, ~st_centroid(.x)[[2]])\r\n\r\n\r\nOnce the latitude and longitude are derived, I will use cbind function to bind the columns together.\r\n\r\n\r\ncoords <- cbind(longitude, latitude)\r\n\r\n\r\nQueen Contiguity-based Neighbours\r\nI will use poly2nb function and pass TRUE to queen argument to derive the queen contiguity based neighbours.\r\nNote that the queen argument is TRUE by default.\r\n\r\n\r\nmsia_map_queen <-\r\n  poly2nb(msia_map, queen = TRUE)\r\n\r\n\r\nTo visualize the results, I will do the following:\r\nPlot an empty Malaysia map\r\nThen, plot the graphs on queen contiguity based neighbours\r\n\r\n\r\nplot(msia_map$geometry, \r\n     border=\"lightgrey\",\r\n     main=\"Queen Contiguity-based Neighbours\")\r\n\r\nplot(msia_map_queen, \r\n     coords, \r\n     pch = 20, \r\n     cex = 0.5, \r\n     add = TRUE,\r\n     col= \"blue\")\r\n\r\n\r\n\r\nRemember to indicate the add argument should be TRUE, otherwise the code will return the neighbours map without the underlying Malaysia map.\r\nRook Contiguity-based Neighbours\r\nTo derive Rook contiguity-based neighbours, I will still use ploy2nb function and specify FALSE for the queen argument.\r\n\r\n\r\nmsia_map_rook <-\r\n  poly2nb(msia_map, queen = FALSE)\r\n\r\n\r\nSimilarly, to visualize the result, I will follow the same steps taken when visualizing the results for Queen contiguity-based neighbours.\r\n\r\n\r\nplot(msia_map$geometry, border=\"lightgrey\")\r\nplot(msia_map_rook, \r\n     coords, \r\n     pch = 20, \r\n     cex = 0.5, \r\n     add = TRUE, \r\n     col= \"blue\")\r\n\r\n\r\n\r\nAt the first glance, it looks like the list of neighbours derived based on both Queen and Rook approaches look the same.\r\nTo find the different neighbours, I will use diffnb function to compare.\r\n\r\n\r\ndiff_nb <- diffnb(msia_map_queen, msia_map_rook)\r\n\r\n\r\nThen, I will use similar approach to visualize the differences.\r\nI will also color the differences in red colors.\r\n\r\n\r\nplot(msia_map$geometry, border=\"lightgrey\")\r\nplot(diff_nb, \r\n     coords, \r\n     pch = 20, \r\n     cex = 0.5, \r\n     add = TRUE, \r\n     col= \"red\")\r\n\r\n\r\n\r\nBased on the result, it seems like most of the neighbours derived are rather similar between Queen and Rook approaches.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Victor Freitas on Unsplash\r\n\r\n\r\n\r\nAnselin, Luc. 2020. “Contiguity-Based Spatial Weights.” https://geodacenter.github.io/workbook/4a_contig_weights/lab4a.html#contiguity-weights.\r\n\r\n\r\nDe Bellefon, Marie-Pierre, Vincent Loonis, and Ronan Le Gleut. 2018. Chapter 2 - Codfying the Neighbourhood Structure. INSEE.\r\n\r\n\r\nKam, Tin Seong. 2020. “Lesson 7: Global and Local Measures of Spatial Autocorrelation.” https://is415.netlify.app/lesson/lesson07/lesson07-spaauto#1.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-02-05-contiguity-spatial-weights/image/weights.jpg",
    "last_modified": "2023-02-04T13:33:47+08:00",
    "input_file": "contiguity-spatial-weights.knit.md"
  },
  {
    "path": "posts/2023-01-10-cox-ph/",
    "title": "Cox Proportional Hazard",
    "description": "One of the popular methods in survival analysis",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-01-11",
    "categories": [
      "Machine Learning",
      "Survival Model"
    ],
    "contents": "\r\n\r\nContents\r\nDifferences Between parametric, semi-parametric, and non-parametric models\r\nCox Proportional Hazard\r\nWhat is Cox proportional hazard?\r\nHow to interpret Cox Proportional Hazard?\r\nCategorical variable\r\nNumerical variable\r\n\r\nDifference between Cox Proportional Hazard and Kaplan-Meier Survival Curve\r\n\r\nConcordance Index\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nBuilding Cox model\r\nExtract the results\r\n\r\nVisualize the survival curve\r\nVisualize the cox proportional hazard results\r\nFeature Selection\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring one of the popular methods for survival analysis, Cox proportional hazard.\r\n\r\n\r\n\r\nPhoto by Alexander Andrews on Unsplash\r\nBefore jumping into cox proportional hazard, let’s take a look what are the differences between parametric, semi-parametric, and non-parametric models.\r\nDifferences Between parametric, semi-parametric, and non-parametric models\r\nAccording to the post by Ravi Charan (Charan 2020), below are the summary of the differences:\r\nNon-parametric model: Make no assumptions about the functional form of \\(\\lambda\\) (i.e. hazard function)\r\nParametric model: Assume the precise functional form of \\(\\lambda\\) in modeling\r\nSemi-parametric model: Cox proportional hazard does not make any functional assumptions on the shape of the hazard function\r\nFunctional form assumptions are made about the effects of the covariates alone\r\n\r\nCox Proportional Hazard\r\nWhat is Cox proportional hazard?\r\nSo, what is cox proportional hazard?\r\nCox proportional hazard is a semi-parametric survival model.\r\nThe Cox proportional hazard formula can be written as follows (STHDA, n.d.):\r\n\\[h(t) = h_0(t) \\times exp(b_1x_1 + b_2x_2 + ... + b_px_p) \\]\r\nwhere\r\nt is the survival time\r\nh(t) is the hazard function determined by a set of covariates (\\(x_1, x_2, ..., x_p\\))\r\nthe coefficients (\\(b_1, b_2, ...., b_p\\)) measure the impact of covariates\r\n\\(h_0\\) is the baseline hazard\r\nAs shown in the formula, Cox proportional hazard model allows the users to assess the effects of multiple risk factors on survival time at the same time.\r\nHow to interpret Cox Proportional Hazard?\r\nOne of the key outputs from Cox proportional hazard model is the hazard ratio.\r\nBelow is how one could interpret the hazard ratio:\r\nCategorical variable\r\nHazard ratio = 1 —> no effect\r\nHazard ratio > 1 increase relative risk compared to the reference group\r\nHazard ratio < 1 decrease in relative risk compared to the reference group\r\nNumerical variable\r\nThe hazard ratio represents the increase/decrease in risk for every unit change\r\nExamples will be provided in the demonstration so that it’s easier to understand the concepts.\r\nDifference between Cox Proportional Hazard and Kaplan-Meier Survival Curve\r\n\r\nOne of the disadvantages of the Kaplan-Meier survival curve is unable to model numeric variables. Each unique numeric figure will be treated as a separate group when fitting the survival curve under the Kaplan-Meier model.\r\nAnother disadvantage of the Kaplan-Meier model is that model is unable to include many explanatory variables.\r\nThese issues could be overcome if Cox proportional hazard model is used.\r\nConcordance Index\r\nBefore we move on, it’s important to discuss one of the important performance metrics in the survival model.\r\nConcordance index (a.k.a. C-index) is one of the commonly used performance measures for survival models.\r\nThis index can be interpreted as the fraction of all pairs of subjects whose predicted survival times are correctly ordered among all subjects that can actually be ordered (Raykar et al., n.d.).\r\nBelow is a chart on how the concordance index is being calculated:\r\n\r\n\r\n\r\nTaken from “How to Evaluate Survival Analysis Models” post by Nicolo Cosimo Albanese\r\nIn the post, the author also summarised the interpretation of the concordance index as follows:\r\nC = 1: perfect concordance between risks and event times.\r\nC = 0: perfect anti-concordance between risks and event times.\r\nC = 0.5: random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\r\nDemonstration\r\nIn this demonstration, I will be using this bank dataset from Kaggle.\r\nSetup the environment\r\nIn this demonstration, I will be using Cox proportional hazard function from survival package.\r\nI will be using functions from survminer package to visualize the results.\r\nFirst, I will load the necessary packages into the environment.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, janitor, survival, survminer, scales)\r\n\r\n\r\nImport Data\r\nNext, I will import the dataset into the environment.\r\nI will also clean the column names, drop the columns I don’t need, and transform the columns to the right format.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-09-10-kaplan-meier/data/Churn_Modelling.csv\") %>%\r\n  clean_names() %>%\r\n  select(-c(row_number, customer_id, surname)) %>%\r\n  mutate(has_cr_card = factor(has_cr_card),\r\n         is_active_member = factor(is_active_member),\r\n         credit_score = credit_score/100,\r\n         balance = balance/10000,\r\n         estimated_salary = estimated_salary/10000) %>%\r\n  filter(tenure > 0)\r\n\r\n\r\nNote that I have also scaled credit_score, balance, and estimated_salary in respective units.\r\nBuilding Cox model\r\nTo build a cox model, I will use coxph function from survival package to build a cox proportional hazard model.\r\n\r\n\r\ncox_model <- coxph(Surv(tenure, exited) ~ ., \r\n                   data = df,\r\n                   x = TRUE,\r\n                   y = TRUE)\r\n\r\n\r\nWe could call the fitted model to see the model result.\r\n\r\n\r\ncox_model\r\n\r\nCall:\r\ncoxph(formula = Surv(tenure, exited) ~ ., data = df, x = TRUE, \r\n    y = TRUE)\r\n\r\n                       coef exp(coef)  se(coef)       z       p\r\ncredit_score      -0.048759  0.952410  0.023060  -2.114  0.0345\r\ngeographyGermany   0.488400  1.629706  0.055680   8.772 < 2e-16\r\ngeographySpain     0.034473  1.035074  0.062178   0.554  0.5793\r\ngenderMale        -0.382557  0.682115  0.045836  -8.346 < 2e-16\r\nage                0.048125  1.049302  0.001816  26.506 < 2e-16\r\nbalance            0.020393  1.020603  0.004495   4.537 5.7e-06\r\nnum_of_products   -0.051172  0.950116  0.039461  -1.297  0.1947\r\nhas_cr_card1      -0.059315  0.942410  0.049585  -1.196  0.2316\r\nis_active_member1 -0.755391  0.469827  0.048751 -15.495 < 2e-16\r\nestimated_salary   0.001221  1.001222  0.003926   0.311  0.7558\r\n\r\nLikelihood ratio test=1124  on 10 df, p=< 2.2e-16\r\nn= 9587, number of events= 1942 \r\n\r\nIt seems like the summary function would provide more info about the fitted model if we pass the fitted model into the function.\r\n\r\n\r\nsummary(cox_model)\r\n\r\nCall:\r\ncoxph(formula = Surv(tenure, exited) ~ ., data = df, x = TRUE, \r\n    y = TRUE)\r\n\r\n  n= 9587, number of events= 1942 \r\n\r\n                       coef exp(coef)  se(coef)       z Pr(>|z|)    \r\ncredit_score      -0.048759  0.952410  0.023060  -2.114   0.0345 *  \r\ngeographyGermany   0.488400  1.629706  0.055680   8.772  < 2e-16 ***\r\ngeographySpain     0.034473  1.035074  0.062178   0.554   0.5793    \r\ngenderMale        -0.382557  0.682115  0.045836  -8.346  < 2e-16 ***\r\nage                0.048125  1.049302  0.001816  26.506  < 2e-16 ***\r\nbalance            0.020393  1.020603  0.004495   4.537  5.7e-06 ***\r\nnum_of_products   -0.051172  0.950116  0.039461  -1.297   0.1947    \r\nhas_cr_card1      -0.059315  0.942410  0.049585  -1.196   0.2316    \r\nis_active_member1 -0.755391  0.469827  0.048751 -15.495  < 2e-16 ***\r\nestimated_salary   0.001221  1.001222  0.003926   0.311   0.7558    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n                  exp(coef) exp(-coef) lower .95 upper .95\r\ncredit_score         0.9524     1.0500    0.9103    0.9964\r\ngeographyGermany     1.6297     0.6136    1.4612    1.8176\r\ngeographySpain       1.0351     0.9661    0.9163    1.1692\r\ngenderMale           0.6821     1.4660    0.6235    0.7462\r\nage                  1.0493     0.9530    1.0456    1.0530\r\nbalance              1.0206     0.9798    1.0117    1.0296\r\nnum_of_products      0.9501     1.0525    0.8794    1.0265\r\nhas_cr_card1         0.9424     1.0611    0.8551    1.0386\r\nis_active_member1    0.4698     2.1284    0.4270    0.5169\r\nestimated_salary     1.0012     0.9988    0.9935    1.0090\r\n\r\nConcordance= 0.72  (se = 0.006 )\r\nLikelihood ratio test= 1124  on 10 df,   p=<2e-16\r\nWald test            = 1169  on 10 df,   p=<2e-16\r\nScore (logrank) test = 1211  on 10 df,   p=<2e-16\r\n\r\nNote that the hazard ratio would be exp(coef) in the above.\r\nThe three tests at the bottom of the results are the statistical tests for the overall significance of the model, where the null hypothesis of the statistical tests is as follows:\r\n\\(H_0:\\beta = 0\\)\r\nAs the p-values for all the statistical tests are smaller than 0.05, we reject the null hypothesis.\r\nFor more info on how we could interpret the result, I find this post very helpful.\r\nAlternatively, chapter 5 of the book Applied Survival Analysis Using R by Dirk F. Moore has a quite clear explanation of cox proportional hazards as well.\r\nExtract the results\r\nThe concordance results can be extracted by using the following method:\r\n\r\n\r\ncox_model$concordance[\"concordance\"]\r\n\r\nconcordance \r\n  0.7204698 \r\n\r\nAlternatively, we could extract the concordance index by passing the fitted model into the concordance function from survival package.\r\n\r\n\r\nconcordance(cox_model)\r\n\r\nCall:\r\nconcordance.coxph(object = cox_model)\r\n\r\nn= 9587 \r\nConcordance= 0.7205 se= 0.006461\r\nconcordant discordant     tied.x     tied.y    tied.xy \r\n   7192433    2790544          0     193333          0 \r\n\r\nVisualize the survival curve\r\nHaving fit a Cox model to the data, it’s possible to visualize the predicted survival proportion at any given point in time for a particular risk group. The function survfit() estimates the survival proportion, by default at the mean values of covariates.\r\n\r\n\r\nggsurvplot(survfit(cox_model), \r\n           data = df)\r\n\r\n\r\n\r\nTo plot the graph, it seems like we need to pass the fit object into survfit function\r\nggsurvplot also supports multiple datasets.\r\nTo do so, we need to combine the different survfit objects into a list.\r\nFor demonstration, I will first split the dataset by gender.\r\n\r\n\r\ndf_male <-\r\n  df %>% \r\n  filter(gender == \"Male\") %>%\r\n  select(-gender)\r\n\r\ndf_female <-\r\n  df %>% \r\n  filter(gender == \"Female\") %>%\r\n  select(-gender)\r\n\r\n\r\nThen, I will fit two cox proportional hazard models based on each dataset.\r\n\r\n\r\nsurvfit_male <-\r\n  coxph(Surv(tenure, exited) ~ .,\r\n        data = df_male)\r\n\r\nsurvfit_female <-\r\n  coxph(Surv(tenure, exited) ~ .,\r\n        data = df_female)\r\n\r\n\r\nThen, I will create the “list” before passing the list into ggsurvplot function.\r\n\r\n\r\nsurv_fit_list <- \r\n  list(\"male\" = survfit(survfit_male),\r\n       \"female\" = survfit(survfit_female))\r\n\r\nggsurvplot(surv_fit_list, \r\n           combine = TRUE,\r\n           conf.int = TRUE,\r\n           ylim = c(0.35, 1))\r\n\r\n\r\n\r\nVisualize the cox proportional hazard results\r\nNext, I will visualize the fitted model results by using ggforest function.\r\n\r\n\r\nggforest(cox_model)\r\n\r\n\r\n\r\nThis graph contains the different variables used in fitting Cox model and the different categories within the categorical variables.\r\nOn the right side of the graph, we have the hazard ratio, the confidence interval for the different hazard ratios, and p-value for each variable.\r\nAs discussed earlier, the higher the hazard ratio, the riskier the customer is, compared to the reference group.\r\nFor example, on average a male customer is 32% less risky than a female customer.\r\nMeanwhile, for the numerical variable, we could interpret the hazard ratio as the change in relative risk for every unit change.\r\nFor example, for every increase in 1 for age, the risk would increase approximately by 5%.\r\nNevertheless, below are what we could observe from the cox model result:\r\nInterestingly enough that in general, the churn increases when the age increases\r\n\r\n\r\nggplot(df, aes(x = age, fill = as.factor(exited), alpha = 0.5)) +\r\n  geom_histogram()\r\n\r\n\r\n\r\nThe likelihood of churning also depends on whether the customer is being tagged as an active member\r\nThe company could run some campaigns to engage the non-active customers to reduce the churn rate\r\n\r\nThe customers from Germany are more likely to churn, compared to France and Spain\r\nThis is consistent with what we observe in the dataset\r\nIt is probably worthwhile to understand further whether there are any reasons why customers from Germany are more likely to churn.\r\nFor example, it could be the customers from Germany are mostly from a certain industry, resulting in a higher churn rate.\r\nThis is to help us to spot whether any customer characteristics that are not captured in the dataset.\r\n\r\n\r\nggplot(df, aes(x = geography, fill = as.factor(exited))) +\r\n  geom_bar(position = \"fill\")\r\n\r\n\r\n\r\nI find that visualizing the results as follows makes it easier to understand.\r\nFeature Selection\r\nWe could take one step further to perform feature selection on the variables to be used in cox proportional hazard modeling.\r\nOver here, I will be using step function from stats package in performing stepwise feature selection.\r\nNote that this method uses AIC in selecting the model.\r\nI will also indicate that the direction should be “both”, i.e. the model could include or exclude variables at each step.\r\n\r\n\r\nstats::step(\r\n  cox_model,\r\n  scope = list(upper = ~.,\r\n               lower = ~is_active_member),\r\n  direction = \"both\"\r\n)\r\n\r\nStart:  AIC=31267.66\r\nSurv(tenure, exited) ~ credit_score + geography + gender + age + \r\n    balance + num_of_products + has_cr_card + is_active_member + \r\n    estimated_salary\r\n\r\n                   Df   AIC\r\n- estimated_salary  1 31266\r\n- has_cr_card       1 31267\r\n- num_of_products   1 31267\r\n<none>                31268\r\n- credit_score      1 31270\r\n- balance           1 31286\r\n- gender            1 31336\r\n- geography         2 31348\r\n- age               1 31881\r\n\r\nStep:  AIC=31265.75\r\nSurv(tenure, exited) ~ credit_score + geography + gender + age + \r\n    balance + num_of_products + has_cr_card + is_active_member\r\n\r\n                   Df   AIC\r\n- has_cr_card       1 31265\r\n- num_of_products   1 31265\r\n<none>                31266\r\n+ estimated_salary  1 31268\r\n- credit_score      1 31268\r\n- balance           1 31284\r\n- gender            1 31334\r\n- geography         2 31347\r\n- age               1 31879\r\n\r\nStep:  AIC=31265.16\r\nSurv(tenure, exited) ~ credit_score + geography + gender + age + \r\n    balance + num_of_products + is_active_member\r\n\r\n                   Df   AIC\r\n- num_of_products   1 31265\r\n<none>                31265\r\n+ has_cr_card       1 31266\r\n+ estimated_salary  1 31267\r\n- credit_score      1 31268\r\n- balance           1 31284\r\n- gender            1 31334\r\n- geography         2 31345\r\n- age               1 31880\r\n\r\nStep:  AIC=31264.82\r\nSurv(tenure, exited) ~ credit_score + geography + gender + age + \r\n    balance + is_active_member\r\n\r\n                   Df   AIC\r\n<none>                31265\r\n+ num_of_products   1 31265\r\n+ has_cr_card       1 31265\r\n+ estimated_salary  1 31267\r\n- credit_score      1 31267\r\n- balance           1 31289\r\n- gender            1 31333\r\n- geography         2 31344\r\n- age               1 31883\r\nCall:\r\ncoxph(formula = Surv(tenure, exited) ~ credit_score + geography + \r\n    gender + age + balance + is_active_member, data = df, x = TRUE, \r\n    y = TRUE)\r\n\r\n                       coef exp(coef)  se(coef)       z        p\r\ncredit_score      -0.048060  0.953076  0.023066  -2.084   0.0372\r\ngeographyGermany   0.479384  1.615078  0.055375   8.657  < 2e-16\r\ngeographySpain     0.034141  1.034731  0.062170   0.549   0.5829\r\ngenderMale        -0.383472  0.681491  0.045820  -8.369  < 2e-16\r\nage                0.048282  1.049466  0.001813  26.631  < 2e-16\r\nbalance            0.022047  1.022292  0.004329   5.093 3.53e-07\r\nis_active_member1 -0.757448  0.468862  0.048702 -15.553  < 2e-16\r\n\r\nLikelihood ratio test=1120  on 7 df, p=< 2.2e-16\r\nn= 9587, number of events= 1942 \r\n\r\nFrom the result, we see that the following variables are being dropped in the feature selection:\r\nnum_of_products\r\nhas_cr_card\r\nestimated_salary\r\nWith this, I will rebuild the cox proportional hazard model with lesser variables.\r\n\r\n\r\ncox_model_sub <- \r\n  coxph(Surv(tenure, exited) ~ credit_score + geography + gender + age + balance + is_active_member, \r\n        data = df, \r\n        x = TRUE, \r\n        y = TRUE)\r\n\r\nggforest(cox_model_sub)\r\n\r\n\r\n\r\nNext, I will extract the concordance index of the new model.\r\n\r\n\r\ncox_model_sub$concordance[\"concordance\"]\r\n\r\nconcordance \r\n  0.7200558 \r\n\r\nThe new model requires fewer variables in predicting the outcome although there is a slight drop in the concordance index in the new model.\r\nThe concordance index dropped by 0.06%.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Helena Lopes\r\n\r\n\r\n\r\nCharan, Ravi. 2020. “The Cox Proportional Hazards Model.” https://towardsdatascience.com/the-cox-proportional-hazards-model-35e60e554d8f.\r\n\r\n\r\nRaykar, Vikas C., Harald Steck, Balaji Krishnapuram, Cary Dehing-Oberije, and Philippe Lambin. n.d. “On Ranking in Survival Analysis: Bounds on the Concordance Index.” https://proceedings.neurips.cc/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf.\r\n\r\n\r\nSTHDA. n.d. “Cox Proportional-Hazards Model.” http://www.sthda.com/english/wiki/cox-proportional-hazards-model.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-10-cox-ph/image/survival-kit.jpg",
    "last_modified": "2023-01-09T22:41:18+08:00",
    "input_file": "cox-ph.knit.md"
  },
  {
    "path": "posts/2023-01-09-survival-diff/",
    "title": "Survival Modelling - Log Rank Test",
    "description": "Are the survival curves same? Yes or no?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-01-09",
    "categories": [
      "Machine Learning",
      "Survival Model"
    ],
    "contents": "\r\n\r\nContents\r\nLog-rank test\r\nDifferent variations of log-rank tests\r\n\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nLog-rank Test\r\nComparing two survival curve\r\nComparing more than two survival curves\r\nPairwise survival curves\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Daniel Reche\r\nIn my previous post, I shared about how to build the survival curve.\r\nOften, one of the questions raised while building the survival curve is whether the survival curves observed under the different groups are statistically different from one another.\r\nLog-rank test\r\nLog-rank test is a chi-square test.\r\nIt compares the observed and expected counts to see whether the survival curves are statistically different.\r\nBelow is the null and alternative hypothesis of the log-rank test:\r\n\r\n\r\nHypothesis\r\n\r\n\r\nRemarks\r\n\r\n\r\nNull\r\n\r\n\r\nAll survival curves are the same\r\n\r\n\r\nAlternative\r\n\r\n\r\nAt least one of the survival curves is different from the rest\r\n\r\n\r\n\r\n\r\n\r\nTaken from DATAtab\r\nDifferent variations of log-rank tests\r\nThere are different variations to the log-rank test (David G. Kleinbaum 2012).\r\nThey allow the users to apply different weights at the f-th failure time.\r\n\r\n\r\n\r\nTaken from Survival Analysis - A Self Learning Text book\r\nOne of the arguments in the test is rho. Below is the difference when different rho is assumed in the test (Sestelo 2017):\r\nThe default for rho is 0, which is the log-rank test.\r\nThis test gives the same weighting to both early and late failure.\r\n\r\nWhen rho = 1, it would be the peto peto modification of the gehan-wilcoxon test.\r\nUnder this approach, the test will be sensitive to early differences.\r\n\r\nAccording to the author, the weighting method to be used in the log-rank test should be an priori decision, instead of trial and error to get the desirable results.\r\nThis is to avoid bias in the results (David G. Kleinbaum 2012).\r\nNevertheless, let’s start the demonstration!\r\nDemonstration\r\nIn this demonstration, I will be using this bank dataset from Kaggle.\r\nSetup the environment\r\nFirst, I will load the necessary packages into the environment.\r\n\r\n\r\npacman::p_load(tidyverse, survival, janitor, survminer)\r\n\r\n\r\nWith this, I will be using survival package to perform the log-rank test.\r\nImport Data\r\nFirst I will import the data into the environment.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-09-10-kaplan-meier/data/Churn_Modelling.csv\")\r\n\r\n\r\nNext, I will perform similar data wrangling.\r\nRefer to my previous post for the details.\r\n\r\n\r\n\r\nLog-rank Test\r\nComparing two survival curve\r\nIn this demonstration, I will compare the survival curve under different genders.\r\nRecall that to visualize the survival curve, I will first create the survfit object and the created object into ggsurvplot function to visualize the survival curves.\r\n\r\n\r\nsurv_fit <- survfit(Surv(tenure, exited) ~ gender, data = df)\r\n\r\nggsurvplot(surv_fit)\r\n\r\n\r\n\r\nFrom the graph, it looks like the survival curves are visually different under different genders.\r\nTo confirm this, I will perform a chi-square test on this to check whether the survival curves are indeed different.\r\nAs such, I will use survdiff function to perform the relevant task.\r\n\r\n\r\nsurvdiff(Surv(tenure, exited) ~ gender, data = df)\r\n\r\nCall:\r\nsurvdiff(formula = Surv(tenure, exited) ~ gender, data = df)\r\n\r\n                 N Observed Expected (O-E)^2/E (O-E)^2/V\r\ngender=Female 4543     1139      920      52.2       101\r\ngender=Male   5457      898     1117      43.0       101\r\n\r\n Chisq= 101  on 1 degrees of freedom, p= <2e-16 \r\n\r\nAs the p-value is greater than 0.05, we will reject the null hypothesis. There is statistical evidence that the two survival curves are different from one another.\r\nComparing more than two survival curves\r\nSimilarly, survdiff function also can be used when there are more than two survival curves.\r\nFor example, I would like to find out that the survival curves are indeed different when the number of products held by the customers differs.\r\n\r\n\r\nsurvdiff(Surv(tenure, exited) ~ num_of_products, data = df)\r\n\r\nCall:\r\nsurvdiff(formula = Surv(tenure, exited) ~ num_of_products, data = df)\r\n\r\n                     N Observed Expected (O-E)^2/E (O-E)^2/V\r\nnum_of_products=1 5084     1409   1027.2       142       304\r\nnum_of_products=2 4590      348    941.4       374       737\r\nnum_of_products=3  266      220     54.7       499       545\r\nnum_of_products=4   60       60     13.7       157       169\r\n\r\n Chisq= 1246  on 3 degrees of freedom, p= <2e-16 \r\n\r\nAs shown in the result above, we reject the null hypothesis and conclude that all the survival curves are not the same.\r\nHowever, this test does not tell us whether the survival curves are similar for some of the groups.\r\nFor example, if we plot out the survival curve for customers that held a different number of products, it seems like the survival curves for customers who held 3 products and customers who held 4 products are rather similar.\r\n\r\n\r\nsurv_fit <- survfit(Surv(tenure, exited) ~ num_of_products, data = df)\r\n\r\nggsurvplot(surv_fit)\r\n\r\n\r\n\r\nPairwise survival curves\r\nTo verify the hypothesis above, I will use pairwise_survdiff function to generate the pairwise results.\r\n\r\n\r\npairwise_survdiff(Surv(tenure, exited) ~ num_of_products, data = df)\r\n\r\n\r\n    Pairwise comparisons using Log-Rank test \r\n\r\ndata:  df and num_of_products \r\n\r\n  1      2      3   \r\n2 <2e-16 -      -   \r\n3 <2e-16 <2e-16 -   \r\n4 <2e-16 <2e-16 0.43\r\n\r\nP value adjustment method: BH \r\n\r\nFrom the results above, we fail to reject the null hypothesis when comparing the survival curves for customers who held 3 and 4 products. There is no statistical evidence that the survival curves for customers with 3 and 4 products are different.\r\nBy default, the function will perform log-rank test (i.e. rho = 0).\r\nTo perform peto-peto test, we will just need to set the rho to 1 as shown below.\r\n\r\n\r\npairwise_survdiff(Surv(tenure, exited) ~ num_of_products, data = df, rho = 1)\r\n\r\n\r\n    Pairwise comparisons using Peto & Peto test \r\n\r\ndata:  df and num_of_products \r\n\r\n  1      2      3   \r\n2 <2e-16 -      -   \r\n3 <2e-16 <2e-16 -   \r\n4 <2e-16 <2e-16 0.52\r\n\r\nP value adjustment method: BH \r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by George Milton\r\n\r\n\r\n\r\nDavid G. Kleinbaum, Mitchel Klein. 2012. Survival Analysis: A Self-Learning Text. 3rd ed. Springer.\r\n\r\n\r\nSestelo, Marta. 2017. A Short Course on Survival Analysis Applied to the Financial Industry. https://bookdown.org/sestelo/sa_financial/comparing-survival-curves.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-09-survival-diff/image/eggs.jpg",
    "last_modified": "2023-01-09T22:37:46+08:00",
    "input_file": "survival_survdiff.knit.md"
  },
  {
    "path": "posts/2023-01-04-textpdf/",
    "title": "Reading PDF into R",
    "description": {},
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2023-01-04",
    "categories": [
      "Text Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nDemonstration\r\nSetup the environment\r\nImport the pdf into the environment\r\nN-gram\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by cottonbro\r\nPreviously, while I was reading up the different text analytics, I happened to come acorss a R package that helps to read PDF documents into R environment.\r\nThat makes me wonder what if I could use this to help me to summarise the key points in the PDF documents, instead of needing to read through the entire documents.\r\n\r\nTaken from giphy\r\nAs there isn’t much theory on this exploration, so I will jump straight into the demonstration and it would be a rather short post.\r\nDemonstration\r\nIn this demonstration, I will use the pdf copy of the paper I wrote together with my professor when I was doing the capstone project for my master degree.\r\nThe pdf can be found under this link.\r\nSetup the environment\r\nI will first call all the packages I need.\r\n\r\n\r\npacman::p_load(tidyverse, pdftools, readr, tidytext, ggwordcloud, quanteda, spacyr)\r\n\r\n\r\nImport the pdf into the environment\r\nTo import the pdf into the environment, I will use pdf_text function from pdftools package to do so.\r\n\r\n\r\ndf <- pdf_text(\"data/MITB_Capstone_Lok Jun Haur_Final Report_20210430.pdf\")\r\n\r\n\r\nTo find out how pages there are in the pdf, we could use length function on the dataset.\r\n\r\n\r\nlength(df)\r\n\r\n[1] 103\r\n\r\nWe could also extract the content of the selected page by using double bracket as shown below.\r\n\r\n\r\ndf[[10]]\r\n\r\n[1] \"data, this project aims to benefit the actuarial community by providing the readers with a\\nuse case on how modern data science can be implemented in an actuarial context.\\n\\n3.2 Claim Cost Estimation\\nClaim cost estimation is one of the important tasks in actuarial science. The ability to\\nestimate claim cost accurately could translate into a lower cost of capital or a lower premium\\nfor the customers.\\nIn general, cash flow can be categorized into 4 types depending on the timing of the cash flow\\nand whether the amount of the cash flow is known upfront. They are as shown following:\\n\\n          Timing of Cashflow      Cashflow Amount\\n Type I   Deterministic           Deterministic\\n Type II Deterministic            Stochastic\\n Type III Stochastic              Deterministic\\n Type IV Stochastic               Stochastic\\n\\nFigure 4: Different Types of Liability Cashflow\\nOften both claim timing and claim payout in general insurance are stochastic. For example,\\nthe policyholders may claim any time during their policy coverage and the claim size may\\ndepend on other factors such as hospital bill size, cost of repairing the car, and so on. In other\\nwords, the claim size depends on what is being covered under the policy. The claim size is\\nusually not known or fixed at the point of the inception of the policy.\\nTherefore, insurers will typically attempt to estimate the amount of money to set aside to\\nmeet the potential claim payout. Incorrect assumptions could hurt the business, even\\nresulting in the insurers being insolvent under the extreme scenario.\\nBesides, with the advancement of the internet and technology, there are more and more\\nthird-party aggregators (eg. CompareFirst, MoneySmart, and so on) that consolidate the\\npremiums charged by the different insurers. This has created more transparency for the\\ncustomers to compare the premiums insurers are charging for different plans. This has\\nindirectly created a price war among the different insurers, resulting in a shrinking profit.\\nHence, an inappropriate premium setting might leave the insurers with a pool of “bad” risks.\\nAs the competition in the market becomes increasingly intense, insurers are also looking at\\ndifferent ways to reduce or lower the cost while protecting the profit margin of the business.\\nHence, sharpening assumptions become increasingly important to ensure the assumptions\\nare not overly prudent. Updating the assumptions to be more aligned with the underlying\\nrisks could potentially allow the insurers to release the capital and use it for other purposes\\n(eg. expanding the business into other markets, invest in the infrastructure, and so on).\\n\"\r\n\r\nTo make the text more readable, we could use cat function from base R.\r\n\r\n\r\ncat(df[[10]])\r\n\r\ndata, this project aims to benefit the actuarial community by providing the readers with a\r\nuse case on how modern data science can be implemented in an actuarial context.\r\n\r\n3.2 Claim Cost Estimation\r\nClaim cost estimation is one of the important tasks in actuarial science. The ability to\r\nestimate claim cost accurately could translate into a lower cost of capital or a lower premium\r\nfor the customers.\r\nIn general, cash flow can be categorized into 4 types depending on the timing of the cash flow\r\nand whether the amount of the cash flow is known upfront. They are as shown following:\r\n\r\n          Timing of Cashflow      Cashflow Amount\r\n Type I   Deterministic           Deterministic\r\n Type II Deterministic            Stochastic\r\n Type III Stochastic              Deterministic\r\n Type IV Stochastic               Stochastic\r\n\r\nFigure 4: Different Types of Liability Cashflow\r\nOften both claim timing and claim payout in general insurance are stochastic. For example,\r\nthe policyholders may claim any time during their policy coverage and the claim size may\r\ndepend on other factors such as hospital bill size, cost of repairing the car, and so on. In other\r\nwords, the claim size depends on what is being covered under the policy. The claim size is\r\nusually not known or fixed at the point of the inception of the policy.\r\nTherefore, insurers will typically attempt to estimate the amount of money to set aside to\r\nmeet the potential claim payout. Incorrect assumptions could hurt the business, even\r\nresulting in the insurers being insolvent under the extreme scenario.\r\nBesides, with the advancement of the internet and technology, there are more and more\r\nthird-party aggregators (eg. CompareFirst, MoneySmart, and so on) that consolidate the\r\npremiums charged by the different insurers. This has created more transparency for the\r\ncustomers to compare the premiums insurers are charging for different plans. This has\r\nindirectly created a price war among the different insurers, resulting in a shrinking profit.\r\nHence, an inappropriate premium setting might leave the insurers with a pool of “bad” risks.\r\nAs the competition in the market becomes increasingly intense, insurers are also looking at\r\ndifferent ways to reduce or lower the cost while protecting the profit margin of the business.\r\nHence, sharpening assumptions become increasingly important to ensure the assumptions\r\nare not overly prudent. Updating the assumptions to be more aligned with the underlying\r\nrisks could potentially allow the insurers to release the capital and use it for other purposes\r\n(eg. expanding the business into other markets, invest in the infrastructure, and so on).\r\n\r\nNext, I will perform some text analysis.\r\nTo do so, I will convert the character object into data frame so that it would be easier to analyze later.\r\nTo do so, I will first convert the text into different lines by using read_lines function.\r\n\r\n\r\ndf[[10]] %>%\r\n  read_lines()\r\n\r\n [1] \"data, this project aims to benefit the actuarial community by providing the readers with a\"        \r\n [2] \"use case on how modern data science can be implemented in an actuarial context.\"                   \r\n [3] \"\"                                                                                                  \r\n [4] \"3.2 Claim Cost Estimation\"                                                                         \r\n [5] \"Claim cost estimation is one of the important tasks in actuarial science. The ability to\"          \r\n [6] \"estimate claim cost accurately could translate into a lower cost of capital or a lower premium\"    \r\n [7] \"for the customers.\"                                                                                \r\n [8] \"In general, cash flow can be categorized into 4 types depending on the timing of the cash flow\"    \r\n [9] \"and whether the amount of the cash flow is known upfront. They are as shown following:\"            \r\n[10] \"\"                                                                                                  \r\n[11] \"          Timing of Cashflow      Cashflow Amount\"                                                 \r\n[12] \" Type I   Deterministic           Deterministic\"                                                   \r\n[13] \" Type II Deterministic            Stochastic\"                                                      \r\n[14] \" Type III Stochastic              Deterministic\"                                                   \r\n[15] \" Type IV Stochastic               Stochastic\"                                                      \r\n[16] \"\"                                                                                                  \r\n[17] \"Figure 4: Different Types of Liability Cashflow\"                                                   \r\n[18] \"Often both claim timing and claim payout in general insurance are stochastic. For example,\"        \r\n[19] \"the policyholders may claim any time during their policy coverage and the claim size may\"          \r\n[20] \"depend on other factors such as hospital bill size, cost of repairing the car, and so on. In other\"\r\n[21] \"words, the claim size depends on what is being covered under the policy. The claim size is\"        \r\n[22] \"usually not known or fixed at the point of the inception of the policy.\"                           \r\n[23] \"Therefore, insurers will typically attempt to estimate the amount of money to set aside to\"        \r\n[24] \"meet the potential claim payout. Incorrect assumptions could hurt the business, even\"              \r\n[25] \"resulting in the insurers being insolvent under the extreme scenario.\"                             \r\n[26] \"Besides, with the advancement of the internet and technology, there are more and more\"             \r\n[27] \"third-party aggregators (eg. CompareFirst, MoneySmart, and so on) that consolidate the\"            \r\n[28] \"premiums charged by the different insurers. This has created more transparency for the\"            \r\n[29] \"customers to compare the premiums insurers are charging for different plans. This has\"             \r\n[30] \"indirectly created a price war among the different insurers, resulting in a shrinking profit.\"     \r\n[31] \"Hence, an inappropriate premium setting might leave the insurers with a pool of “bad” risks.\"      \r\n[32] \"As the competition in the market becomes increasingly intense, insurers are also looking at\"       \r\n[33] \"different ways to reduce or lower the cost while protecting the profit margin of the business.\"    \r\n[34] \"Hence, sharpening assumptions become increasingly important to ensure the assumptions\"             \r\n[35] \"are not overly prudent. Updating the assumptions to be more aligned with the underlying\"           \r\n[36] \"risks could potentially allow the insurers to release the capital and use it for other purposes\"   \r\n[37] \"(eg. expanding the business into other markets, invest in the infrastructure, and so on).\"         \r\n\r\nNext, I will trim additional whitespace by using str_squish function.\r\n\r\n\r\ndf[[10]] %>%\r\n  read_lines() %>%\r\n  str_squish()\r\n\r\n [1] \"data, this project aims to benefit the actuarial community by providing the readers with a\"        \r\n [2] \"use case on how modern data science can be implemented in an actuarial context.\"                   \r\n [3] \"\"                                                                                                  \r\n [4] \"3.2 Claim Cost Estimation\"                                                                         \r\n [5] \"Claim cost estimation is one of the important tasks in actuarial science. The ability to\"          \r\n [6] \"estimate claim cost accurately could translate into a lower cost of capital or a lower premium\"    \r\n [7] \"for the customers.\"                                                                                \r\n [8] \"In general, cash flow can be categorized into 4 types depending on the timing of the cash flow\"    \r\n [9] \"and whether the amount of the cash flow is known upfront. They are as shown following:\"            \r\n[10] \"\"                                                                                                  \r\n[11] \"Timing of Cashflow Cashflow Amount\"                                                                \r\n[12] \"Type I Deterministic Deterministic\"                                                                \r\n[13] \"Type II Deterministic Stochastic\"                                                                  \r\n[14] \"Type III Stochastic Deterministic\"                                                                 \r\n[15] \"Type IV Stochastic Stochastic\"                                                                     \r\n[16] \"\"                                                                                                  \r\n[17] \"Figure 4: Different Types of Liability Cashflow\"                                                   \r\n[18] \"Often both claim timing and claim payout in general insurance are stochastic. For example,\"        \r\n[19] \"the policyholders may claim any time during their policy coverage and the claim size may\"          \r\n[20] \"depend on other factors such as hospital bill size, cost of repairing the car, and so on. In other\"\r\n[21] \"words, the claim size depends on what is being covered under the policy. The claim size is\"        \r\n[22] \"usually not known or fixed at the point of the inception of the policy.\"                           \r\n[23] \"Therefore, insurers will typically attempt to estimate the amount of money to set aside to\"        \r\n[24] \"meet the potential claim payout. Incorrect assumptions could hurt the business, even\"              \r\n[25] \"resulting in the insurers being insolvent under the extreme scenario.\"                             \r\n[26] \"Besides, with the advancement of the internet and technology, there are more and more\"             \r\n[27] \"third-party aggregators (eg. CompareFirst, MoneySmart, and so on) that consolidate the\"            \r\n[28] \"premiums charged by the different insurers. This has created more transparency for the\"            \r\n[29] \"customers to compare the premiums insurers are charging for different plans. This has\"             \r\n[30] \"indirectly created a price war among the different insurers, resulting in a shrinking profit.\"     \r\n[31] \"Hence, an inappropriate premium setting might leave the insurers with a pool of “bad” risks.\"      \r\n[32] \"As the competition in the market becomes increasingly intense, insurers are also looking at\"       \r\n[33] \"different ways to reduce or lower the cost while protecting the profit margin of the business.\"    \r\n[34] \"Hence, sharpening assumptions become increasingly important to ensure the assumptions\"             \r\n[35] \"are not overly prudent. Updating the assumptions to be more aligned with the underlying\"           \r\n[36] \"risks could potentially allow the insurers to release the capital and use it for other purposes\"   \r\n[37] \"(eg. expanding the business into other markets, invest in the infrastructure, and so on).\"         \r\n\r\nThen, I will convert the text into a tibble table by using as_tibble function. I will filter out all the empty rows after converting into the data frame.\r\n\r\n\r\ndf[[10]] %>%\r\n  read_lines() %>%\r\n  str_squish() %>%\r\n  as_tibble() %>%\r\n  filter(value != \"\")\r\n\r\n# A tibble: 34 x 1\r\n   value                                                              \r\n   <chr>                                                              \r\n 1 data, this project aims to benefit the actuarial community by prov~\r\n 2 use case on how modern data science can be implemented in an actua~\r\n 3 3.2 Claim Cost Estimation                                          \r\n 4 Claim cost estimation is one of the important tasks in actuarial s~\r\n 5 estimate claim cost accurately could translate into a lower cost o~\r\n 6 for the customers.                                                 \r\n 7 In general, cash flow can be categorized into 4 types depending on~\r\n 8 and whether the amount of the cash flow is known upfront. They are~\r\n 9 Timing of Cashflow Cashflow Amount                                 \r\n10 Type I Deterministic Deterministic                                 \r\n# ... with 24 more rows\r\n\r\n\r\n\r\ndf_page <- df[[10]] %>%\r\n  read_lines() %>%\r\n  str_squish() %>%\r\n  as_tibble() %>%\r\n  filter(value != \"\")\r\n\r\ndf_page\r\n\r\n# A tibble: 34 x 1\r\n   value                                                              \r\n   <chr>                                                              \r\n 1 data, this project aims to benefit the actuarial community by prov~\r\n 2 use case on how modern data science can be implemented in an actua~\r\n 3 3.2 Claim Cost Estimation                                          \r\n 4 Claim cost estimation is one of the important tasks in actuarial s~\r\n 5 estimate claim cost accurately could translate into a lower cost o~\r\n 6 for the customers.                                                 \r\n 7 In general, cash flow can be categorized into 4 types depending on~\r\n 8 and whether the amount of the cash flow is known upfront. They are~\r\n 9 Timing of Cashflow Cashflow Amount                                 \r\n10 Type I Deterministic Deterministic                                 \r\n# ... with 24 more rows\r\n\r\nNext, I will tokenize the words by using token function.\r\nI will also perform the following when tokenizing the words:\r\nRemove punctuation\r\nRemove numbers\r\nRemove symbols\r\nRemove separators\r\nSplit the words that join together by hyphens\r\nI will also convert the words to small letters and remove the stopwords.\r\n\r\n\r\ndf_token <- \r\n  tokens(df_page$value, \r\n         remove_punct = TRUE,\r\n         remove_numbers = TRUE,\r\n         remove_symbols = TRUE,\r\n         remove_separators = TRUE,\r\n         split_hyphens = TRUE) %>%\r\n  tokens_tolower() %>%\r\n  tokens_remove(stopwords(language = \"en\", source = \"smart\"), padding = FALSE)\r\n\r\n\r\n\r\n\r\ndf_token\r\n\r\nTokens consisting of 34 documents.\r\ntext1 :\r\n[1] \"data\"      \"project\"   \"aims\"      \"benefit\"   \"actuarial\"\r\n[6] \"community\" \"providing\" \"readers\"  \r\n\r\ntext2 :\r\n[1] \"case\"        \"modern\"      \"data\"        \"science\"    \r\n[5] \"implemented\" \"actuarial\"   \"context\"    \r\n\r\ntext3 :\r\n[1] \"claim\"      \"cost\"       \"estimation\"\r\n\r\ntext4 :\r\n[1] \"claim\"      \"cost\"       \"estimation\" \"important\"  \"tasks\"     \r\n[6] \"actuarial\"  \"science\"    \"ability\"   \r\n\r\ntext5 :\r\n [1] \"estimate\"   \"claim\"      \"cost\"       \"accurately\" \"translate\" \r\n [6] \"lower\"      \"cost\"       \"capital\"    \"lower\"      \"premium\"   \r\n\r\ntext6 :\r\n[1] \"customers\"\r\n\r\n[ reached max_ndoc ... 28 more documents ]\r\n\r\nNow the page is being tokenized.\r\nI will perform frequency count on the word as shown below.\r\n\r\n\r\ndf_token_df <- \r\n  df_token %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count)) %>%\r\n  arrange(desc(tot_count))\r\n\r\ndf_token_df\r\n\r\n# A tibble: 127 x 2\r\n   term          tot_count\r\n   <chr>             <dbl>\r\n 1 claim                10\r\n 2 insurers              8\r\n 3 cost                  6\r\n 4 stochastic            5\r\n 5 assumptions           4\r\n 6 deterministic         4\r\n 7 size                  4\r\n 8 type                  4\r\n 9 actuarial             3\r\n10 amount                3\r\n# ... with 117 more rows\r\n\r\nNext I will visualize the texts in word cloud.\r\n\r\n\r\ndf_token_df %>%\r\n  filter(tot_count > 1) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"circle\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nFrom the word cloud, we can see that the content of the selected page is about insurance claim cost. Other key words are ‘assumptions’, ‘cashflow’, ‘stochastic’ and so on.\r\nTo analyze the whole context of the document, I will use a for loop to loop through the main content of the report.\r\n\r\n\r\npage_num <- 5:93 # page number for the main content of the report\r\n\r\ndf_page_2 <- tibble()\r\n\r\nfor(i in page_num){\r\n  temp <- \r\n    df[[i]] %>%\r\n    read_lines() %>%\r\n    str_squish() %>%\r\n    as_tibble() %>%\r\n    filter(value != \"\") \r\n\r\ntemp_page <- \r\n  tokens(temp$value, \r\n         remove_punct = TRUE,\r\n         remove_numbers = TRUE,\r\n         remove_symbols = TRUE,\r\n         remove_separators = TRUE,\r\n         split_hyphens = FALSE) %>%\r\n  tokens_tolower() %>%\r\n  tokens_remove(stopwords(language = \"en\", source = \"smart\"), padding = FALSE) %>%\r\n  tokens_replace(pattern = lexicon::hash_lemmas$token, \r\n                 replacement = lexicon::hash_lemmas$lemma) %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count)) %>%\r\n  mutate(page = i)\r\n\r\ndf_page_2 <- df_page_2 %>%\r\n  bind_rows(temp_page)\r\n}\r\n\r\n\r\n\r\n\r\ndf_page_3 <- df_page_2 %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(tot_count)) %>%\r\n  arrange(desc(tot_count))\r\n\r\n\r\n\r\n\r\ndf_page_3 %>%\r\n  filter(tot_count >= 20) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"circle\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nFrom the word cloud, we can see the document is about datum (i.e. the lemma word for data) and model.\r\nN-gram\r\nPersonally, I prefer generating different ngrams to see whether there is any interesting insights could be extracted.\r\n\r\n\r\ndf_page_2_ngram <- tibble()\r\n\r\nfor(i in page_num){\r\n  temp <- \r\n    df[[i]] %>%\r\n    read_lines() %>%\r\n    str_squish() %>%\r\n    as_tibble() %>%\r\n    filter(value != \"\") \r\n\r\ntemp_page <- \r\n  tokens(temp$value, \r\n         remove_punct = TRUE,\r\n         remove_numbers = TRUE,\r\n         remove_symbols = TRUE,\r\n         remove_separators = TRUE,\r\n         split_hyphens = FALSE) %>%\r\n  tokens_tolower() %>%\r\n  tokens_remove(stopwords(language = \"en\", source = \"smart\"), padding = FALSE) %>%\r\n  tokens_replace(pattern = lexicon::hash_lemmas$token, \r\n                 replacement = lexicon::hash_lemmas$lemma) %>%\r\n  tokens_ngrams(n = 2:3, concatenator = \" \") %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count)) %>%\r\n  mutate(page = i)\r\n\r\ndf_page_2_ngram <- df_page_2_ngram %>%\r\n  bind_rows(temp_page)\r\n}\r\n\r\n\r\n\r\n\r\ndf_page_2_ngram %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(tot_count)) %>%\r\n  arrange(desc(tot_count)) %>%\r\n  filter(tot_count >= 15) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"circle\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\nNow the words in the word cloud make more sense.\r\nNow we can see the report is about machine learning and data science.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nRefer to this link for the blog disclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Tyler Lastovich\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-04-textpdf/image/scan_text.jpg",
    "last_modified": "2023-01-04T20:32:39+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-06-timesseries-decom/",
    "title": "Time Series Decomposition",
    "description": "Breaking down to the \"lego block\" of time series",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-11-06",
    "categories": [
      "Time Series"
    ],
    "contents": "\r\n\r\nContents\r\nTime series components\r\nSo\r\nwhen should we use additive or multiplicative components?\r\nDifferences between\r\nthe different methods\r\nDemonstration\r\nSetup the environment\r\n\r\nDecompose the time series\r\nClassical Decomposition\r\nX11\r\nDecomposition\r\nSEATS\r\nDecomposition\r\nSTL\r\nDecomposition\r\n\r\nConclusion\r\n\r\nThis post continues our previous topic on time series.\r\nAs the name of the post suggested, we will be breaking the time\r\nseries into different components.\r\n\r\n\r\n\r\nPhoto by\r\nSen\r\non\r\nUnsplash\r\nTime series components\r\nIn general, time series can be seen as made out of following\r\ncomponents:\r\nTrend\r\nSeasonal\r\nRemainder\r\nIt can be written in following mathematical format:\r\n\\[y_t = S_t + T_t + R_t\\]\r\nwhere\r\n\\(S_t\\) is the seasonal\r\ncomponent\r\n\\(T_t\\) is the trend\r\ncomponent\r\n\\(R_t\\) is the remainder\r\ncomponent\r\nWhile the above mathematical expression shows the additive time\r\nseries decomposition, it can be written as multiplicative decomposition\r\nas well.\r\n\\[y_t = S_t \\times T_t \\times\r\nR_t\\]\r\nSo\r\nwhen should we use additive or multiplicative components?\r\nThis depends on the magnitude of seasonal variations or variation\r\naround trend.\r\nIf the variation seems to be increasing by time, then multiplicative\r\ncomponents are likely to be more suitable.\r\nDifferences between\r\nthe different methods\r\nThere are several methods in decomposing the time series (Hyndman 2021):\r\n\r\n\r\nMethod\r\n\r\n\r\nRemarks\r\n\r\n\r\nClassical decomposition\r\n\r\n\r\nSome of the problems with this method:- the estimate of trend is not\r\navailable for first few and last few observations- trend estimate\r\ntends to over-smooth rapid rises and falls- assume the seasonal\r\ncomponents repeats from year to year, which may not be reasonable-\r\nnot robust to outliers\r\n\r\n\r\nX11 decomposition\r\n\r\n\r\nBased on classical decomposition, but includes many extra steps and\r\nfeatures to overcome the drawbacks of classical decomposition\r\n\r\n\r\nSEATS decomposition\r\n\r\n\r\nThe full name is ‘seasonal extraction in ARIMA time series’This\r\nmethod only works with quarterly and monthly data\r\n\r\n\r\nSTL decomposition\r\n\r\n\r\nThe full name is ‘seasonal decomposition of time series by\r\nLoess’Following are the advantages over the classical, SEATS and\r\nX11 decomposition methods:- Unlike SEATS and X11, this method\r\nhandles any type of seasonality, not only monthly and quarterly\r\ndata- Seasonal component is allowed to change over time-\r\nSeasonal component and smoothness of the trend can be controlled by the\r\nuser- Trend and seasonal component of this method is robust to\r\noutliers\r\n\r\n\r\nDemonstration\r\nSince the high inflation has been a hot topic, in this demonstration,\r\nI will be using the consumer price index dataset I obtained from\r\nSingstat website.\r\n\r\nTaken from giphy\r\nSetup the environment\r\nFirst, I will setup the environment by calling the necessary\r\npackages.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, timetk, tsibble, janitor, feasts, fable, seasonal)\r\n\r\n\r\n\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-10-23-timesseries/data/M212881.csv\", \r\n               skip = 10) %>%\r\n  slice(1:148) %>%\r\n  filter(`Data Series` != \"Hawker Centres\" &\r\n           `Data Series` != \"Food Courts & Coffee Shops\")\r\n\r\n\r\n\r\nAs some of the columns are imported in incorrect column types, I will\r\nuse sapply function to convert the columns into the correct\r\ncolumn types.\r\n\r\n\r\ncols_num <- c(2:ncol(df))\r\n\r\ndf[cols_num] <- sapply(df[cols_num], as.numeric)\r\n\r\n\r\n\r\nOnce the columns are imported into the correct types, I will reshape\r\nthe dataset by using pivot_longer function.\r\n\r\n\r\nstart_date <- \"2014-01-01\"\r\nend_date <- \"2021-12-01\"\r\n\r\n\r\n\r\n\r\n\r\ndf_1 <- df %>%\r\n  pivot_longer(!`Data Series`, names_to = \"Month Year\", values_to = \"CPI\") %>%\r\n  clean_names() %>%\r\n  mutate(month_year_recoded = ym(month_year)) %>%\r\n  select(-month_year) %>%\r\n  filter(month_year_recoded >= start_date & month_year_recoded <= end_date)\r\n\r\n\r\n\r\nas_tsibble function is to convert the dataset into\r\ntsibble dataframe.\r\nWe will pass month_year_recoded into index argument.\r\nAs another thing to take note is we need to pass info\r\n\r\n\r\ndf_1_ts <- df_1 %>%\r\n  mutate(month_year_recoded = yearmonth(month_year_recoded)) %>%\r\n  as_tsibble(index = month_year_recoded,\r\n             key = data_series)\r\n\r\n\r\n\r\nDecompose the time series\r\nIn this demonstration, I will be using one of the items to\r\ndemonstrate on how we could decompose the time series.\r\n\r\n\r\ninterested_cpi_item <- \"Clothing & Footwear\"\r\n\r\n\r\n\r\nBefore jumping into the time series decomposition, let’s look at the\r\ntime series again.\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  autoplot() +\r\n  xlab(\"\") +\r\n  ylab(\"CPI\") +\r\n  labs(title = paste0(\"CPI Time Series of \", interested_cpi_item)) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nClassical Decomposition\r\nFrom the graph, it seems like the seasonality or variation around\r\ntime series are quite consistent throughout the period.\r\nHence, I will indicate the type should be “additive” in the\r\ntype argument in classical_decomposition\r\nfunction.\r\n\r\n\r\nclassical_model <- df_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(classical_decomposition(type = \"additive\"))\r\n\r\nclassical_model\r\n\r\n\r\n# A mable: 1 x 2\r\n# Key:     data_series [1]\r\n  data_series         `classical_decomposition(type = \"additive\")`\r\n  <chr>                                                    <model>\r\n1 Clothing & Footwear                              <DECOMPOSITION>\r\n\r\nThe output consists of the selected data series and the model.\r\nTo extract the info of the fitted model, I will use the\r\ncomponents function.\r\n\r\n\r\nclassical_model %>%\r\n  components()\r\n\r\n\r\n# A dable: 96 x 8 [1M]\r\n# Key:     data_series, .model [1]\r\n# :        cpi = trend + seasonal + random\r\n   data_series    .model month_year_reco~   cpi trend seasonal  random\r\n   <chr>          <chr>             <mth> <dbl> <dbl>    <dbl>   <dbl>\r\n 1 Clothing & Fo~ \"clas~         2014 Jan  99.5  NA     0.0906 NA     \r\n 2 Clothing & Fo~ \"clas~         2014 Feb  97.3  NA     0.457  NA     \r\n 3 Clothing & Fo~ \"clas~         2014 Mar 100.   NA     1.40   NA     \r\n 4 Clothing & Fo~ \"clas~         2014 Apr 100.   NA     0.679  NA     \r\n 5 Clothing & Fo~ \"clas~         2014 May  99.1  NA     0.419  NA     \r\n 6 Clothing & Fo~ \"clas~         2014 Jun  96.6  NA    -1.18   NA     \r\n 7 Clothing & Fo~ \"clas~         2014 Jul  97.3  98.4  -1.22    0.111 \r\n 8 Clothing & Fo~ \"clas~         2014 Aug  99.7  98.3  -0.597   2.06  \r\n 9 Clothing & Fo~ \"clas~         2014 Sep  97.4  98.2  -0.174  -0.694 \r\n10 Clothing & Fo~ \"clas~         2014 Oct  98.2  98.1  -0.0440  0.0717\r\n# ... with 86 more rows, and 1 more variable: season_adjust <dbl>\r\n\r\nAside from the fitted values (i.e. trend, seasonal, random and\r\nseason_adjust), it also contains the original time series value\r\n(i.e. cpi in this demonstration).\r\nAs the default value for seasonality in\r\nclassical_decomposition function is 12 (which 12 means its\r\na monthly seasonality), we can observe that the trend and\r\nrandom column do not contain any value for the first 6\r\nmonths and last 6 months.\r\nWe can visualize the different components by passing the objects into\r\nautoplot function as shown below.\r\n\r\n\r\nclassical_model %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nNote that there are some grey bars on the left side of the\r\ngraphs.\r\nThose bars represents the relative scales of the different components\r\nsince all the components are plotted under different scales.\r\nIn other words, it probably be easier to understand the bars as how\r\nmuch we have “zoomed in” to illustrate the components.\r\nThe longer the bar, the more we have “zoomed in”.\r\nPersonally, I find the bars can cause some confusion when I try to\r\nexplain the graph.\r\nTo avoid the unnecessary confusion, we can turn off the\r\nscale_bars by passing FALSE into the\r\nargument.\r\n\r\n\r\nclassical_model %>%\r\n  components() %>%\r\n  autoplot(scale_bars = FALSE)\r\n\r\n\r\n\r\n\r\nWe can also change the seasonality to other values.\r\nFollowing are the values for the seasonality:\r\n\r\n\r\nseasonality_df <- tibble(Value = c(\"4\", \"12\", \"7\"),\r\n                         `Corresponding seasonality` = c(\"Quarterly\",\r\n                                                         \"Monthly\",\r\n                                                         \"Daily\"))\r\n\r\nseasonality_df %>%\r\n  kbl(escape = FALSE) %>%\r\n  kable_paper(\"hover\", full_width = F, html_font = \"Cambria\", font_size = 15)\r\n\r\n\r\n\r\nValue\r\n\r\n\r\nCorresponding seasonality\r\n\r\n\r\n4\r\n\r\n\r\nQuarterly\r\n\r\n\r\n12\r\n\r\n\r\nMonthly\r\n\r\n\r\n7\r\n\r\n\r\nDaily\r\n\r\n\r\nSo, if I were to change the seasonality to quarterly seasonality,\r\nbelow are the components of the revised model:\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(classical_decomposition(cpi ~ season(4),\r\n                                type = \"additive\")) %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nThis revised decomposition model seems to “perform” worse than the\r\ndecomposition model with monthly seasonality.\r\nUnder the quarterly decomposition model, there seems to have some\r\n“patterns” uncaptured by the seasonality.\r\nX11 Decomposition\r\nNext, I will use X11 decomposition method.\r\n\r\n\r\ntemp <- df_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(X_13ARIMA_SEATS(cpi ~ x11())) %>%\r\n  report()\r\n\r\n\r\nSeries: cpi \r\nModel: X-13ARIMA-SEATS \r\n\r\nCoefficients:\r\n                  Estimate Std. Error z value Pr(>|z|)    \r\nAO2021.Jun        -3.52197    0.85069  -4.140 3.47e-05 ***\r\nAR-Seasonal-12     0.57036    0.09241   6.172 6.75e-10 ***\r\nMA-Nonseasonal-01  0.38491    0.09214   4.178 2.95e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nX11 adj.  ARIMA: (0 1 1)(1 0 0)  Obs.: 96  Transform: none\r\nAICc: 287.1, BIC: 296.9  QS (no seasonality in final):    0  \r\nBox-Ljung (no autocorr.): 15.47   Shapiro (normality): 0.9895  \r\n\r\n\r\n\r\nobjects(temp)\r\n\r\n\r\n[1] \"data_series\"                  \"X_13ARIMA_SEATS(cpi ~ x11())\"\r\n\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(X_13ARIMA_SEATS(cpi ~ x11())) %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nDo refer to this instruction\r\nmanual if you want to find out the arguments can be accepted by the\r\nfunctions.\r\nFor example, by default, the seasonal adjustment decomposition of X11\r\nis set to be pseudo additive.\r\nTo change the type of seasonal adjustment decomposition, we will do\r\nthe following:\r\nInclude the transform argument\r\nOtherwise, the code will return error even if we are not\r\ntransforming the data series\r\n\r\nPass the selected type of seasonal adjustment decomposition\r\ncalculation to the mode argument\r\npage 224\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(X_13ARIMA_SEATS(cpi ~ transform(`function` = \"none\") + x11(mode = \"add\"))) %>%\r\n  report()\r\n\r\n\r\nSeries: cpi \r\nModel: X-13ARIMA-SEATS \r\n\r\nCoefficients:\r\n                  Estimate Std. Error z value Pr(>|z|)    \r\nAO2021.Jun        -3.52197    0.85069  -4.140 3.47e-05 ***\r\nAR-Seasonal-12     0.57036    0.09241   6.172 6.75e-10 ***\r\nMA-Nonseasonal-01  0.38491    0.09214   4.178 2.95e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nX11 adj.  ARIMA: (0 1 1)(1 0 0)  Obs.: 96  Transform: none\r\nAICc: 287.1, BIC: 296.9  QS (no seasonality in final):    0  \r\nBox-Ljung (no autocorr.): 15.47   Shapiro (normality): 0.9895  \r\n\r\nAlso, the manual mentioned above is also applicable for the\r\nSEATS decomposition method that I am covering next.\r\nSEATS Decomposition\r\nNext, I will use SEATS to decompose the time series.\r\nTo do so, we can either just pass in the time series into\r\nX_13ARIMA_SEATS function without specifying the specs or\r\nexplicitly indicate the specs should be seats.\r\nMethod 1: Without specifying the specs\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(X_13ARIMA_SEATS(cpi)) %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nMethod 2: Explicitly specify the specs as\r\nseats\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(X_13ARIMA_SEATS(cpi ~ seats())) %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nNote that the graph does not mention which adjustment is being\r\nselected when seats specs is used to decompose the time\r\nseries.\r\nSimilarly, the report function allows to check the model\r\noutput result.\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(X_13ARIMA_SEATS(cpi ~ seats())) %>%\r\n  report()\r\n\r\n\r\nSeries: cpi \r\nModel: X-13ARIMA-SEATS \r\n\r\nCoefficients:\r\n                  Estimate Std. Error z value Pr(>|z|)    \r\nAO2021.Jun        -3.52197    0.85069  -4.140 3.47e-05 ***\r\nAR-Seasonal-12     0.57036    0.09241   6.172 6.75e-10 ***\r\nMA-Nonseasonal-01  0.38491    0.09214   4.178 2.95e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nSEATS adj.  ARIMA: (0 1 1)(1 0 0)  Obs.: 96  Transform: none\r\nAICc: 287.1, BIC: 296.9  QS (no seasonality in final):    0  \r\nBox-Ljung (no autocorr.): 15.47   Shapiro (normality): 0.9895  \r\n\r\nSTL Decomposition\r\nLastly, I will use STL function to decompose the time\r\nseries.\r\nAs such, I will use the STL function from\r\nfeasts package to perform the task.\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(STL(cpi)) %>%\r\n  components()\r\n\r\n\r\n# A dable: 96 x 8 [1M]\r\n# Key:     data_series, .model [1]\r\n# :        cpi = trend + season_year + remainder\r\n   data_series         .model month_year_reco~   cpi trend season_year\r\n   <chr>               <chr>             <mth> <dbl> <dbl>       <dbl>\r\n 1 Clothing & Footwear STL(c~         2014 Jan  99.5  98.9      0.0215\r\n 2 Clothing & Footwear STL(c~         2014 Feb  97.3  98.8     -0.0474\r\n 3 Clothing & Footwear STL(c~         2014 Mar 100.   98.7      1.39  \r\n 4 Clothing & Footwear STL(c~         2014 Apr 100.   98.7      1.06  \r\n 5 Clothing & Footwear STL(c~         2014 May  99.1  98.6      0.535 \r\n 6 Clothing & Footwear STL(c~         2014 Jun  96.6  98.5     -1.24  \r\n 7 Clothing & Footwear STL(c~         2014 Jul  97.3  98.4     -1.13  \r\n 8 Clothing & Footwear STL(c~         2014 Aug  99.7  98.3     -0.395 \r\n 9 Clothing & Footwear STL(c~         2014 Sep  97.4  98.3     -0.185 \r\n10 Clothing & Footwear STL(c~         2014 Oct  98.2  98.2      0.0158\r\n# ... with 86 more rows, and 2 more variables: remainder <dbl>,\r\n#   season_adjust <dbl>\r\n\r\nAs usual, I will pass the components to autoplot\r\nfunction to visualize the graph.\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(STL(cpi)) %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nSTL function also allows the users to pass in additional\r\nparameters.\r\nFor example, in the previous STL graph, the season is set as “yearly”\r\nas there is a “_year” beside “season”.\r\nWe can modify the seasonality by passing the information to\r\nseason argument as shown below.\r\n\r\n\r\ndf_1_ts %>%\r\n  filter(data_series == interested_cpi_item) %>%\r\n  model(STL(cpi ~ season(3))) %>%\r\n  components() %>%\r\n  autoplot()\r\n\r\n\r\n\r\n\r\nThe graph will show the number of observations in each seasonal\r\nperiod we have indicated in the earlier argument.\r\nFor more information on the parameters allowed, please refer to the\r\ndocumentation page.\r\nfeats\r\ndocumentation\r\nstl\r\ndocumentation\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\nKeila\r\nHötzel on\r\nUnsplash\r\n\r\n\r\n\r\nHyndman, & Athanasopoulos, R. J. 2021. Forecasting: Principles\r\nand Practice, 3rd Edition. https://otexts.com/fpp3/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-06-timesseries-decom/image/lego.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-05-exponential_smooth/",
    "title": "Exponential Smoothing",
    "description": "Smooth the time series out!",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-11-05",
    "categories": [
      "Time Series",
      "Time Series Forecasting"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is exponential\r\nsmoothing?\r\nTrend\r\nSeasonal\r\nfactor\r\nAdditive\r\nComponents\r\nMultiplicative Components\r\n\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nSplit into Training and\r\nTesting Dataset\r\nModel\r\nBuilding\r\nForecast\r\n\r\nConclusion\r\n\r\nPreviously I have attended a time series workshop\r\nconducted by my professor.\r\nIt has got my interest to re-study on my notes on time series.\r\nIt was one of the topic I have learnt when I was learning the\r\ndifferent analytics techniques pertaining to operation analytics.\r\nHence, in this post, I will be summarize what I have learnt about\r\nexponential smoothing and more importantly how to implement this method\r\nin R.\r\n\r\n\r\n\r\nPhoto by cottonbro\r\nWhat is exponential\r\nsmoothing?\r\n(Hyndman\r\n2021a) explained that exponential smoothing produces\r\nforecasts that are weighted averages of past observations, with the\r\nweights decaying exponentially as the observations get older.\r\nFor example, single exponential smoothing has following formula:\r\n\\[\\hat{y}_{t+1} = \\alpha y_{t} + (1 -\r\n\\alpha)\\hat{y}_{t}\\]\r\nwhere\r\n\\(\\hat{y}_{t}\\) refers to the\r\nforecasted value at time t\r\n\\(y_{t}\\) refers to the actual value\r\nat time t\r\n\\(\\alpha\\) is the weight\r\nFrom the formula, we can note that the forecasted value at time t+1\r\nis a weighted average between actual value and forecasted value at time\r\nt.\r\nNow you might be asking why this method is called ‘exponential’?\r\nIf we go back to the formula above, we could replace the forecasted\r\nvalue with the same formula as follows:\r\n\\[\\hat{y}_{t+1} = \\alpha y_{t} + \\alpha (1\r\n- \\alpha){y}_{t-1} + \\alpha (1 - \\alpha)^{2} y_{t-2}+...\\]\r\nAs shown above, the current forecasted value is sum of past actual\r\nvalues, where the weights decreases exponentially when we go back in\r\ntime.\r\nNevertheless, the author also mentioned that this method generates\r\nreliable forecasts quickly and for a wide range of time series, which is\r\na great advantage and of major importance to applications in\r\nindustry.\r\nIn general, below are the three different types of exponential\r\nsmoothing models:\r\nSimple exponential smoothing\r\nDouble exponential smoothing\r\nTriple exponential smoothing\r\n\r\n\r\nSmoothing Method\r\n\r\n\r\nDescription\r\n\r\n\r\nRemarks\r\n\r\n\r\nSimple exponential smoothing\r\n\r\n\r\nSuitable when there is no clear trend or seasonal pattern within the\r\ndata\r\n\r\n\r\n\r\n\r\nDouble exponential smoothing\r\n\r\n\r\nInclude trend\r\n\r\n\r\nA.k.a. Holt-winters’ two parameter or Holt’s linear trend method\r\n\r\n\r\nTriple exponential smoothing\r\n\r\n\r\nInclude both trend and seasonal factor\r\n\r\n\r\nA.k.a. Holt-winters’ three parameter\r\n\r\n\r\nNot to bore the readers the formula under these models, I shall\r\nexclude them from the post. Otherwise this post will be full of\r\nformula.\r\n\r\nTaken from giphy\r\nTrend\r\nFor the trend components, it can be either damped version or\r\nnon-damped version.\r\nDamped as in the trend will be flatted in the future.\r\nSeasonal factor\r\nFor the seasonal factor, it can be either additive or\r\nmultiplicative.\r\nNote that following is the definitions of seasonal and cyclical:\r\n\r\n\r\nTypes\r\n\r\n\r\nDescription\r\n\r\n\r\nSeasonal\r\n\r\n\r\nWhen the components repeat the pattern with a fixed and known duration\r\n\r\n\r\nCyclical\r\n\r\n\r\nWhen the components seem to have some regular and repeated fluctuation\r\naround the trend but we don’t know the duration of the fluctuations\r\n\r\n\r\nAdditive Components\r\nThis would be more appropriate when the magnitude of the seasonal\r\nfluctuations, or the variations around the trend-cycle, does not vary\r\nwith the level of the time series (Hyndman 2021b).\r\nThe additive components can be written as follows:\r\n\\[y_t = S_t + T_t + R_t\\]\r\nWhere\r\n\\(y_t\\) is the data\r\n\\(S_t\\) is the seasonal\r\ncomponent\r\n\\(T_t\\) is the trend-cycle\r\ncomponent\r\n\\(R_t\\) is the remainder\r\ncomponent\r\nMultiplicative Components\r\nWhen the variation in the seasonal pattern, or the variation around\r\nthe trend-cycle, appears to be proportional to the level of time series,\r\nthen a multiplicative decomposition is more appropriate.\r\nThe multiplicative components can be written as follows:\r\n\\[y_t = S_t \\times T_t \\times\r\nR_t\\]\r\nWhere\r\n\\(y_t\\) is the data\r\n\\(S_t\\) is the seasonal\r\ncomponent\r\n\\(T_t\\) is the trend-cycle\r\ncomponent\r\n\\(R_t\\) is the remainder\r\ncomponent\r\nThat’s all for the discussion on exponential smoothing.\r\nFollowing are the summary of the discussion above:\r\n\r\n\r\n\r\nTaken from Chapter 8.4 of Forecasting: Principles and\r\nPractice\r\nDemonstration\r\nIn this demonstration, I will use the comsumer price index dataset\r\nfrom SingStat.\r\n\r\n\r\n\r\nPhoto by\r\nColeen\r\nRivas on\r\nUnsplash\r\nSetup the environment\r\nFirst, I will load the necessary packages into the environment.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, timetk, tsibble, janitor, \r\n               feasts, fable, rsample)\r\n\r\n\r\n\r\nImport Data\r\nNext, I will import in the dataset and perform some pre-processing to\r\nclean up the data.\r\nRefer to my previous\r\nblog post for the pre-processing discussion.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-10-23-timesseries/data/M212881.csv\", \r\n               skip = 10) %>%\r\n  slice(1:148) %>%\r\n  filter(`Data Series` != \"Hawker Centres\" &\r\n           `Data Series` != \"Food Courts & Coffee Shops\")\r\n\r\n# convert columns into numeric\r\ncols_num <- c(2:ncol(df))\r\ndf[cols_num] <- sapply(df[cols_num], as.numeric)\r\n\r\n\r\n\r\n\r\n\r\n# pivot longer the columns\r\ndf_1 <- df %>%\r\n  pivot_longer(!`Data Series`, \r\n               names_to = \"Month Year\", \r\n               values_to = \"CPI\") %>%\r\n  clean_names() %>%\r\n  mutate(month_year_recoded = ym(month_year)) %>%\r\n  select(-month_year) %>%\r\n  filter(month_year_recoded >= \"2014-01-01\" & month_year_recoded <= \"2020-12-01\") %>%\r\n  filter(data_series == \"All Items\")\r\n\r\n\r\n\r\n\r\n\r\n# convert into tsibble data frame\r\ndf_1_ts <- df_1 %>%\r\n  mutate(month_year_recoded = yearmonth(month_year_recoded)) %>%\r\n  as_tsibble(index = month_year_recoded,\r\n             key = data_series)\r\n\r\n\r\n\r\nSplit into Training and\r\nTesting Dataset\r\nrsample package provides users the function\r\n(i.e. initial_time_split) to split the time series data\r\ninto training and testing dataset.\r\n\r\n\r\nts_split <- df_1_ts %>%\r\n  initial_time_split(prop = 0.8)\r\n\r\n\r\n\r\nSimilar to the usual train test split for machine learning, I will\r\nuse training and testing function to split the\r\ndataset.\r\n\r\n\r\nts_train <- training(ts_split)\r\nts_test <- testing(ts_split)\r\n\r\n\r\n\r\nModel Building\r\nNow that is done, I will pass the training dataset into the\r\nmodel function. I will also specify I would like to build\r\nan ETS model by using ETS function.\r\n\r\n\r\nts_ets <- ts_train %>%\r\n  select(-data_series) %>%\r\n  model(ETS(cpi))\r\n\r\n\r\n\r\nNote that I have not specified the following in the ETS\r\nfunction:\r\nWhether there is a trend or seasonality\r\nShould the trend or seasonality be additive or\r\nmultiplicative\r\nShould the trend or season be damped\r\n\r\n\r\nts_ets\r\n\r\n\r\n# A mable: 1 x 1\r\n    `ETS(cpi)`\r\n       <model>\r\n1 <ETS(A,N,A)>\r\n\r\nOn surface, it seems like there is no info for the fitted model. The\r\ninfo of fitted model is actually within the object.\r\nSo, we can “unpack” the info by using augment\r\nfunction.\r\n\r\n\r\naugment(ts_ets)\r\n\r\n\r\n# A tsibble: 67 x 6 [1M]\r\n# Key:       .model [1]\r\n   .model   month_year_recoded   cpi .fitted    .resid    .innov\r\n   <chr>                 <mth> <dbl>   <dbl>     <dbl>     <dbl>\r\n 1 ETS(cpi)           2014 Jan  99.5    99.4  0.144     0.144   \r\n 2 ETS(cpi)           2014 Feb  99.5    99.6 -0.138    -0.138   \r\n 3 ETS(cpi)           2014 Mar  99.7    99.6  0.156     0.156   \r\n 4 ETS(cpi)           2014 Apr  99.3    99.3 -0.000674 -0.000674\r\n 5 ETS(cpi)           2014 May  99.7    99.6  0.0641    0.0641  \r\n 6 ETS(cpi)           2014 Jun  99.5    99.7 -0.195    -0.195   \r\n 7 ETS(cpi)           2014 Jul  99.2    99.3 -0.0478   -0.0478  \r\n 8 ETS(cpi)           2014 Aug  99.7    99.6  0.160     0.160   \r\n 9 ETS(cpi)           2014 Sep  99.6    99.6 -0.0699   -0.0699  \r\n10 ETS(cpi)           2014 Oct  99.3    99.3  0.00315   0.00315 \r\n# ... with 57 more rows\r\n\r\nAs shown, we can see that fitted model contains the original values,\r\nthe fitted values and residuals.\r\nAlternatively, we could pass the fitted model into the\r\nreport function to summarize the model info (eg. model\r\nparameters, model performance etc).\r\n\r\n\r\nts_ets %>% \r\n  report()\r\n\r\n\r\nSeries: cpi \r\nModel: ETS(A,N,A) \r\n  Smoothing parameters:\r\n    alpha = 0.6682514 \r\n    gamma = 0.0001000068 \r\n\r\n  Initial states:\r\n     l[0]      s[0]      s[-1]      s[-2]      s[-3]     s[-4]\r\n 99.44194 0.0604655 0.02728087 -0.2385938 0.08574436 0.1519245\r\n      s[-5]     s[-6]      s[-7]     s[-8]     s[-9]     s[-10]\r\n -0.2016484 0.1103206 0.06183502 -0.207833 0.1466583 0.08352913\r\n      s[-11]\r\n -0.07968302\r\n\r\n  sigma^2:  0.0479\r\n\r\n      AIC      AICc       BIC \r\n 92.48791 101.89967 125.55830 \r\n\r\nAccording to the documentation\r\npage, following are the meaning of the different smoothing\r\nparameters:\r\nalpha - value of smoothing parameter for the level\r\nLarger alpha value means the model will give more weights to the\r\nmore recent observations\r\n\r\nbeta - value of smoothing parameter for the slope\r\nLarger beta value means the model will give more weights to the more\r\nrecent trend\r\n\r\nphi - value of dampening parameter for the slope\r\ngamma - value of the smoothing parameter for the seasonal\r\npattern\r\nLarger gamma value means the model will give more weights to the\r\nmore recent seasonality\r\n\r\nTo help us to visually inspect the fitted model, we could use\r\ngg_tsresiduals function.\r\nThe graph would contain the following:\r\nInnovation residuals\r\nACF chart\r\nDistribution of the residuals\r\n\r\n\r\ngg_tsresiduals(ts_ets)\r\n\r\n\r\n\r\n\r\nNote that innovation residuals is the residuals on the transformed\r\nscale if there is transformation used in the model (Hyndman\r\n2021c).\r\nI can only say the name “innovation residual” is so fancy.\r\n\r\nTaken from giphy\r\nNevertheless, the graphs from gg_tsresiduals function\r\nshow us the following:\r\nThe innovation residuals hover around 0, except for a couple of\r\ndata points\r\nThe residual distribution chart also shows a left skewed,\r\nresulted by outliers\r\nBy the excluding the outliers, the residual distribution would look\r\ncloser to a normal distribution with mean = 0\r\n\r\nNone of the bars in ACF graph exceeds the blue lines, suggesting\r\nthat the autocorrelations are not statistically different from\r\nzero.\r\nEarlier I have used auto method to find the combination of error,\r\ntrend and season that gives the best model since I have not defined the\r\nerror, trend and season in the function.\r\nIf we want to self define the model, we could pass in additional\r\narguments into ETS function as following:\r\n\r\n\r\nts_train %>%\r\n  select(-data_series) %>%\r\n  model(ETS(cpi ~ error(\"A\") + trend(\"Ad\") + season(\"M\"))) %>%\r\n  report()\r\n\r\n\r\nSeries: cpi \r\nModel: ETS(A,Ad,M) \r\n  Smoothing parameters:\r\n    alpha = 0.5446765 \r\n    beta  = 0.04208475 \r\n    gamma = 0.0001036066 \r\n    phi   = 0.9771571 \r\n\r\n  Initial states:\r\n     l[0]        b[0]     s[0]    s[-1]     s[-2]    s[-3]    s[-4]\r\n 99.78493 -0.04522087 1.000853 1.000824 0.9981717 1.001364 1.001747\r\n     s[-5]    s[-6]    s[-7]     s[-8]    s[-9]   s[-10]    s[-11]\r\n 0.9977731 1.000635 1.000436 0.9977261 1.001104 1.000435 0.9989317\r\n\r\n  sigma^2:  0.0492\r\n\r\n      AIC      AICc       BIC \r\n 96.31516 110.56516 135.99962 \r\n\r\nRefer to the documentation\r\npage for the forms allowed under error,\r\ntrend and season arguments.\r\nForecast\r\nTo forecast the future values, I will use forecast\r\nfunction.\r\nI will also indicate the forecast time period should be 17 months so\r\nthat I can compare with the testing dataset.\r\n\r\n\r\nts_ets_forecast <- ts_ets %>%\r\n  forecast(h = \"17 months\")\r\n\r\n\r\n\r\nAfter that, I will plot the actual CPI, fitted CPI and forecast\r\nCPI.\r\n\r\n\r\nts_train %>%\r\n  ggplot(aes(x = month_year_recoded, \r\n             y = cpi,\r\n             color = \"Actual - Training\")) +\r\n  autolayer(ts_ets_forecast, alpha = 0.3) +\r\n  geom_line(size = 1) +\r\n  geom_line(aes(y = .fitted,\r\n                color = \"Fitted\"),\r\n            size = 1,\r\n            data = augment(ts_ets)) +\r\n  geom_line(aes(y = .mean,\r\n                color = \"Forecast\"),\r\n            size = 1,\r\n            data = ts_ets_forecast) +\r\n  geom_line(aes(y = cpi,\r\n                color = \"Actual - Testing\"),\r\n            size = 1,\r\n            data = ts_test) +\r\n  labs(title = \"CPI\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        legend.position = \"bottom\",\r\n        legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nOver here, I have used autolayer function to plot out\r\nthe forecast values in a ggplot layer. This function will plot out the\r\nforecast values in a line form and shade the 80 and 95 confidence\r\nintervals as default confidence intervals to show on the graph.\r\nThe forecasted values are rather flat, compared to the actual\r\ntraining values. This is consistent with the model info, i.e. the fitted\r\nmodel have beta value of 0.\r\nAlso, we can observe that the fitted value tends to lag behind the\r\nactual value. The spike or drop in the values tend to happen one period\r\nafter the actual value.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\nAkram\r\nHuseyn on\r\nUnsplash\r\n\r\n\r\n\r\nHyndman, & Athanasopoulos, R. J. 2021a. Forecasting: Principles\r\nand Practice, 3rd Edition. https://otexts.com/fpp3/expsmooth.html.\r\n\r\n\r\n———. 2021b. Forecasting: Principles and Practice, 3rd Edition.\r\nhttps://otexts.com/fpp3/expsmooth.html.\r\n\r\n\r\n———. 2021c. Forecasting: Principles and Practice, 3rd Edition.\r\nhttps://otexts.com/fpp3/residuals.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-05-exponential_smooth/image/smoothen.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-23-timesseries/",
    "title": "Visualizing Time Series",
    "description": "It's about time",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-10-23",
    "categories": [
      "Time Series",
      "Data Visualization"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is\r\ntime series?\r\nDemonstration\r\nSetup the environment\r\n\r\nVisualization\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Palu\r\nMalerba\r\nRecently there is a lot of discussions on rising inflation.\r\nThis makes me curious about how inflation has changed over time.\r\n\r\n\r\n\r\nExtracted from CNA\r\non 23 Oct 2022\r\nBefore we start visualizing the time series, let’s take a look what\r\nis time series.\r\nWhat is time series?\r\nTime series data is the data collected at regular time intervals.\r\nAn observed time series can be decomposed into three components: the\r\ntrend (long term direction), the seasonal (systematic, calendar related\r\nmovements) and the irregular (unsystematic, short term fluctuations)\r\n(Australian Bureau of Statistics).\r\nDemonstration\r\nIn this demonstration, I will be using the consumer price index\r\ndataset I obtained from Singstat website.\r\nThis is also to satisfy my curiosity about what is driving inflation\r\nin Singapore upwards.\r\nSetup the environment\r\nFirst, I will set up the environment by calling the necessary\r\npackages.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, timetk, tsibble, janitor, \r\n               feasts, plotly, wesanderson)\r\n\r\n\r\n\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\ndf <- read_csv(\"data/M212881.csv\", skip = 10) %>%\r\n  slice(1:148) %>%\r\n  filter(`Data Series` != \"Hawker Centres\" &\r\n           `Data Series` != \"Food Courts & Coffee Shops\")\r\n\r\n\r\n\r\nAs some of the columns are imported in incorrect column types, I will\r\nuse sapply function to convert the columns into the correct\r\ncolumn types.\r\n\r\n\r\ncols_num <- c(2:ncol(df))\r\n\r\ndf[cols_num] <- sapply(df[cols_num], as.numeric)\r\n\r\n\r\n\r\nOnce the columns are imported into the correct types, I will reshape\r\nthe dataset by using pivot_longer function.\r\n\r\n\r\nstart_date <- \"2014-01-01\"\r\nend_date <- \"2022-06-01\"\r\n\r\n\r\n\r\n\r\n\r\ndf_1 <- df %>%\r\n  pivot_longer(!`Data Series`, names_to = \"Month Year\", values_to = \"CPI\") %>%\r\n  clean_names() %>%\r\n  mutate(month_year_recoded = ym(month_year)) %>%\r\n  select(-month_year) %>%\r\n  filter(month_year_recoded >= start_date & month_year_recoded <= end_date)\r\n\r\n\r\n\r\nas_tsibble function is to convert the dataset into\r\ntsibble dataframe.\r\nWe will pass month_year_recoded into index argument.\r\nAnother thing to take note of is we need to pass info\r\n\r\n\r\ndf_1_ts <- df_1 %>%\r\n  mutate(month_year_recoded = yearmonth(month_year_recoded)) %>%\r\n  as_tsibble(index = month_year_recoded,\r\n             key = data_series)\r\n\r\n\r\n\r\nThe has_gaps function can help us to check whether there\r\nare any gaps in the data.\r\n\r\n\r\nhas_gaps(df_1_ts) %>%\r\n  filter(.gaps == TRUE)\r\n\r\n\r\n# A tibble: 0 x 2\r\n# ... with 2 variables: data_series <chr>, .gaps <lgl>\r\n\r\n\r\n\r\ndf_1 %>%\r\n  as_tsibble(index = month_year_recoded,\r\n             key = data_series) %>%\r\n  has_gaps()\r\n\r\n\r\n# A tibble: 146 x 2\r\n   data_series                                                   .gaps\r\n   <chr>                                                         <lgl>\r\n 1 Accommodation                                                 TRUE \r\n 2 Air Fares                                                     TRUE \r\n 3 Alcoholic Drinks & Tobacco                                    TRUE \r\n 4 All Items                                                     TRUE \r\n 5 All Items Less Accommodation                                  TRUE \r\n 6 All Items Less Imputed Rentals On Owner-Occupied Accommodati~ TRUE \r\n 7 Audio-Visual Equipment & Others                               TRUE \r\n 8 Beef, Chilled                                                 TRUE \r\n 9 Beer                                                          TRUE \r\n10 Biscuits & Cookies                                            TRUE \r\n# ... with 136 more rows\r\n\r\nIf the dataset contains only one key, we can use\r\nas_tsibble function without specifying the key.\r\n\r\n\r\ndf_1 %>%\r\n  filter(data_series == \"All Items\") %>%\r\n  as_tsibble(index = month_year_recoded)\r\n\r\n\r\n# A tsibble: 102 x 3 [1D]\r\n   data_series   cpi month_year_recoded\r\n   <chr>       <dbl> <date>            \r\n 1 All Items    99.5 2014-01-01        \r\n 2 All Items    99.5 2014-02-01        \r\n 3 All Items    99.7 2014-03-01        \r\n 4 All Items    99.3 2014-04-01        \r\n 5 All Items    99.7 2014-05-01        \r\n 6 All Items    99.5 2014-06-01        \r\n 7 All Items    99.2 2014-07-01        \r\n 8 All Items    99.7 2014-08-01        \r\n 9 All Items    99.6 2014-09-01        \r\n10 All Items    99.3 2014-10-01        \r\n# ... with 92 more rows\r\n\r\nOnce the dataset is converted into tsibble format, the usual\r\nfilter function would not work.\r\nTo do so, use filter_index function to perform the\r\nfiltering job.\r\n\r\n\r\ndf_1_ts %>%\r\n  filter_index(~\"2021-12\")\r\n\r\n\r\n# A tsibble: 14,016 x 3 [1M]\r\n# Key:       data_series [146]\r\n   data_series     cpi month_year_recoded\r\n   <chr>         <dbl>              <mth>\r\n 1 Accommodation  117.           2014 Jan\r\n 2 Accommodation  117.           2014 Feb\r\n 3 Accommodation  117.           2014 Mar\r\n 4 Accommodation  114.           2014 Apr\r\n 5 Accommodation  116.           2014 May\r\n 6 Accommodation  116.           2014 Jun\r\n 7 Accommodation  114.           2014 Jul\r\n 8 Accommodation  116.           2014 Aug\r\n 9 Accommodation  116.           2014 Sep\r\n10 Accommodation  114.           2014 Oct\r\n# ... with 14,006 more rows\r\n\r\nVisualization\r\nNext, I will visualize how the CPI has changed between 2014-01-01 and\r\n2022-06-01.\r\nAs such, I will use ggplot functions to visualize the\r\nCPI.\r\nI will also pre-defined the dates of the two arrows I will be adding\r\nto the graph. The date info is obtained from Google.\r\n\r\n\r\ndate_arrow <- as_date(\"2020-04-07\", \"%Y-%m-%d\")\r\ndate_petrol_duty <- as_date(\"2021-02-16\", \"%Y-%m-%d\")\r\ndate_ukraine_arrow <- as_date(\"2022-02-20\", \"%Y-%m-%d\")\r\n\r\ndf_1 %>%\r\n  filter(data_series == \"All Items\") %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n         y = cpi)) +\r\n  geom_line(size = 0.5) +\r\n  geom_smooth(method = \"loess\", se = FALSE) +\r\n  geom_hline(aes(yintercept = 100), \r\n             color = \"blue\", \r\n             linetype = 2) +\r\n  annotate(\"segment\", \r\n           x = date_arrow, \r\n           y = 101.5, \r\n           xend = date_arrow, \r\n           yend = 100.55, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_arrow, \r\n           y = 101.9, \r\n           label = \"Circuit Breaker\") +\r\n  annotate(\"segment\", \r\n           x = date_petrol_duty, \r\n           y = 103, \r\n           xend = date_petrol_duty,\r\n           yend = 102, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 103.65, \r\n           label = \"Petrol Duty\") +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 103.25, \r\n           label = \"Increase\") +\r\n  annotate(\"segment\", \r\n           x = date_ukraine_arrow, \r\n           y = 99, \r\n           xend = date_ukraine_arrow, \r\n           yend = 99.9, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 98.8, \r\n           label = \"Ukraine\") +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 98.4, \r\n           label = \"War\") +\r\n  labs(title = \"Singapore Consumer Price Index\",\r\n       caption = \"Data Source: Singstat (extracted as at Oct 2022)\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        plot.caption = element_text(hjust = 0))\r\n\r\n\r\n\r\n\r\n\r\n\r\ncircuit_breaker <- \r\n  list(x = date_arrow, \r\n       y = 101,\r\n       text = \"Circuit Breaker\",\r\n       showarrow = TRUE,\r\n       arrowhead = 1,\r\n       ax = 0)\r\n\r\npetrol_duty <-\r\n  list(x = date_petrol_duty, \r\n       y = 103.5,\r\n       text = \"Petrol Duty Increase\",\r\n       showarrow = TRUE,\r\n       arrowhead = 1,\r\n       ax = 0)\r\n\r\nukraine_war <- \r\n  list(x = date_ukraine_arrow, \r\n       y = 98.7,\r\n       text = \"Ukraine War\",\r\n       showarrow = TRUE,\r\n       arrowhead = 1,\r\n       ax = 0,\r\n       ay = 60)\r\n\r\ncaption <-\r\n  list(x = 0, \r\n       y = -0.15, \r\n       text = \"Data source: Singstat (extracted as at Oct 2022)\", \r\n       showarrow = F,\r\n       xref='paper', \r\n       yref='paper')\r\n\r\ndf_1 %>%\r\n  filter(data_series == 'All Items') %>%\r\n  plot_time_series(month_year_recoded,\r\n                   cpi,\r\n                   .line_size = 0.5,\r\n                   .interactive = TRUE,\r\n                   .smooth_degree = 2,\r\n                   .title = \"Singapore Consumer Product Index\") %>%\r\n  layout(annotations = list(circuit_breaker, \r\n                            ukraine_war, \r\n                            petrol_duty,\r\n                            caption)) %>%\r\n  add_lines(x = range(df_1$month_year_recoded), \r\n            y = 100, \r\n            line = list(color = \"blue\",\r\n                        dash = 'dot')) \r\n\r\n\r\n\r\n\r\nAlternatively, we can still use the conventional methods to create\r\nthe interactive graphs, e.g. plotting the graph by using\r\nplot_ly function or creating the ggplot object\r\nand then passing the object to ggplotly function.\r\nI prefer the second method so I need not remember so many\r\nsyntaxes.\r\n\r\n\r\nggplotly(\r\n  df_1 %>%\r\n  filter(data_series == \"All Items\") %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n         y = cpi)) +\r\n  geom_line(size = 0.5) +\r\n  geom_smooth(method = \"loess\", se = FALSE) +\r\n  geom_hline(aes(yintercept = 100), \r\n             color = \"blue\", \r\n             linetype = 2) +\r\n  annotate(\"segment\", \r\n           x = date_arrow, \r\n           y = 101.5, \r\n           xend = date_arrow, \r\n           yend = 100.55, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_arrow, \r\n           y = 102, \r\n           label = \"Circuit Breaker\") +\r\n  annotate(\"segment\", \r\n           x = date_petrol_duty, \r\n           y = 103, \r\n           xend = date_petrol_duty, \r\n           yend = 102, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 103.85, \r\n           label = \"Petrol Duty\") +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 103.3, \r\n           label = \"Increase\") +\r\n  annotate(\"segment\", \r\n           x = date_ukraine_arrow, \r\n           y = 99, \r\n           xend = date_ukraine_arrow, \r\n           yend = 99.9, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 98.7, \r\n           label = \"Ukraine\") +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 98.15, \r\n           label = \"War\") +\r\n  labs(title = \"Singapore Consumer Price Index\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        plot.caption = element_text(hjust = 0))\r\n  ) %>%\r\n  layout(annotations = list(caption))\r\n\r\n\r\n\r\n\r\nAlthough overall CPI is increasing, the different components within\r\nCPI have different trends.\r\nSplit by different components within CPI\r\n\r\n\r\ndf_1 %>%\r\n  filter(data_series == 'Food' |\r\n          data_series == 'Food Excl Food Serving Services' |\r\n          data_series == 'Food Serving Services' |\r\n          data_series == 'Clothing & Footwear' |\r\n          data_series == 'Housing & Utilities' |\r\n          data_series == 'Household Durables & Services' |\r\n          data_series == 'Health Care' |\r\n          data_series == 'Transport' |\r\n          data_series == 'Communication' |\r\n          data_series == 'Recreation & Culture' |\r\n          data_series == 'Education' |\r\n          data_series == 'Miscellaneous Goods & Services') %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n             y = cpi)) +\r\n  geom_line() +\r\n  geom_hline(aes(yintercept = 100), color = \"blue\", linetype = 2) +\r\n  facet_wrap(~data_series, labeller = labeller(data_series = label_wrap_gen(25))) +\r\n    labs(title = str_wrap(\"Singapore Consumer Price Index By Components Between  2014 Jan - 2021 Dec\", 65),\r\n       caption = \"Data Source: Singstat (extracted as at Oct 2022)\") +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0)) +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.text.x = element_text(angle = 90),\r\n        axis.title.y = element_blank(),\r\n        plot.caption = element_text(hjust = 0))\r\n\r\n\r\n\r\n\r\n\r\n\r\ndf_1 %>%\r\n  filter(data_series == \"Private Transport\" |\r\n           data_series == \"Public Transport\" |\r\n           data_series == \"Other Transport Services\") %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n         y = cpi,\r\n         color = data_series)) +\r\n  geom_line(size = 1.2) +\r\n  geom_hline(aes(yintercept = 100), color = \"blue\", linetype = 2) +\r\n  labs(title = str_wrap(\"Singapore CPI on Different Transport Components Between 2014 Jan and 2021 Dec\", 65),\r\n       caption = \"Data Source: Singstat (extracted as at Oct 2022)\") +\r\n  scale_color_brewer(palette = \"Pastel1\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        legend.position = \"top\",\r\n        legend.justification='left',\r\n        legend.title = element_blank(),\r\n        plot.caption = element_text(hjust = 0))\r\n\r\n\r\n\r\n\r\n\r\n\r\ndf_1 %>%\r\n  filter(data_series == \"Cars\" |\r\n           data_series == \"Motorcycles\" |\r\n           data_series == \"Petrol\" |\r\n           data_series == \"Other Private Transport\") %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n         y = cpi,\r\n         color = data_series)) +\r\n  geom_line(size = 1.2) +\r\n  geom_hline(aes(yintercept = 100), \r\n             color = \"blue\", \r\n             linetype = 2) +\r\n  annotate(\"segment\", \r\n           x = date_arrow, \r\n           y = 115, \r\n           xend = date_arrow, \r\n           yend = 108, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_arrow, \r\n           y = 120, \r\n           label = \"Circuit Breaker\") +\r\n  annotate(\"segment\", \r\n           x = date_petrol_duty, \r\n           y = 130, \r\n           xend = date_petrol_duty, \r\n           yend = 115, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 139, \r\n           label = \"Petrol Duty\") +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 134, \r\n           label = \"Increase\") +\r\n  annotate(\"segment\", \r\n           x = date_ukraine_arrow, \r\n           y = 90, \r\n           xend = date_ukraine_arrow, \r\n           yend = 99, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 89, \r\n           label = \"Ukraine\") +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 84, \r\n           label = \"War\") +\r\n  labs(title = str_wrap(\"Singapore CPI on Different Private Transport Components Between  2014 Jan and 2021 Dec\", 65),\r\n       caption = \"Data Source: Singstat (extracted as at Oct 2022)\") +\r\n  scale_color_brewer(palette = \"Pastel1\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        legend.position = \"top\",\r\n        legend.justification='left',\r\n        legend.title = element_blank(),\r\n        plot.caption = element_text(hjust = 0))  \r\n\r\n\r\n\r\n\r\nStrangely enough that the CPI for clothing and footwear drop\r\n\r\n\r\ndf_1 %>%\r\n  filter(data_series == \"Clothing & Footwear\") %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n         y = cpi)) +\r\n  geom_line(size = 0.5) +\r\n  geom_hline(aes(yintercept = 100), color = \"blue\", linetype = 2) +\r\n  labs(title = str_wrap(\"Singapore CPI on Clothing and Footwear Between  2014 Jan - 2021 Dec\", 65),\r\n       caption = \"Data Source: Singstat (extracted as at Oct 2022)\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        plot.caption = element_text(hjust = 0))\r\n\r\n\r\n\r\n\r\n\r\n\r\ndf_1 %>%\r\n  filter(data_series == \"Clothing\" |\r\n           data_series == \"Other Articles & Related Services\" |\r\n           data_series == \"Footwear\") %>%\r\n  ggplot(aes(x = month_year_recoded,\r\n         y = cpi,\r\n         color = data_series)) +\r\n  geom_line(size = 1.2) +\r\n  geom_hline(aes(yintercept = 100), \r\n             color = \"blue\", \r\n             linetype = 2) +\r\n  annotate(\"segment\", \r\n           x = date_arrow, \r\n           y = 110, \r\n           xend = date_arrow, \r\n           yend = 102, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_arrow, \r\n           y = 113, \r\n           label = \"Circuit Breaker\") +\r\n  annotate(\"segment\", \r\n           x = date_petrol_duty, \r\n           y = 120, \r\n           xend = date_petrol_duty, \r\n           yend = 102, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 125, \r\n           label = \"Petrol Duty\") +\r\n  annotate(\"text\", \r\n           x = date_petrol_duty, \r\n           y = 122, \r\n           label = \"Increase\") +\r\n  annotate(\"segment\", \r\n           x = date_ukraine_arrow, \r\n           y = 90, \r\n           xend = date_ukraine_arrow, \r\n           yend = 99, \r\n           color = \"red\", \r\n           arrow = arrow(length = unit(0.05,\"npc\"))) +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 89, \r\n           label = \"Ukraine\") +\r\n  annotate(\"text\", \r\n           x = date_ukraine_arrow, \r\n           y = 86, \r\n           label = \"War\") +\r\n  labs(title = str_wrap(\"Singapore CPI on Clothing & Footwear Between 2014 Jan and 2021 Dec\", 65),\r\n       caption = \"Data Source: Singstat (extracted as at Oct 2022)\") +\r\n  scale_color_brewer(palette = \"Pastel1\") +\r\n  theme_minimal() +\r\n  theme(axis.title.x = element_blank(),\r\n        axis.title.y = element_blank(),\r\n        legend.position = \"top\",\r\n        legend.justification='left',\r\n        legend.title = element_blank(),\r\n        plot.caption = element_text(hjust = 0))\r\n\r\n\r\n\r\n\r\nDespite all odds, consumer price index of clothing and footwear\r\ncontinue to drop.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Soulful\r\nPizza\r\n\r\n\r\n\r\nAustralian Bureau of Statistics. “Time Series Analysis: The\r\nBasics.” https://www.abs.gov.au/websitedbs/d3310114.nsf/home/time+series+analysis:+the+basics.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-23-timesseries/image/esplanade.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-24-glm/",
    "title": "Generalized Linear Model",
    "description": "Back to Actuaries Most Beloved Model",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2022-09-24",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nGeneralized Linear Models\r\n(GLM)\r\nAssumptions\r\nFamily\r\nOffset\r\nfunction\r\nPros and cons of using GLM\r\nmodels\r\n\r\nApplication of GLM models\r\nDemonstration\r\nSetup the environment\r\nDataset\r\nExplotary Data Analysis\r\n(EDA)\r\nModel Building\r\nData\r\nSplitting\r\nBuilding\r\nModel\r\n\r\nCross\r\nValidation\r\nPrediction\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring actuaries’ most beloved model,\r\ngeneralized linear model (GLM).\r\n\r\n\r\n\r\nPhoto by\r\nJason\r\nLeung on\r\nUnsplash\r\nThis will set the scene for future sharing on other topics, such as\r\ngeneralized additive model, frequency severity modeling and so on.\r\nGeneralized Linear Models\r\n(GLM)\r\nGLM is an extension of linear model.\r\nIn linear model, we assume the relationship between independent and\r\ndependent variables are linear.\r\nHowever, this assumption is not appropriate in most of the scenarios\r\n(eg. insurance premium setting). This is where GLM could help us in our\r\nmodeling work.\r\nThe idea of using GLM is that with appropriate transformation on the\r\ntarget variable, the relationship between independent variables and\r\ntransformed dependent variable remain linear.\r\n(kjytay 2020) A GLM model consists of\r\nfollowing 3 parts:\r\nLinear predictor \\(\\eta_i=\\beta_0+\\beta^Tx_i\\)\r\nLink function \\(\\eta_i=g(\\mu_i)\\)\r\nRandom component \\(y_i\\sim\r\nf(y|\\mu_i)\\)\r\nwhere the general form of GLM formula can be found below:\r\n\\[y_i=\\beta^Tx_i+\\epsilon_i\\]\r\nAssumptions\r\nBelow are some of the assumptions when using GLM models:\r\nThe predictors (a.k.a. independent variables) are independent\r\nfrom one another\r\nThe dependent variable is assumed to be one of the distribution\r\nfrom an exponential family (eg. normal, Poisson, binomial etc)\r\nErrors are independent from one another\r\nThe model performance might be affected if any of the assumptions is\r\nbeing violated.\r\nFamily\r\nBelow are some of the families supported under GLM:\r\nGaussian\r\nBinomial\r\nPoisson\r\nMultinomial\r\nCox\r\nTweedie\r\n(Molnar 2022)\r\nsummarized down the appropriate families for the different predictions\r\nwe are trying to make. This is the link\r\nto the book.\r\nOffset function\r\nThe offset enters the score in an additive way.\r\nThis\r\npost provides a clear example on how the offset function works in a\r\nPoisson regression context.\r\nThen you might be asking instead of passing feature into offset\r\nfunction, why can’t we just divide the target variable by offset\r\nvariable?\r\nThe short answer is the likelihood function would change if we divide\r\nthe target variable by using the offset variable. (Tiwari 2020) provides the mathematical\r\nproof on how the likelihood would change if we divide the target\r\nvariable by the offset variable.\r\nOne important thing to note when using offset is offset should be in\r\nthe same scale as the target variable. For example, when using Poisson\r\nfamily, the default link function is log. The offset variable should be\r\nlog as well.\r\nNote that offset function should not be confused with weight function\r\nin GLM.\r\nIn short, following are the differences between offset &\r\nweight:\r\nOffset is to adjust the target so that the targets of the\r\ndifferent data points are in same unit of measurements\r\nWeight is to assign different importance to some observations\r\nthan the rest\r\nPros and cons of using GLM\r\nmodels\r\nOf course there is not perfect model. Sometimes the advantages of the\r\nmodels bring could be disadvantages of using the relevant models.\r\nFollowing are advantages and disadvantages of using GLM models:\r\nAdvantages of using GLM\r\nDoes not assume the relationship between independent and\r\ndependent variables is linear\r\nEasier to interpret than some other models, such as neural\r\nnetwork\r\nLess resistant from the business users since GLM can produce\r\nrating factors, which has been widely used\r\nDisadvantages of using GLM\r\nSensitive to outliers\r\nUnable to handle categorical variables\r\nModel performance will be affected if any of the model\r\nassumptions mentioned above is being violated\r\nApplication of GLM models\r\n(Michel, Donatien, and Trufin\r\n2019) GLMs were originally introduced to improve the accuracy\r\nof motor insurance pricing. Today GLM is being used in many insurance\r\ncontexts.\r\n(Michel, Donatien, and Trufin\r\n2019) also listed some of the typical insurance use case by\r\nusing GLMs in the book:\r\nClaim count modeling\r\nClaim amount modeling\r\nGraduation of mortality and morbidity rates\r\nElasticity modeling\r\nLoss reserving\r\nCompetitive analysis\r\nNevertheless, let’s start the demonstration!\r\n\r\n\r\n\r\nPhoto by\r\nClemens\r\nvan Lay on\r\nUnsplash\r\nDemonstration\r\nSetup the environment\r\nIn this demonstration, I will be using glmnet package to\r\nbuild the GLM model. The documentation can be found under this link.\r\nAt the point of writing, it seems like tidymodels\r\npackage doesn’t support all functionalities of glmnet\r\npackage.\r\nAccording to this issue\r\nticket, it seems like tidymodels is unable to support\r\noffset function for time being. glmnet requires the users\r\nto specify the variable to be used as offset in both training and\r\ntesting datasets. However, tidymodels only allows the users\r\nto specify offset in training dataset.\r\nTherefore, I will be using glmnet package directly in\r\nbuilding the model and tidymodels package to prepare the\r\ndata for model building.\r\nI will call the relevant packages to setup the environment.\r\n\r\n\r\npacman::p_load(tidyverse, readr, tidymodels, corrplot, glmnet,\r\n              glue)\r\n\r\n\r\n\r\nDataset\r\nFor the demonstration, I will be using the car insurance dataset from\r\nChapter 1 of Predictive Modeling Applications in Actuarial Science -\r\nVolume 1.\r\nThe dataset can be found from the book\r\nwebsite.\r\n\r\n\r\n\r\nPhoto by\r\nwhy\r\nkei on\r\nUnsplash\r\nI will first import the data into environment.\r\nI will also specify that the year of driving licence should be\r\nimported as character, instead of continuous variable.\r\n\r\n\r\ndf <- read_csv(\"data/sim-modeling-dataset.csv\",\r\n               col_types = list(yrs.lic = col_character()))\r\n\r\n\r\n\r\nThis is a rather clean data to work with since there is no missing\r\nvalue.\r\nExplotary Data Analysis (EDA)\r\nBefore building the model, I will perform some simple EDA to perform\r\nsimple data cleaning.\r\nI will first select all the numeric variables.\r\n\r\n\r\ndf_num <- df %>%\r\n  select_if(is.numeric)\r\n\r\n\r\n\r\nThen I will pass them into corrplot function.\r\n\r\n\r\ncorrplot(cor(df_num, use=\"pairwise.complete.obs\"), \r\n         method = \"number\", \r\n         type = \"upper\", \r\n         tl.cex = 0.65, \r\n         number.cex = 0.65, \r\n         diag = FALSE)\r\n\r\n\r\n\r\n\r\nNote that there are some variables are perfectly correlated.\r\nBased on the variable names, it seems like these variables are\r\nmeasuring the same things.\r\nHence I will remove the highly correlated variable.\r\n\r\n\r\ndf_1 <- df %>%\r\n  select(-c(year, drv.age, veh.age, clm.incurred))\r\n\r\n\r\n\r\nAlso, GLM is unable to handle non-numeric variables.\r\nTo overcome this, I have used to the step_dummy function\r\nin recipe package to encode the non-numeric variables by using one hot\r\nencoding method. all_nominal_predictiors function will tell\r\nthe function to identify all the nominal independent variables in the\r\ndataset.\r\nThen I will pass the recipe into the prep function. The\r\ncurrent dataset will then be passed into bake function to\r\ngenerate out the pre-processed data.\r\n\r\n\r\ndf_recoded <- df_1 %>%\r\n  recipe(clm.count ~ .) %>%\r\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%\r\n  prep() %>%\r\n  bake(df_1)\r\n\r\n\r\n\r\nNext, I will be building a model to predict the claim count from the\r\nvarious policies.\r\nModel Building\r\nData Splitting\r\nI will split the dataset into training and testing dataset.\r\n\r\n\r\ndf_split <- initial_split(df_recoded, prop = 0.6, strata = clm.count)\r\n\r\n\r\n\r\nAs usual, I will use training and testing\r\nfunction to crave the dataset into training and testing dataset.\r\n\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nBuilding Model\r\nglmnet requires the independent variables to be in\r\nmatrix format. Hence I will pass the object into\r\ndata.matrix to convert the tibble table into matrix\r\nformat.\r\n\r\n\r\n# predictors need to be in matrix format\r\nx <- df_train %>%\r\n  select(-c(clm.count, exposure)) %>%\r\n  data.matrix()\r\n\r\n\r\n\r\nSimilarly, I will “cut out” the target variable from the training\r\ndataset.\r\n\r\n\r\ny <- df_train$clm.count\r\n\r\n\r\n\r\nI will also convert the test dataset into matrix format.\r\n\r\n\r\nnewx <- df_test %>%\r\n  select(-c(clm.count, exposure)) %>%\r\n  data.matrix()\r\n\r\n\r\n\r\nNext, I will start building the model.\r\nAs I am trying to predict the claim count, hence I will specify\r\nPoisson to be the family in the GLM function.\r\n\r\n\r\nglmnet_model <- glmnet(x, \r\n                       y, \r\n                       family = \"poisson\", \r\n                       offset = log(df_train$exposure))\r\n\r\n\r\n\r\nAs in the data, we are not told whether the claim counts and claim\r\namounts are being adjusted for the exposure. Hence, I will assume the\r\nclaim counts and claim amounts are not adjusted for the exposure of the\r\npolicies.\r\nTo adjust for the exposure, I will specify what should be the\r\n“offset” in the model.\r\nAlso, glmnet package allows us to fit regularized model.\r\nThe algorithm will fit a lasso model if we didn’t specify the value of\r\nalpha.\r\nTo fit a ridge model, we just need to set the alpha to be 0 as shown\r\nbelow.\r\n\r\n\r\nglmnet(x, \r\n       y, \r\n       family = \"poisson\", \r\n       offset = log(df_train$exposure),\r\n       alpha = 0)\r\n\r\n\r\n\r\ntidy function from tidymodels package can\r\nbe used to convert the output into tidy data format so that its easier\r\nto read and perform data wrangling.\r\n\r\n\r\nglmnet_model %>% \r\n  tidy()\r\n\r\n\r\n# A tibble: 1,714 x 5\r\n   term         step estimate lambda dev.ratio\r\n   <chr>       <dbl>    <dbl>  <dbl>     <dbl>\r\n 1 (Intercept)     1    -1.82 0.0263  2.53e-14\r\n 2 (Intercept)     2    -1.77 0.0240  3.54e- 3\r\n 3 (Intercept)     3    -1.73 0.0218  6.50e- 3\r\n 4 (Intercept)     4    -1.69 0.0199  8.96e- 3\r\n 5 (Intercept)     5    -1.65 0.0181  1.10e- 2\r\n 6 (Intercept)     6    -1.62 0.0165  1.27e- 2\r\n 7 (Intercept)     7    -1.59 0.0151  1.41e- 2\r\n 8 (Intercept)     8    -1.56 0.0137  1.53e- 2\r\n 9 (Intercept)     9    -1.54 0.0125  1.71e- 2\r\n10 (Intercept)    10    -1.53 0.0114  1.88e- 2\r\n# ... with 1,704 more rows\r\n\r\nThe results above show the different lambda values are being used to\r\nfit the model.\r\nThe model results also contains dev.ratio, which\r\nrepresents the proportion of deviance explained.\r\nCross Validation\r\nAlternatively, glmnet package also has functions to help\r\nus in performing cross validation.\r\nAs from the documentation, it doesn’t seem like the cross validation\r\nfunction in glmnet package allows us to iterate over\r\ndifferent alpha values.\r\nTo overcome this, I will loop the cross validation over different\r\nalpha values.\r\n\r\n\r\nalpha_list <- c(0, 0.25, 0.5, 0.75, 1)\r\n\r\nfor (i in alpha_list){\r\n  assign(glue(\"cv_glmnet_\", i), cv.glmnet(x, \r\n                       y, \r\n                       family = \"poisson\",\r\n                       intercept = FALSE,\r\n                       type.measure = \"deviance\", \r\n                       alpha = i,\r\n                       nfolds = 20, \r\n                       offset = log(df_train$exposure))\r\n         )\r\n  \r\n  print(glue(\"cv_glmnet_\", i))\r\n  print(get(glue(\"cv_glmnet_\", i)))\r\n}\r\n\r\n\r\ncv_glmnet_0\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n    Lambda Index Measure       SE Nonzero\r\nmin  0.824   100  0.3988 0.008554      43\r\n1se 13.429    70  0.4070 0.007821      43\r\ncv_glmnet_0.25\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n     Lambda Index Measure       SE Nonzero\r\nmin 0.00330   100  0.3898 0.005572      29\r\n1se 0.03703    74  0.3949 0.005402       6\r\ncv_glmnet_0.5\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n    Lambda Index Measure       SE Nonzero\r\nmin 0.0681    60  0.4026 0.006298       1\r\n1se 0.8395    33  0.4082 0.005454       2\r\ncv_glmnet_0.75\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n    Lambda Index Measure       SE Nonzero\r\nmin 0.0498    59  0.4025 0.005125       1\r\n1se 0.5100    34  0.4069 0.004504       1\r\ncv_glmnet_1\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n    Lambda Index Measure       SE Nonzero\r\nmin 0.0374    59  0.4025 0.006287       1\r\n1se 0.4607    32  0.4084 0.005440       1\r\n\r\nwe can pull out the coefficient by calling coef\r\nfunction.\r\n\r\n\r\ncoef(cv_glmnet_0.25)\r\n\r\n\r\n45 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                                   s1\r\n(Intercept)              .           \r\nrow.id                   .           \r\ndriver.age              -0.0006220311\r\nyrs.licensed            -0.0268724105\r\nncd.level               -0.0720340462\r\nregion                   .           \r\nvehicle.age              .           \r\nvehicle.value            .           \r\nseats                    .           \r\nccm                      .           \r\nhp                       .           \r\nweight                   .           \r\nlength                   .           \r\nwidth                    .           \r\nheight                  -0.8287184062\r\nprior.claims             0.0342454915\r\nnb.rb_NB                 .           \r\nnb.rb_RB                -0.0175072183\r\ndriver.gender_Female     .           \r\ndriver.gender_Male       .           \r\nmarital.status_Divorced  .           \r\nmarital.status_Married   .           \r\nmarital.status_Single    .           \r\nmarital.status_Widow     .           \r\nyrs.lic_X1               .           \r\nyrs.lic_X2               .           \r\nyrs.lic_X3               .           \r\nyrs.lic_X4               .           \r\nyrs.lic_X5               .           \r\nyrs.lic_X6               .           \r\nyrs.lic_X7               .           \r\nyrs.lic_X8               .           \r\nyrs.lic_X9               .           \r\nyrs.lic_X9.              .           \r\nbody.code_A              .           \r\nbody.code_B              .           \r\nbody.code_C              .           \r\nbody.code_D              .           \r\nbody.code_E              .           \r\nbody.code_F              .           \r\nbody.code_G              .           \r\nbody.code_H              .           \r\nfuel.type_Diesel         .           \r\nfuel.type_Gasoline       .           \r\nfuel.type_LPG            .           \r\n\r\nNote that over here, I am pulling the coefficients from the\r\nregularized model when cross validation error is within one standard\r\nerror of the minimum.\r\nAlternatively, we could extract the coefficient of the regularized\r\nmodel when cross validation error is the minimum by setting s to be\r\n“lambda.min” in the argument.\r\n\r\n\r\ncoef(cv_glmnet_0.25, s = \"lambda.min\")\r\n\r\n\r\n45 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                                   s1\r\n(Intercept)              .           \r\nrow.id                   3.077315e-06\r\ndriver.age              -6.003862e-03\r\nyrs.licensed            -7.089763e-02\r\nncd.level               -9.562086e-02\r\nregion                  -7.018851e-04\r\nvehicle.age             -1.777227e-02\r\nvehicle.value           -7.633382e-03\r\nseats                    .           \r\nccm                      .           \r\nhp                       6.876185e-04\r\nweight                   .           \r\nlength                   5.860735e-03\r\nwidth                    2.131378e-01\r\nheight                  -7.334423e-01\r\nprior.claims             1.281979e-01\r\nnb.rb_NB                 .           \r\nnb.rb_RB                -1.769263e-01\r\ndriver.gender_Female     .           \r\ndriver.gender_Male      -8.771084e-02\r\nmarital.status_Divorced -9.900298e-02\r\nmarital.status_Married  -7.775612e-03\r\nmarital.status_Single    .           \r\nmarital.status_Widow     5.797761e-01\r\nyrs.lic_X1               2.569229e-02\r\nyrs.lic_X2               .           \r\nyrs.lic_X3              -4.162422e-02\r\nyrs.lic_X4               7.175686e-02\r\nyrs.lic_X5               .           \r\nyrs.lic_X6              -2.272347e-01\r\nyrs.lic_X7               .           \r\nyrs.lic_X8              -1.465572e-01\r\nyrs.lic_X9               3.793601e-01\r\nyrs.lic_X9.              .           \r\nbody.code_A              .           \r\nbody.code_B             -2.891222e-01\r\nbody.code_C              .           \r\nbody.code_D              .           \r\nbody.code_E              8.646049e-03\r\nbody.code_F             -7.096296e-03\r\nbody.code_G              1.862983e-01\r\nbody.code_H              1.299067e-01\r\nfuel.type_Diesel         .           \r\nfuel.type_Gasoline      -9.764875e-02\r\nfuel.type_LPG            .           \r\n\r\nAlternatively, we could use tidy function to tidy up the\r\nobjects and extract the output from the steps we are interested in.\r\n\r\n\r\ntidy(cv_glmnet_0.25$glmnet.fit) %>%\r\n  filter(step == 75)\r\n\r\n\r\n# A tibble: 6 x 5\r\n  term          step  estimate lambda dev.ratio\r\n  <chr>        <dbl>     <dbl>  <dbl>     <dbl>\r\n1 driver.age      75 -0.000946 0.0337     0.589\r\n2 yrs.licensed    75 -0.0309   0.0337     0.589\r\n3 ncd.level       75 -0.0748   0.0337     0.589\r\n4 height          75 -0.812    0.0337     0.589\r\n5 prior.claims    75  0.0424   0.0337     0.589\r\n6 nb.rb_RB        75 -0.0291   0.0337     0.589\r\n\r\nPrediction\r\nTo obtain the predictions, I will pass “response” to the type\r\naugment.\r\nSince I have used offset to fit the model, offset needs to be\r\nspecified in the predict function again based on the documentation (kjytay 2019).\r\n\r\n\r\nglmnet_predict <- predict(glmnet_model, \r\n                          type = \"response\", \r\n                          newx = newx, \r\n                          newoffset = log(df_test$exposure))\r\n\r\n\r\n\r\nNote that according to this stack\r\noverflow ticket, without performing cross validation, the glmnet\r\nwill generate the predictions under all lambda values.\r\nAfter performing cross validation, the algorithm will choose the\r\nlargest lambda which MSE is witin one standard error of the smallest MSE\r\nby default.\r\n\r\n\r\ncv_predict_glmnet <- predict(cv_glmnet_0.25, \r\n                             newx = newx, \r\n                             newoffset = log(df_test$exposure), \r\n                             type = \"response\")\r\n\r\ncv_predict_glmnet %>% \r\n  as_tibble()\r\n\r\n\r\n# A tibble: 16,304 x 1\r\n   lambda.1se\r\n        <dbl>\r\n 1     0.121 \r\n 2     0.156 \r\n 3     0.0130\r\n 4     0.158 \r\n 5     0.158 \r\n 6     0.159 \r\n 7     0.194 \r\n 8     0.153 \r\n 9     0.197 \r\n10     0.193 \r\n# ... with 16,294 more rows\r\n\r\nI have passed the predictions to as_tibble function to\r\nconvert the object into a tibble table. This is to avoid R to print out\r\nall the predictions when we call the predictions.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\nMarkus\r\nSpiske on\r\nUnsplash\r\n\r\n\r\n\r\nkjytay. 2019. “A Deep Dive into Glmnet: Offset.” https://www.r-bloggers.com/2019/01/a-deep-dive-into-glmnet-offset/.\r\n\r\n\r\n———. 2020. “Glmnet V4.0: Generalizing the Family\r\nParameter.” https://statisticaloddsandends.wordpress.com/2020/05/14/glmnet-v4-0-generalizing-the-family-parameter/.\r\n\r\n\r\nMichel, Denuit, Hainaut Donatien, and Julien Trufin. 2019. Effective\r\nStatistical Learning Methods for Actuaries i: GLMs and Extensions.\r\nSpringer.\r\n\r\n\r\nMolnar, Christoph. 2022. Interpretable Machine Learning: A Guide for\r\nMaking Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/extend-lm.html#more-lm-extension.\r\n\r\n\r\nTiwari, Ajay. 2020. “Offsetting the Model — Logic to\r\nImplementation.” https://towardsdatascience.com/offsetting-the-model-logic-to-implementation-7e333bc25798.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-24-glm/image/tiles.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-10-kaplan-meier/",
    "title": "Survival Modelling - Kaplan-Meier",
    "description": "In the long run we are all dead - John Maynard Keynes",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-09-10",
    "categories": [
      "Machine Learning",
      "Survival Model"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is survival analysis?\r\nSurvival\r\nfunction\r\nHazard\r\nfunction\r\nCensoring\r\nSurvival analysis vs\r\nlogistic regression\r\nKaplan-Meier Survival\r\nCurve\r\nAdvantages\r\nand disadvantages of using Kaplan-Meier\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nSurvival curve\r\nOverall survival curve\r\nVisualize Survival Curve\r\n\r\nSurvival curve by groups\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by\r\nIan\r\nKeefe on\r\nUnsplash\r\nRecently, I was reading my notes on survival analysis.\r\nAlthough this was one of the topics I learned during my\r\nundergraduate, I forgot most of the details so I thought it would be\r\nbeneficial to read more about survival analysis.\r\nIn this post, I will be summarizing what is survival analysis and\r\nsome important concepts of survival analysis.\r\nI will be using one of the popular methods, Kaplan-Meier to estimate\r\nthe survival curve.\r\nWhat is survival analysis?\r\nSurvival analysis corresponds to a set of statistical approaches used\r\nto investigate the time it takes for an event of interest to occur (STHDA).\r\nSurvival analysis is also commonly known as ‘time to event’\r\nanalysis.\r\nLet’s look at some important concepts of survival models.\r\nSurvival function\r\nSurvival function is defined as the probability of surviving up to a\r\npoint t.\r\nIt can be written as follows:\r\n\\[S(t) = Pr(T > t)\\]\r\nHazard function\r\nThe hazard function is defined as the instantaneous rate at which\r\nsubject’s experience the event of interest, given that they have\r\nsurvived up to time t (Bartlett 2014).\r\nThe function is also known as intensity function, instantaneous\r\nfailure rate or force of mortality.\r\n\\[h(t)=\\lim_{\\delta\\to0}\\frac{Pr(t<T<t+\\delta|T>t)}{\\delta}\\]\r\nCensoring\r\nCensoring occurs when we have some information about individual\r\nsurvival time, but we don’t know the time exactly (Sestelo 2017).\r\nThe author also summarized the reasons on why censoring may occur as\r\nfollows:\r\na person does not experience the event before the study\r\nends\r\na person is lost to follow-up during the study period\r\na person withdraws from the study because of death (if death is\r\nnot the event of interest) or some other reason\r\nIn general, there are three types of censoring:\r\n\r\n\r\nType\r\n\r\n\r\nDescription\r\n\r\n\r\nRight\r\n\r\n\r\nOccur when the true unobserved event is on the right of the censoring\r\ntime\r\n\r\n\r\nLeft\r\n\r\n\r\nEvent has already occurred before the observed time\r\n\r\n\r\nInterval\r\n\r\n\r\nThe event of interest happen within a specific interval of time\r\n\r\n\r\nI find the illustration by Professor Pesta making it easier to\r\nunderstand the different types of censoring.\r\n\r\n\r\n\r\nIllustration by Michal\r\nPesta\r\nSurvival analysis vs\r\nlogistic regression\r\nAt the first glance, it looks like both survival analysis and\r\nlogistic regression are performing the same analysis.\r\nHowever, there is a difference between survival analysis and logistic\r\nregression.\r\nSurvival analysis accounts of censored data in the analysis, where\r\nlogistic regression treats all the individuals didn’t experience the\r\nevent as ‘survived’ (Zablotski 2021).\r\nThis would overestimate the survival probability.\r\nIn this post, I will be exploring Kaplan-Meier (KM) method to\r\nestimate the survival curve.\r\nKaplan-Meier Survival Curve\r\nKM is a non-parametric method in estimating the survival curve.\r\nKM survival curve is defined as the probability of surviving in a\r\ngiven length of time while considering time in many small intervals\r\n(Manish Kumar Goel 2010).\r\nThe formula can be written as following:\r\n\\[\\hat{S}(t) = \\prod_{t_i \\leq j}(1-\r\n\\frac{d_i}{n_i})\\]\r\nwhere\r\n\\(\\hat{S}(t)\\) is the\r\nconditional survival probability at time t\r\n\\(d_i\\) is the number of failure\r\nevents at time \\(t_i\\)\r\n\\(n_i\\) is the number of people\r\nat risk at time \\(t_i\\)\r\nIn short, the function estimates the survival probability of a\r\nparticular individual, provided this individual has survived in all the\r\nprevious period.\r\nAdvantages\r\nand disadvantages of using Kaplan-Meier\r\nBelow are some of the advantages of using KM model (Zablotski\r\n2021; Sestelo\r\n2017):\r\nCommonly used to describe survival\r\nCommonly used to compare two study populations\r\nIntuitive graphical presentation\r\nDoes not require many assumptions for using such method\r\nFollowing are the disadvantages of using KM model:\r\nUnable to model numeric variables\r\nUnable to include many explanatory variables\r\nDoes not control for covariates\r\nCannot accommodate time-dependent variables\r\nDemonstration\r\nIn this demonstration, I will be using this bank\r\ndataset from Kaggle.\r\n\r\n\r\n\r\nPhoto by\r\nAlex\r\nMotoc on\r\nUnsplash\r\nSetup the environment\r\nFirst, I will load the necessary packages into the environment.\r\n\r\n\r\npacman::p_load(tidyverse, lubridate, tidymodels, rsample, survival, \r\n               censored, janitor, survminer)\r\n\r\n\r\n\r\nWith this, I will be using survival package to perform\r\nthe survival analysis.\r\n\r\n\r\n\r\nTaken from survival Github\r\nImport Data\r\nFirst I will import the dataset into the environment.\r\nI will also clean the column names, drop the columns I don’t need and\r\ntransform the columns to be the right format.\r\n\r\n\r\ndf <- read_csv(\"data/Churn_Modelling.csv\") %>%\r\n  clean_names() %>%\r\n  select(-c(row_number, customer_id)) %>%\r\n  mutate(has_cr_card = factor(has_cr_card),\r\n         is_active_member = factor(is_active_member),\r\n         age_group = case_when(age < 30 ~ \"20s\",\r\n                               age < 40 ~ \"30s\",\r\n                               age < 50 ~ \"40s\",\r\n                               age < 60 ~ \"50s\",\r\n                               TRUE ~ \"60s+\"))\r\n\r\n\r\n\r\nSurvival curve\r\nAccording to the documentation\r\npage, if the event variable (i.e. exited\r\ncolumn in this dataset) is a factor, the survival type will be assumed\r\nto multi-state.\r\nThe Surv object would look slightly different from the\r\nusual single state survival analysis as shown below.\r\n\r\n\r\ndf_1_factor <- df %>%\r\n  mutate(exited = as.factor(exited))\r\n\r\n# only show the first 10 records\r\nSurv(df_1_factor$tenure, df_1_factor$exited)[1:10]\r\n\r\n\r\n [1] 2:1 1+  8:1 1+  2+  8:1 7+  4:1 4+  2+ \r\n\r\nHowever, for this dataset, there are only two possible outcomes under\r\nexited variable, i.e. whether the customer has left the\r\nbank or remains as a customer.\r\nSo, I will convert the exited variable to numeric\r\nvariable instead.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(exited = as.numeric(exited))\r\n\r\n\r\n\r\nNow, if I were to call the Surv object, we will note\r\nthat it looks slightly different from the multi-state Surv\r\nobject.\r\n\r\n\r\nSurv(df_1$tenure, df_1$exited)[1:10]\r\n\r\n\r\n [1] 2  1+ 8  1+ 2+ 8  7+ 4  4+ 2+\r\n\r\nNote that the tenure with a plus sign indicates the data point is\r\nbeing right censored.\r\nAlso note that event = 1 indicates that the event has been observed\r\nand event = 0 indicates that the event is being censored.\r\nOverall survival curve\r\nNow, I will start building the survival curve.\r\nWith that, I will be using the survfit function to fit\r\nthe survival curve.\r\nNote that the argument is very similar to how we usually fit a\r\nmachine learning model, except the dependent variable is now a\r\nsurv object.\r\n\r\n\r\nsurv_fit <- survfit(Surv(tenure, exited) ~ 1, data = df_1)\r\n\r\n\r\n\r\nIn order to show the overall survival curve, I will pass “1” as the\r\nindependent variable into the survfit function.\r\n\r\n\r\nsurv_fit\r\n\r\n\r\nCall: survfit(formula = Surv(tenure, exited) ~ 1, data = df_1)\r\n\r\n         n events median 0.95LCL 0.95UCL\r\n[1,] 10000   2037     10      10      NA\r\n\r\nWe can also extract the fitted survival curve by passing the created\r\nsurvfit object into summary function.\r\n\r\n\r\nsummary(surv_fit)\r\n\r\n\r\nCall: survfit(formula = Surv(tenure, exited) ~ 1, data = df_1)\r\n\r\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\r\n    0  10000      95    0.991 0.00097        0.989        0.992\r\n    1   9587     232    0.967 0.00182        0.963        0.970\r\n    2   8552     201    0.944 0.00238        0.939        0.948\r\n    3   7504     213    0.917 0.00294        0.911        0.923\r\n    4   6495     203    0.888 0.00347        0.882        0.895\r\n    5   5506     209    0.855 0.00404        0.847        0.863\r\n    6   4494     196    0.817 0.00466        0.808        0.827\r\n    7   3527     177    0.776 0.00535        0.766        0.787\r\n    8   2499     197    0.715 0.00647        0.703        0.728\r\n    9   1474     213    0.612 0.00857        0.595        0.629\r\n   10    490     101    0.486 0.01309        0.461        0.512\r\n\r\nApart of the cumulative survival curve, the result contains the\r\nfollowing:\r\nhow many people at risk at the beginning of each time point\r\n(i.e. n.risk)\r\nhow many event has occurred during the time point\r\n(i.e. n.event)\r\nstandard error and confidence interval of the survival\r\nvalue\r\nVisualize Survival Curve\r\nThere are a few way to visualise the survival curve.\r\nIn this sub-section, I will be exploring the different approaches to\r\nvisualize the survival curve.\r\nMethod 1: Use ggsurplot function from\r\nsurvminer pacakage\r\nTo visualize the survival curve, we could use the\r\nggsurvplot function from survminer\r\npackage.\r\n\r\n\r\nggsurvplot(surv_fit)\r\n\r\n\r\n\r\n\r\nMeanwhile, the package also offers the users a list of additional\r\narguments to modify the graph.\r\nFor example, we can draw the median survival time in the graph.\r\n\r\n\r\nggsurvplot(surv_fit, surv.median.line = \"hv\")\r\n\r\n\r\n\r\n\r\nOne of the interesting argument is that the ggsurvplot\r\nfunction enables the users to risk tables by “turning on”\r\nrisk.table argument.\r\n\r\n\r\ngraph <- ggsurvplot(surv_fit,\r\n                    risk.table = TRUE)\r\n\r\n# remove the formating on the risk tables\r\ngraph$table <- graph$table + theme_cleantable()\r\n\r\ngraph\r\n\r\n\r\n\r\n\r\nTo add the title, we could pass the title name into the title\r\nargument as shown below.\r\n\r\n\r\nggsurvplot(surv_fit,\r\n           title = \"Overall Survival Curve\")\r\n\r\n\r\n\r\n\r\nAlternatively, we could use the usual ggplot approach to include the\r\nmodifications we would like to make on the graph.\r\n\r\n\r\nggsurvplot(surv_fit) +\r\n  labs(title = \"Overall Survival Curve\")\r\n\r\n\r\n\r\n\r\nNote that the graph object from ggsurvplot function is\r\nnot ggplot object. Hence, some of the usual way of modifying the graph\r\n(eg. layering the different layers to modify the graph) may not\r\nwork.\r\nAs such, we will need to use the list of allowed arguments to modify\r\nthe graph.\r\nFor more details, please refer to documentation\r\npage.\r\nMethod 2: Use autoplot function from\r\nggfortify package\r\nNext, I will use autoplot function from\r\nggfortify function to create the graph.\r\n\r\n\r\nautoplot(surv_fit)\r\n\r\n\r\n\r\n\r\nThe advantage of this method is the created graph is a ggplot\r\nobject.\r\nThis would allow us to make changes to the graph through different\r\nggplot layers.\r\n\r\n\r\nautoplot(surv_fit) +\r\n  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) +\r\n  scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1),\r\n                     labels = c(\"0%\", \"25%\", \"50%\", \"75%\", \"100%\"),\r\n                     limits = c(0, 1)) +\r\n  xlab(\"Time\") +\r\n  ylab(\"Survival Probability\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nNote that the censor objects appeared at the each step of survival\r\ncurve. This is because in the dataset, the tenure captured for each\r\ncustomer is an integer, i.e. doesn’t have any decimals.\r\nMethod 3: Use functions from ggplot2\r\npackage\r\nLastly, I will use the good old ggplot2 package to\r\ncreate the survival curve from scratch.\r\nTo do so, I will first pass the survfit object into\r\ntidy function to make the fitted object into a tibble\r\ndata.\r\n\r\n\r\nsurv_fit_tb <- surv_fit %>%\r\n  tidy()\r\n\r\nsurv_fit_tb\r\n\r\n\r\n# A tibble: 11 x 8\r\n    time n.risk n.event n.censor estimate std.error conf.high conf.low\r\n   <dbl>  <dbl>   <dbl>    <dbl>    <dbl>     <dbl>     <dbl>    <dbl>\r\n 1     0  10000      95      318    0.990  0.000979     0.992    0.989\r\n 2     1   9587     232      803    0.967  0.00188      0.970    0.963\r\n 3     2   8552     201      847    0.944  0.00252      0.948    0.939\r\n 4     3   7504     213      796    0.917  0.00320      0.923    0.911\r\n 5     4   6495     203      786    0.888  0.00390      0.895    0.882\r\n 6     5   5506     209      803    0.855  0.00473      0.863    0.847\r\n 7     6   4494     196      771    0.817  0.00570      0.827    0.808\r\n 8     7   3527     177      851    0.776  0.00689      0.787    0.766\r\n 9     8   2499     197      828    0.715  0.00904      0.728    0.703\r\n10     9   1474     213      771    0.612  0.0140       0.629    0.595\r\n11    10    490     101      389    0.486  0.0269       0.512    0.461\r\n\r\nAlternatively, the surv_summary function from\r\nsurvminer package could be used to “tidy” the results into\r\na data frame.\r\n\r\n\r\nsurv_summary(surv_fit)\r\n\r\n\r\n   time n.risk n.event n.censor      surv      std.err     upper\r\n1     0  10000      95      318 0.9905000 0.0009793424 0.9924031\r\n2     1   9587     232      803 0.9665305 0.0018830569 0.9701042\r\n3     2   8552     201      847 0.9438138 0.0025219694 0.9484906\r\n4     3   7504     213      796 0.9170238 0.0032021035 0.9227971\r\n5     4   6495     203      786 0.8883624 0.0039013915 0.8951814\r\n6     5   5506     209      803 0.8546414 0.0047314807 0.8626038\r\n7     6   4494     196      771 0.8173673 0.0057038892 0.8265563\r\n8     7   3527     177      851 0.7763483 0.0068930935 0.7869081\r\n9     8   2499     197      828 0.7151476 0.0090420982 0.7279346\r\n10    9   1474     213      771 0.6118054 0.0140126693 0.6288411\r\n11   10    490     101      389 0.4856986 0.0269487042 0.5120420\r\n       lower\r\n1  0.9886006\r\n2  0.9629698\r\n3  0.9391601\r\n4  0.9112866\r\n5  0.8815954\r\n6  0.8467525\r\n7  0.8082805\r\n8  0.7659302\r\n9  0.7025853\r\n10 0.5952312\r\n11 0.4607104\r\n\r\nThe output from surv_summary function is same as the\r\noutput from tidy function, with some differences on the\r\ncolumn naming.\r\nThen, I will pass the tibble into ggplot function.\r\nVoilà!\r\n\r\n\r\nsurv_fit_tb %>%\r\n  ggplot() +\r\n  # survival curve\r\n  geom_step(aes(x = time, y = estimate)) +\r\n  # shaded area of confidence interval\r\n  geom_rect(aes(xmin = time, \r\n                xmax = lead(time),\r\n                ymin = conf.low, \r\n                ymax = conf.high),\r\n            fill = \"blue\",\r\n            alpha = 0.2) +\r\n  scale_y_continuous(labels = scales::percent, \r\n                     limits = c(0, 1)) +\r\n  labs(title = \"Overall Survival Curve\") +\r\n  xlab(\"Time\") +\r\n  ylab(\"Survival Probability\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nSurvival curve by groups\r\nNext, I will fit the survival curve by different groups.\r\n\r\n\r\nsurv_fit_age_group <- survfit(Surv(tenure, exited) ~ age_group, data = df_1)\r\n\r\nggsurvplot(surv_fit_age_group)\r\n\r\n\r\n\r\n\r\nFrom the graph above, somehow the younger customers are more\r\n“sticky”.\r\nThey have higher median survival time than the older customers.\r\nIf we input more than 1 variable, the function will create the\r\ndifferent survival curves through permutating over the different\r\ncategories of selected variables.\r\nFor example, if we were to pass both age_group and\r\ngender into the function, below is how the graph would look\r\nlike:\r\n\r\n\r\nsurv_fit_age_gender <- survfit(Surv(tenure, exited) ~ age_group + gender, data = df_1)\r\n\r\nggsurvplot(surv_fit_age_gender)\r\n\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\ntakahiro\r\ntaguchi on\r\nUnsplash\r\n\r\n\r\n\r\nBartlett, Jopnathan. 2014. “Interpreting Changes in Hazard and\r\nHazard Ratios.” https://thestatsgeek.com/2014/03/28/interpreting-changes-in-hazard-and-hazard-ratios/.\r\n\r\n\r\nManish Kumar Goel, Jugal Kishore, Pardeep Khanna. 2010.\r\n“Understanding Survival Analysis: Kaplan-Meier Estimate.”\r\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059453/.\r\n\r\n\r\nSestelo, Marta. 2017. A Short Course on Survival Analysis Applied to\r\nthe Financial Industry. Bookdown.\r\n\r\n\r\nSTHDA. “Survival Analysis Basics.” http://www.sthda.com/english/wiki/survival-analysis-basics.\r\n\r\n\r\nZablotski, Yury. 2021. “yuzaR-Blog: Survival Analysis 1: A Gentle\r\nIntroduction into Kaplan-Meier Curves.” https://yuzar-blog.netlify.app/posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-10-kaplan-meier/image/fire.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-06-fairness/",
    "title": "Fairness",
    "description": "Is the model judgement fair?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "Machine Learning",
      "Model Explanability"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is\r\nfairness?\r\nWhy is fairness important?\r\nIf\r\nfairness is important, would it solve the problem by excluding the\r\nsensitive attributes?\r\nFairness Implementation\r\nDemonstration\r\nSetup the environment\r\nImport the\r\ndata\r\nBuild a model\r\nData\r\ncleaning\r\nFirst model - Random\r\nforest\r\nSecond &\r\nthird model - XGBoost & Logistic regression\r\n\r\nCombine all of them\r\ntogether\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Thirdman\r\nRecently I was watching this YouTube video by CrashCourse on “Algorithmic Bias and Fairness”,\r\nit has successfully piqued my curiosity on fairness in machine learning\r\ncontext.\r\nThis has led me to read more about fairness.\r\nWhat is fairness?\r\nSo, what is fairness?\r\n\r\n\r\n\r\nPhoto by Karolina\r\nGrabowska\r\nOh I am not talking about this type of “fairness”…\r\n(FAT/ML) defines fairness as “ensure that\r\nalgorithmic decisions do not create discriminatory or unjust impacts\r\nwhen comparing across different demographics (e.g. race, sex, etc)”.\r\nIn short, fairness looks at how machine learning models have treated\r\ndifferent groups. Typically the groups are defined by sensitive\r\nattributes, such as sex, race, nationality and so on.\r\nAccording to MAS Feat principles, there are two aspects within the\r\nfairness principles (MAS).\r\nJustifiability\r\nAccuracy and bias\r\nIn short, individuals or groups should not be disadvantaged unless\r\nthe decisions can be justified.\r\nThe algorithm and decisions should be regularly reviewed and minimize\r\nthe unintentional bias.\r\nYou may refer to this link\r\nfor more info.\r\nWhy is fairness important?\r\nAs simple as there are repercussions when there is unintended bias or\r\nfairness issue we make decisions based on the predictions from the\r\nmachine learning models.\r\nOne famous example is the algorithm that approves credit loan limits\r\nfor Apple credit cards.\r\nThere are reports that the algorithm Apple used in approving credit\r\ncard limits might have fairness issues. This has resulted in complaints\r\nand regulators stepping in to investigate this matter (BBC 2019).\r\nIf\r\nfairness is important, would it solve the problem by excluding the\r\nsensitive attributes?\r\nIt would not be sufficient to just exclude sensitive attributes\r\nduring the analysis.\r\nThe non-sensitive variables might have some correlations with the\r\nsensitive attributes.\r\nFor example, in some countries, zip codes can act as a proxy for\r\nrace.\r\nFairness Implementation\r\nIn a paper by MIT D lab, the authors suggested implementing several\r\nchecks (i.e. the green boxes in the screenshot below) along the project\r\ncycle to consider “fairness” in the machine learning project (MIT D-Lab, Comprehensive Initiative on Technology\r\nEvaluation, Massachusetts Institute of Technology 2020).\r\n\r\n\r\n\r\nOver here, I will focus on fairness checks when building machine\r\nlearning models.\r\nTo do so, I will be using fairness_check function in\r\nfairness package to assist me in checking the fairness in\r\nthe machine learning model built.\r\nBelow are the fairness measurements produced by\r\nfairness_check function:\r\nAccuracy equality ratio\r\nEqual opportunity ratio\r\nPredictive equality ratio\r\nPredictive parity ratio\r\nStatistical parity ratio\r\n\r\n\r\nFairness Measurement\r\n\r\n\r\nMeasures\r\n\r\n\r\nRemarks\r\n\r\n\r\nAccuracy equality ratio\r\n\r\n\r\nBoth protected and unprotected groups have equal prediction accuracy\r\n\r\n\r\n\r\n\r\nEqual opportunity ratio\r\n\r\n\r\nThe protected and unprotected groups have same false negative rate\r\n\r\n\r\nA.k.a. ‘false negative error rate balance’\r\n\r\n\r\nPredictive equality ratio\r\n\r\n\r\nBoth protected and unprotected groups have same false positive rate\r\n\r\n\r\nA.k.a. ‘false positive error rate balance’\r\n\r\n\r\nPredictive parity ratio\r\n\r\n\r\nBoth protected and unprotected groups have the same precisions\r\n\r\n\r\nA.k.a. ‘outcome test’\r\n\r\n\r\nStatistical parity ratio\r\n\r\n\r\nBoth protected and unprotected groups have the same probabilities to be\r\nassigned to positive predicted class\r\n\r\n\r\nA.k.a. ‘demographic parity, acceptance rate parity and benchmarking’\r\n\r\n\r\nDemonstration\r\nIn this demonstration, I will be using Spaceship\r\nTitanic dataset from Kaggle.\r\nNevertheless, let’s begin the demonstration!\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npacman::p_load(tidyverse, readr, tidymodels, janitor, DALEXtra, fairmodels)\r\n\r\n\r\n\r\nI will be using fairmodels package to calculate the\r\ndifferent fairness metrics discussed earlier.\r\nThe beauty of this package is it could “work” directly with\r\nexplainer from DALEXtra package and\r\ntidymodels without requiring many transformation.\r\nSo, the following are the steps to check the fairness of the\r\nmodels:\r\nFirst, build the model(s)\r\nSecond, create the explainer(s) by using DALEX or\r\nDALEXtra package\r\nThird, pass the explainer(s) into fairness_check\r\nfunction from fairmodels package\r\nImport the data\r\nFirst I will import the data into the environment.\r\n\r\n\r\ndf_org <- read_csv(\"data/train.csv\")\r\n\r\n\r\n\r\nI will set the random seed for reproducibility.\r\n\r\n\r\nset.seed(1234)\r\n\r\n\r\n\r\n\r\n\r\n\r\nBuild a model\r\nNext, I will start building the models.\r\nAs the purpose of this post is to explore how to implement the\r\nfairness measurements, hence I won’t be focusing on how to make the\r\nmodels more accurate.\r\nData cleaning\r\nI will perform some basic cleaning before building the different\r\nmachine learning models.\r\n\r\n\r\ndf <- df_org %>%\r\n  clean_names() %>%\r\n  mutate(transported = as_factor(transported),\r\n         cryo_sleep = as_factor(cryo_sleep), \r\n         vip = as_factor(vip)) %>%\r\n  drop_na() %>%\r\n  mutate(cabin_deck = str_sub(cabin, 1, 1),\r\n         cabin_num = str_sub(cabin, 3, 3),\r\n         cabin_side = str_sub(cabin, -1, -1)) %>%\r\n  select(-c(passenger_id, name, cabin))\r\n\r\n\r\n\r\nFollowing are the data wrangling and cleaning I have performed\r\nabove:\r\nClean the column names by using clean_names from\r\njanitor so that all of the column names are in small\r\nletters\r\nMutate all the logical columns to factor columns\r\nDrop all the rows with missing values\r\nSplit the cabin columns into cabin_deck, cabin_num &\r\ncabin_side based on the descriptions stated on the Kaggle\r\nwebsite\r\nDrop passenger ID, name and cabin columns\r\nOkay, now the data is ready, let’s start the demonstration!\r\nFirst model - Random forest\r\nThe first model I will be building is a random forest model.\r\n\r\n\r\n# model recipe\r\nranger_recipe <- recipe(formula = transported ~ .,\r\n                        data = df) %>%\r\n  step_dummy(all_nominal_predictors())\r\n\r\n# model specification\r\nranger_spec <- \r\n  rand_forest(trees = 1000) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\")\r\n\r\n# model workflow \r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(ranger_recipe) %>% \r\n  add_model(ranger_spec)\r\n\r\n# fitting the model\r\nranger_fit <- ranger_workflow %>%\r\n  fit(data = df)\r\n\r\n\r\n\r\nOnce the model is built, I will proceed and create the explainer\r\nobject.\r\nBefore that, as explainer requires the target variable to be in\r\nnumeric form, so I will convert the target variable into numeric\r\nform.\r\n\r\n\r\ny_numeric <- df %>%\r\n  mutate(transported_numeric = case_when(transported == TRUE ~ 1,\r\n                                       TRUE ~ 0)) %>%\r\n  select(transported_numeric)\r\n\r\n\r\n\r\nOnce that is done, I will create the explainer object.\r\n\r\n\r\nranger_explainer <- explain_tidymodels(ranger_fit,\r\n                                       data = select(df, -transported),\r\n                                       y = y_numeric,\r\n                                       label = \"randomForest\", \r\n                                       verbose = FALSE)\r\n\r\n\r\n\r\nNext, I will pass the explainer object into the\r\nfairness_check function.\r\nI will first define the protected variable (or the sensitive\r\nattribute). Over here, I will use the home_planet as the\r\nsensitive attribute for this demonstration.\r\nEarth within the home_planet will be taken\r\nas the privileged group.\r\n\r\n\r\nprotected_var <- df$home_planet\r\nprivileged_subgrp <- \"Earth\"\r\n\r\n\r\n\r\nAccording to the descriptions on the documentation\r\npage, the subgroup parity loss will be calculated with regard to the\r\nprivileged subgroup.\r\n\r\n\r\nranger_fair <- fairness_check(ranger_explainer,\r\n                              protected = protected_var,\r\n                              privileged = privileged_subgrp,\r\n                              colorize = TRUE) \r\n\r\n\r\nCreating fairness classification object\r\n-> Privileged subgroup      : character ([32m Ok [39m )\r\n-> Protected variable       : factor ([33m changed from character [39m )\r\n-> Cutoff values for explainers : 0.5 ( for all subgroups ) \r\n-> Fairness objects     : 0 objects \r\n-> Checking explainers      : 1 in total ( [32m compatible [39m )\r\n-> Metric calculation       : 13/13 metrics calculated for all models\r\n[32m Fairness object created succesfully [39m \r\n\r\nOnce the fairness_object is created, there will be a\r\nresult log.\r\nFrom the result log, we can see how many explainer objects were\r\npassed into fairness_check function.\r\nThe result log will also inform the users whether all the different\r\nmetric calculations are being computed successfully.\r\nTo extract the values of the computed metrics, we can call the\r\nparity_loss_metric_data from the\r\nfairness_object created as shown below.\r\n\r\n\r\nranger_fair$parity_loss_metric_data\r\n\r\n\r\n        TPR       TNR       PPV        NPV      FNR      FPR      FDR\r\n1 0.1454439 0.3417678 0.4406022 0.08489166 2.475674 3.914536 4.944065\r\n       FOR        TS       STP       ACC        F1 NEW_METRIC\r\n1 1.407863 0.5386661 0.3350889 0.2540447 0.2961478   2.621118\r\n\r\nAccording to the this\r\npost, if the metric is zero, NaN will be shown for the metrics so\r\nthat false information will not be shown.\r\n“A picture is worth a thousand words.”\r\nTo help us better understand the results, we can pass the\r\nfairness_object into the plot function to\r\nvisualize it.\r\n\r\n\r\nplot(ranger_fair)\r\n\r\n\r\n\r\n\r\nAccording to the documentation\r\npage, the red color areas show whether the selected metrics have\r\nexceeded the fairness thresholds.\r\nIf the bars reach the red area on the left, this implies there is a\r\nbias towards the unprivileged subgroups.\r\nOn the other hand, if the bars reach the red area on the right, this\r\nimplies there is a bias towards the privileged groups.\r\nHmmm, it seems Mars and Europa have a lower predictive equality\r\nratio.\r\nThis implies they have a lower false positive rate (i.e. less chance\r\nthat a data point that doesn’t transport incorrectly classify as “will\r\nbe transporting”).\r\nTo further check on this, we could also pass the\r\nfairness_object into metric_scores function\r\nand extract the calculated false positive rate under each subgroup.\r\nAfter that, I pass the results into ggplot function as\r\nshown below.\r\n\r\n\r\nmetric_scores(ranger_fair, fairness_metrics = c(\"FPR\"))$metric_scores_data %>%\r\n  ggplot(aes(x = subgroup, y = score)) +\r\n  geom_col() +\r\n  theme_minimal() +\r\n  labs(title = \"False Positive Rate under Each Subgroup of Home Planet\")\r\n\r\n\r\n\r\n\r\nIn this post, I won’t be exploring how we could potentially fix the\r\nobserved fairness issues in the dataset.\r\nAlso, according to the documentation\r\npage, the default acceptable ratio of metrics between unprivileged\r\nand privileged subgroups is set at 0.8.\r\nWe can change this acceptable ratio value by passing the value into\r\nepsilon argument.\r\n\r\n\r\nranger_fair_0.6 <- fairness_check(ranger_explainer,\r\n                              protected = protected_var,\r\n                              privileged = privileged_subgrp,\r\n                              epsilon = 0.6,\r\n                              colorize = FALSE) \r\n\r\n\r\nCreating fairness classification object\r\n-> Privileged subgroup      : character ([32m Ok [39m )\r\n-> Protected variable       : factor ([33m changed from character [39m )\r\n-> Cutoff values for explainers : 0.5 ( for all subgroups ) \r\n-> Fairness objects     : 0 objects \r\n-> Checking explainers      : 1 in total ( [32m compatible [39m )\r\n-> Metric calculation       : 13/13 metrics calculated for all models\r\n Fairness object created succesfully  \r\n\r\nplot(ranger_fair_0.6)\r\n\r\n\r\n\r\n\r\nAs shown in the graph above, with the updated acceptable ratio, the\r\nstatistical parity ratio for Europa is within the acceptable range\r\nnow.\r\nSecond &\r\nthird model - XGBoost & Logistic regression\r\nOne cool thing about fairness_check function is it\r\nallows the users to include multiple explainers into the function to\r\ncompare the results.\r\nOnce that is done, I will build the second and third model for the\r\nfairness comparison later.\r\nSecond model\r\n\r\n\r\n# model recipe\r\nxgboost_recipe <- \r\n  recipe(formula = transported ~ ., \r\n         data = df) %>%\r\n  step_dummy(all_nominal_predictors())\r\n\r\n# model specification\r\nxgboost_spec <- \r\n  boost_tree() %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"xgboost\")\r\n\r\n# model workflow \r\nxgboost_workflow <- \r\n  workflow() %>% \r\n  add_recipe(xgboost_recipe) %>% \r\n  add_model(xgboost_spec) \r\n\r\n# fitting the model\r\nxgboost_fit <- xgboost_workflow %>%\r\n  fit(data = df)\r\n\r\n\r\n[12:41:43] WARNING: amalgamation/../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\r\n\r\n# create explainer\r\nxgboost_explainer <- explain_tidymodels(xgboost_fit,\r\n                   data = select(df, -transported),\r\n                   y = y_numeric,\r\n                   label = \"xgboost\",\r\n                   verbose = FALSE)\r\n\r\n\r\n\r\nThird model\r\n\r\n\r\n# model recipe\r\nlogit_recipe <- \r\n  recipe(formula = transported ~ ., \r\n         data = df) %>%\r\n  step_dummy(all_nominal_predictors())\r\n\r\n# model specification\r\nlogit_spec <- \r\n  logistic_reg(penalty = 0.1) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"glmnet\")\r\n\r\n# model workflow \r\nlogit_workflow <- \r\n  workflow() %>% \r\n  add_recipe(logit_recipe) %>% \r\n  add_model(logit_spec) \r\n\r\n# fitting the model\r\nlogit_fit <- logit_workflow %>%\r\n  fit(data = df)\r\n\r\n# create explainer\r\nlogit_explainer <- explain_tidymodels(logit_fit,\r\n                   data = select(df, -transported),\r\n                   y = y_numeric,\r\n                   label = \"logistic\",\r\n                   verbose = FALSE)\r\n\r\n\r\n\r\nCombine all of them together\r\nOnce the models are built, I will pass the different\r\nexplainers into fairness_check function as\r\nshown below.\r\n\r\n\r\nall_fair <- fairness_check(ranger_explainer,\r\n                           xgboost_explainer,\r\n                           logit_explainer,\r\n                           protected = protected_var,\r\n                           privileged = privileged_subgrp,\r\n                           colorize = FALSE) \r\n\r\n\r\nCreating fairness classification object\r\n-> Privileged subgroup      : character ([32m Ok [39m )\r\n-> Protected variable       : factor ([33m changed from character [39m )\r\n-> Cutoff values for explainers : 0.5 ( for all subgroups ) \r\n-> Fairness objects     : 0 objects \r\n-> Checking explainers      : 3 in total ( [32m compatible [39m )\r\n-> Metric calculation       : 13/13 metrics calculated for all models\r\n Fairness object created succesfully  \r\n\r\nAs shown in the results, we can see the value under\r\nchecking explainers is shown as 3 now.\r\nThen, I will pass the created fairness_object to the\r\nplot function.\r\n\r\n\r\nplot(all_fair)\r\n\r\n\r\n\r\n\r\nThe results of the different models will be plotted together so that\r\nits easier to compare.\r\nAlternatively, we can plot the results in a radar graph format as\r\nshown below.\r\n\r\n\r\nplot(fairness_radar(all_fair))\r\n\r\n\r\n\r\n\r\nAnother useful function in fairmodels package is\r\nperformance_and_fairness.\r\nThis function allows users to compare the different models under the\r\nselected performance and metrics.\r\nThe best model is located in the top right corner.\r\nTaking accuracy and statistical parity ratio as an example,\r\n\r\n\r\nplot(performance_and_fairness(all_fair, fairness_metric = \"STP\"))\r\n\r\n\r\nPerformace metric is NULL, setting deafult ( accuracy )  \r\n\r\nCreating object with: \r\nFairness metric: STP \r\nPerformance metric: accuracy \r\n\r\n\r\nNevertheless, these are just some of the functions supported by\r\nfairmodels package. Do check out their documentation\r\npage for more details on the different functions.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nWhile reading through the materials for fairness, it reminded me on\r\nhow I debated with my colleagues and bosses on one of the work I did in\r\nthe past, i.e. equitable in participating fund management.\r\nI will leave what causes the biases and how we could fix these biases\r\nin my future post.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by destiawan\r\nnur agustra\r\n\r\n\r\n\r\nBBC. 2019. “Apple’s ’Sexist’ Credit Card Investigated by US\r\nRegulator.” https://www.bbc.com/news/business-50365609.\r\n\r\n\r\nFAT/ML. “Principles for Accountable Algorithms and a Social Impact\r\nStatement for Algorithms.” https://www.fatml.org/resources/principles-for-accountable-algorithms.\r\n\r\n\r\nMAS. “Principles to Promote Fairness, Ethics, Accountability and\r\nTransparency (FEAT) in the Use of Artificial Intelligence and Data\r\nAnalytics in Singapore’s Financial Sector.” https://www.mas.gov.sg/~/media/MAS/News%20and%20Publications/Monographs%20and%20Information%20Papers/FEAT%20Principles%20Final.pdf.\r\n\r\n\r\nMIT D-Lab, Comprehensive Initiative on Technology Evaluation,\r\nMassachusetts Institute of Technology. 2020. “Exploring Fairness\r\nin Machine Learning for International Development.” https://d-lab.mit.edu/sites/default/files/inline-files/Exploring_fairness_in_machine_learning_for_international_development_03242020_pages_0.pdf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-06-fairness/image/honest.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-09-bayes_tune/",
    "title": "Bayes Optimisation",
    "description": "When the prior info is put to good use",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-07-09",
    "categories": [
      "Machine Learning",
      "Hyperparameter Tuning"
    ],
    "contents": "\r\n\r\nContents\r\nRecap on Hyperparameters\r\ntuning\r\nOptimization Methods\r\nRegular\r\nGrid Search\r\nBayes\r\nOptimization\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the\r\ndata\r\nBuild a\r\nmodel\r\nCreating\r\nGrid\r\nTuning hyperparameters\r\nRegular Grid Search\r\nBayes\r\nOptimization\r\nParameters\r\nTuning Hyperparameters\r\n\r\n\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring bayes optimization.\r\nThis post is the continuation of my previous post on hyperparameter\r\ntuning.\r\n\r\n\r\n\r\nPhoto by Anne Nygard on Unsplash\r\nRecap on Hyperparameters\r\ntuning\r\nAs discussed in my previous post, the hyperparameters tuning task\r\nincludes the following 4 parts (Koehrsen 2018):\r\nObjective function: a function that takes in\r\nhyperparameters and returns a score we are trying to minimize or\r\nmaximize\r\nDomain: the set of hyperparameter values over\r\nwhich we want to search\r\nAlgorithm: method for selecting the next set of\r\nhyperparameters to evaluate in the objective function\r\nResults history: data structure containing each\r\nset of hyperparameters and the resulting score from the objective\r\nfunction\r\nIn this post, I will be exploring bayes optimization (i.e. one of the\r\nalgorithm).\r\nMeanwhile, I will perform regular grid search so that I can compare\r\nhow bayes optimization is different from regular grid search.\r\nOptimization Methods\r\nRegular Grid Search\r\nThis is one of the common hyperparameter searches one would learn\r\nwhen he/she embarks on data science journey.\r\nUnder this method, all the combinations created in the grid search\r\ndomain will be run to obtain the model results.\r\nOnce this is done, the model results under different hyperparameter\r\nsets are compared to obtain the best hyperparameter sets that provide\r\nthe best model performance.\r\nNote that this is an uninformed searching method. The different model\r\ntuning results in each iteration do not affect each other.\r\nThe issue with such an approach is the algorithm could be spending\r\nunnecessary time testing the hyperparameter regions that are likely to\r\nbe not accurate based on the previous few attempts.\r\nBayes Optimization\r\nBayes optimization is a sequential method that uses a model to\r\npredict new candidate parameters for assessment (RStudio).\r\nThe basic idea is to spend a little more time selecting the next\r\nhyperparameters to make fewer calls to the objective function (Koehrsen 2018).\r\nBayesian optimization approaches this task through a method known as\r\nsurrogate optimization, where surrogate function is an approximation of\r\nthe objective function (Ye 2020).\r\nBelow are how the bayes optimization works (Ye\r\n2020):\r\nInitialize a Gaussian Process ‘surrogate function’ prior\r\ndistribution.\r\nChoose several data points x such that the acquisition function\r\na(x) operating on the current prior distribution is maximized.\r\nEvaluate the data points x in the objective cost function c(x)\r\nand obtain the results, y.\r\nUpdate the Gaussian Process prior distribution with the new data\r\nto produce a posterior (which will become the prior in the next\r\nstep).\r\nRepeat steps 2–5 for several iterations.\r\nInterpret the current Gaussian Process distribution (which is\r\nvery cheap to do) to find the global minima.\r\nLater in this post, I will be showing the created grids under\r\ndifferent methods.\r\nDemonstration\r\nIn this demonstration, I will be using the employee\r\nattrition dataset from Kaggle.\r\nNevertheless, let’s begin the demonstration!\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'tidymodels', 'themis', 'doParallel',\r\n              'tictoc')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nFor this demonstration, we will be using an R package called\r\ntune to tune the hyperparameters.\r\ntune package supports both regular grid search method\r\nand bayes optimization.\r\nImport the data\r\nFirst I will import the data into the environment.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-03-12-marketbasket/data/general_data.csv\") %>%\r\n  select(-c(EmployeeCount, StandardHours, EmployeeID))\r\n\r\n\r\n\r\nI will set the random seed for reproducibility.\r\n\r\n\r\nset.seed(1234)\r\n\r\n\r\n\r\nBuild a model\r\nFor simplicity, I will reuse the random forest model building code I\r\nwrote in my previous post.\r\nYou can refer to my previous post for\r\nthe explanations of the model building.\r\n\r\n\r\ndf_split <- initial_split(df, \r\n                          prop = 0.6, \r\n                          strata = Attrition)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\ndf_folds <- vfold_cv(df_train, strata = Attrition)\r\n\r\n\r\n\r\n\r\n\r\nranger_recipe <- \r\n  recipe(formula = Attrition ~ ., \r\n         data = df_train) %>%\r\n  step_impute_mean(NumCompaniesWorked,\r\n                   TotalWorkingYears) %>%\r\n  step_nzv(all_predictors()) %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_upsample(Attrition)\r\n\r\nranger_spec <- \r\n  rand_forest(trees = tune(),\r\n              mtry = tune()) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\") \r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(ranger_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\n\r\n\r\n\r\n\r\n\r\nCreating Grid\r\nNext, I will start creating a grid for the regular grid search\r\nlater.\r\n\r\n\r\ndials_regular <- grid_regular(trees(c(1, 2000)),\r\n                              mtry(c(1, 21)),\r\n                              levels = 12)\r\n\r\n\r\n\r\nRefer to my previous\r\npost if you want to find more on different ways of creating the\r\ngrids for hyperparameter tuning.\r\nTuning hyperparameters\r\nOnce the values of hyperparameters to be tuned are created, we can\r\nstart tuning the hyperparameters.\r\nIn this post, I will be trying both regular grid\r\nsearch and bayes optimization in searching for\r\nthe best pairs of hyperparameters.\r\nRegular Grid Search\r\nIn this sub-section, I will perform regular grid search to find the\r\nbest combinations of hyperparameters.\r\nI will also measure the time taken to find the best\r\nhyperparameters.\r\n\r\n\r\ntic()\r\n\r\n\r\n\r\n\r\n\r\nranger_tune_regular <- tune_grid(ranger_workflow, \r\n                              resample = df_folds,\r\n                              metrics = metric_set(roc_auc),\r\n                              grid = dials_regular)\r\n\r\n\r\n\r\n\r\n\r\nresults_regular <- toc()\r\n\r\n\r\n2641.32 sec elapsed\r\n\r\ntune package also offers a function to allow the users\r\nto plot the model performance against the parameters.\r\n\r\n\r\nautoplot(ranger_tune_regular)\r\n\r\n\r\n\r\n\r\nAs shown in the graph, AUC tends to be low when the number of\r\nrandomly selected predictors and number of trees is low. AUC results are\r\nquite comparable when the number of randomly selected predictors and the\r\nnumber of trees increases.\r\nBayes Optimization\r\nNext, I will perform bayes optimization so that I can compare how\r\nthis optimization approach is different from the regular grid search\r\napproach.\r\nParameters\r\nTo perform Bayes optimization, we need to first define the\r\nhyperparameter value range.\r\n\r\n\r\nrf_param <- \r\n  ranger_workflow %>% \r\n  parameters() %>%\r\n  finalize(df_train)\r\n\r\n\r\n\r\nWe could check the value ranges by calling the object\r\nitem within the rf_param we have created earlier.\r\n\r\n\r\nrf_param$object\r\n\r\n\r\n[[1]]\r\n# Randomly Selected Predictors (quantitative)\r\nRange: [1, 21]\r\n\r\n[[2]]\r\n# Trees (quantitative)\r\nRange: [1, 2000]\r\n\r\nInstead of using the default values from the function, we can change\r\nthe value range by using update function.\r\n\r\n\r\nrf_param <- rf_param %>%\r\n  update(trees = trees(c(1, 5000)))\r\n\r\n\r\n\r\nAs you can see below, the value range of trees have updated.\r\n\r\n\r\nrf_param$object\r\n\r\n\r\n[[1]]\r\n# Randomly Selected Predictors (quantitative)\r\nRange: [1, 21]\r\n\r\n[[2]]\r\n# Trees (quantitative)\r\nRange: [1, 5000]\r\n\r\nTuning Hyperparameters\r\nOnce the parameter value ranges are defined, we can start performing\r\nbayes optimization.\r\n\r\n\r\ntic()\r\n\r\n\r\n\r\n\r\n\r\nranger_tune_bayes <- tune_bayes(ranger_workflow, \r\n                              resample = df_folds,\r\n                              param_info = rf_param,\r\n                              metrics = metric_set(roc_auc),\r\n                              initial = 5,\r\n                              iter = 8,\r\n                              control = control_bayes(no_improve = 5, \r\n                                                      verbose = FALSE, \r\n                                                      save_pred = TRUE))\r\n\r\n\r\n\r\nNote that the initial refers to how many set of\r\nhyperparameters should be tried before the iterations.\r\nAs recommended by the author on the documentation\r\npage, the number of initial results is suggested to be greater than\r\nthe number of parameters being optimized.\r\nSince I am tuning two hyperparameters, hence I have set\r\ninitial to be 5.\r\n\r\n\r\nresults_bayes <- toc()\r\n\r\n\r\n495.11 sec elapsed\r\n\r\nSimilarly, we can plot the model performance under different\r\nhyperparameters by using autoplot function.\r\n\r\n\r\nautoplot(ranger_tune_bayes)\r\n\r\n\r\n\r\n\r\nNote that the default value for type argument is\r\n“marginals”.\r\nAs we can observe from the graph above, the graph contains fewer dots\r\nthan the graph for a regular grid search. This is because this approach\r\nwill stop searching in the region where the hyperparameters do not\r\nprovide a more accurate result.\r\nFor example, as we can see in the graph, the model accuracy tends to\r\nbe low when the value for number of randomly selected predictors\r\n(i.e. mtry) is low regardless of the number of trees.\r\nThis would reduce the time taken to obtain the hyperparameter sets\r\nthat provide the best model results.\r\nOn the other hand, the traditional grid search method computes the\r\nmodel performance for every single hyperparameter defined in the created\r\ngrid as shown below, making this computation method very computation\r\nexpensive.\r\nTo illustrate this, I will first use collect_metrics\r\nfunction to gather the model performance and sort the iteration in\r\nascending order.\r\n\r\n\r\nranger_tune_bayes %>%\r\n  collect_metrics() %>%\r\n  arrange(.iter)\r\n\r\n\r\n# A tibble: 13 x 9\r\n    mtry trees .metric .estimator  mean     n std_err .config    .iter\r\n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      <int>\r\n 1     9  3160 roc_auc binary     0.965    10 0.00554 Preproces~     0\r\n 2    17    15 roc_auc binary     0.951    10 0.00778 Preproces~     0\r\n 3    19  4606 roc_auc binary     0.963    10 0.00549 Preproces~     0\r\n 4     3  1030 roc_auc binary     0.967    10 0.00561 Preproces~     0\r\n 5    10  2156 roc_auc binary     0.964    10 0.00547 Preproces~     0\r\n 6     5  2016 roc_auc binary     0.968    10 0.00553 Iter1          1\r\n 7     1  2087 roc_auc binary     0.924    10 0.00784 Iter2          2\r\n 8     4  1243 roc_auc binary     0.967    10 0.00534 Iter3          3\r\n 9     7  2009 roc_auc binary     0.966    10 0.00539 Iter4          4\r\n10     3   306 roc_auc binary     0.968    10 0.00529 Iter5          5\r\n11     6  4998 roc_auc binary     0.967    10 0.00549 Iter6          6\r\n12    21  4962 roc_auc binary     0.961    10 0.00545 Iter7          7\r\n13     6     5 roc_auc binary     0.934    10 0.00818 Iter8          8\r\n\r\nI will also collect the model performance under regular grid\r\nsearch.\r\n\r\n\r\nranger_tune_regular %>%\r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 144 x 8\r\n    mtry trees .metric .estimator  mean     n std_err .config         \r\n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \r\n 1     1     1 roc_auc binary     0.647    10 0.0216  Preprocessor1_M~\r\n 2     1   182 roc_auc binary     0.925    10 0.00853 Preprocessor1_M~\r\n 3     1   364 roc_auc binary     0.923    10 0.00874 Preprocessor1_M~\r\n 4     1   546 roc_auc binary     0.923    10 0.00830 Preprocessor1_M~\r\n 5     1   727 roc_auc binary     0.924    10 0.00835 Preprocessor1_M~\r\n 6     1   909 roc_auc binary     0.925    10 0.00771 Preprocessor1_M~\r\n 7     1  1091 roc_auc binary     0.925    10 0.00833 Preprocessor1_M~\r\n 8     1  1273 roc_auc binary     0.924    10 0.00783 Preprocessor1_M~\r\n 9     1  1454 roc_auc binary     0.924    10 0.00796 Preprocessor1_M~\r\n10     1  1636 roc_auc binary     0.926    10 0.00817 Preprocessor1_M~\r\n# ... with 134 more rows\r\n\r\nLet’s take a look at how the model performances differ between\r\nregular grid search and bayes optimization.\r\nThe model performance also looks quite similar between the regular\r\ngrid search method and bayes optimization method.\r\n\r\n\r\ntibble() %>%\r\n  bind_rows(show_best(ranger_tune_regular, n = 1) %>%\r\n              mutate(dials_method = \"Regular Grid Search\")) %>%\r\n  bind_rows(show_best(ranger_tune_bayes, n = 1) %>%\r\n              mutate(dials_method = \"Bayes Optimization\")) %>%\r\n  arrange(desc(mean)) %>%\r\n  select(c(dials_method, mean))\r\n\r\n\r\n# A tibble: 2 x 2\r\n  dials_method         mean\r\n  <chr>               <dbl>\r\n1 Regular Grid Search 0.968\r\n2 Bayes Optimization  0.968\r\n\r\nHowever, bayes optimization uses lesser time to obtain similar model\r\nperformance as compared to regular grid search.\r\nThere is about 81% reduction in time spent in searching the optimal\r\nhyperparameters when bayes optimization is used.\r\nThis shows how bayes optimization could spend less time searching the\r\noptimal hyperparameters.\r\nAlso, if we take a look at the model results from bayes optimization,\r\nthe results show that the algorithm didn’t spend much time in searching\r\nthe hyperparameters at the region where mtry value is low.\r\n\r\n\r\nranger_tune_bayes %>%\r\n  collect_metrics() %>%\r\n  arrange(.iter)\r\n\r\n\r\n# A tibble: 13 x 9\r\n    mtry trees .metric .estimator  mean     n std_err .config    .iter\r\n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>      <int>\r\n 1     9  3160 roc_auc binary     0.965    10 0.00554 Preproces~     0\r\n 2    17    15 roc_auc binary     0.951    10 0.00778 Preproces~     0\r\n 3    19  4606 roc_auc binary     0.963    10 0.00549 Preproces~     0\r\n 4     3  1030 roc_auc binary     0.967    10 0.00561 Preproces~     0\r\n 5    10  2156 roc_auc binary     0.964    10 0.00547 Preproces~     0\r\n 6     5  2016 roc_auc binary     0.968    10 0.00553 Iter1          1\r\n 7     1  2087 roc_auc binary     0.924    10 0.00784 Iter2          2\r\n 8     4  1243 roc_auc binary     0.967    10 0.00534 Iter3          3\r\n 9     7  2009 roc_auc binary     0.966    10 0.00539 Iter4          4\r\n10     3   306 roc_auc binary     0.968    10 0.00529 Iter5          5\r\n11     6  4998 roc_auc binary     0.967    10 0.00549 Iter6          6\r\n12    21  4962 roc_auc binary     0.961    10 0.00545 Iter7          7\r\n13     6     5 roc_auc binary     0.934    10 0.00818 Iter8          8\r\n\r\nThe algorithm also tries 5 different sets of hyperparameters before\r\nthe algorithm iterates on the hyperparameters as the\r\ninitial is set to be 5 in the function.\r\nThe type argument in autoplot function also\r\nallows two other values, i.e. parameters and performance.\r\nWhen “parameters” is selected, the graph shows us how the\r\nhyperparameters change over iterations.\r\n\r\n\r\nautoplot(ranger_tune_bayes, type = \"parameters\")\r\n\r\n\r\n\r\n\r\nOn the contrary, if the “performance” is being passed into\r\ntype argument, the graph shows the model performance over\r\niterations.\r\n\r\n\r\nautoplot(ranger_tune_bayes, type = \"performance\")\r\n\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nIn this post, the demonstration barely scratched the surface of what\r\nwe could do with bayes optimization. There is so much more to bayes\r\noptimization.\r\nI will leave the remaining functionality of bayes optimization for\r\nthe next post.\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Alexis Baydoun on Unsplash\r\n\r\n\r\n\r\nKoehrsen, Will. 2018. “A Conceptual Explanation of Bayesian\r\nHyperparameter Optimization for Machine Learning.” https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f.\r\n\r\n\r\nRStudio. “Iterative Bayesian Optimization of a Classification\r\nModel.” https://www.tidymodels.org/learn/work/bayes-opt/.\r\n\r\n\r\nYe, Andre. 2020. “The Beauty of Bayesian Optimization, Explained\r\nin Simple Terms.” https://towardsdatascience.com/the-beauty-of-bayesian-optimization-explained-in-simple-terms-81f3ee13b10f.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-09-bayes_tune/image/button.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-05-grids/",
    "title": "Hyperparameters Tuning With Regular Search and Non-regular Search",
    "description": "Define the domain to search for the hyperparameter combination that gives the best model performance",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-06-05",
    "categories": [
      "Machine Learning",
      "Hyperparameter Tuning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat\r\nis the difference between parameters and hyperparameters?\r\nHyperparameters tuning\r\nDifferent methods in\r\ncreating grids\r\nRegular grid\r\nNon-regular\r\ngrid\r\nRandom grid\r\nLatin hypercube sampling\r\nMaximum entropy design\r\n\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the\r\ndata\r\nBuild a\r\nmodel\r\nCreate dials\r\nRegular\r\ngrid\r\nNon-regular grid\r\nRandom grid\r\nLatin hypercube sampling\r\nMaximum entropy design\r\nRandom\r\ngrid - with fewer combinations than regular grid\r\n\r\nVisualize the points\r\n\r\nTuning hyperparameters\r\nRegular\r\ngrid\r\nNon-regular grid\r\nRandom grid\r\nLatin hypercube sampling\r\nMaximum entropy design\r\nRandom\r\ngrid - with fewer combinations than regular grid\r\n\r\n\r\nCompare\r\nresults\r\n\r\nConclusion\r\n\r\nRecently, I was reading about hyperparameter tuning.\r\nI found out there are other methods for creating the list of possible\r\nhyperparameter values for hyperparameter tuning.\r\n\r\n\r\n\r\nPhoto by Denisse Leon on Unsplash\r\nBefore jumping into the discussion on hyperparameter tuning, let’s\r\nlook at the difference between parameters and hyperparameters.\r\nWhat\r\nis the difference between parameters and hyperparameters?\r\n(Brownlee 2019)\r\nexplained in one of his post that parameters are configuration variables\r\nthat are internal to the model and the values can be estimated from the\r\ndata.\r\nOn the other hand, hyperparameters are configuration that is external\r\nto the model and whose value cannot be estimated from data.\r\nHence, to find the set of hyperparameters that would give us the best\r\nmodel performance, we will perform hyperparameter tuning.\r\nHyperparameters tuning\r\nIn general, the hyperparameters tuning task includes the following 4\r\nparts (Koehrsen\r\n2018):\r\nObjective function: a function that takes in\r\nhyperparameters and returns a score we are trying to minimize or\r\nmaximize\r\nDomain: the set of hyperparameter values over\r\nwhich we want to search\r\nAlgorithm: method for selecting the next set of\r\nhyperparameters to evaluate in the objective function\r\nResults history: data structure containing each\r\nset of hyperparameters and the resulting score from the objective\r\nfunction\r\nIn this post, I will be focusing on how to create the hyperparameter\r\ndomain.\r\nDifferent methods in\r\ncreating grids\r\nThere are two methods for creating the hyperparameter grids for model\r\ntuning purposes, which are regular grid and non-regular grid.\r\nRegular grid\r\nThis is the typical approach to create the grids of tuning\r\nhyperparameters.\r\nIn this method, every combination of hyperparameters within the\r\nspecified hyperparameter ranges is created for later model tuning.\r\nThen, we will have to perform cross-validation on every set of\r\nhyperparameters defined in the grid to find the set of results that\r\nwould give us the best model performance.\r\nHowever, this is an extremely computationally expensive approach to\r\nsearching for the best hyperparameter combinations.\r\nNon-regular grid\r\nCurrently, tidymodels package allows different methods\r\nto create non-regular grids, including:\r\nRandom grid\r\nLatin hypercube sampling\r\nMaximum entropy design\r\nRandom grid\r\n(Koehrsen 2018)\r\nexplained that grid search spends too much time evaluating unpromising\r\nregions of the hyperparameter search space.\r\nThis is where a random grid comes in handy.\r\nThe random grid generates independent uniform random numbers across\r\nthe parameter ranges (Kuhn and Silge\r\n2022).\r\nAlso, the random grid can usually find a good combination of\r\nhyperparameters in fewer iterations.\r\nIn other words, with fewer iterations, the random grids can find\r\n“good enough” combinations of hyperparameters.\r\nHence, this approach is more efficient compared to grid search.\r\nLatin hypercube sampling\r\nThe idea of this sampling is interesting.\r\nI felt (Zach 2020) did a\r\ngreat job in explaining the concept in an easier way to understand.\r\nThe author explained that the idea is divide a given CDF into n\r\ndifferent regions and randomly choose one value from each region to\r\nobtain a sample of size n as shown below.\r\n\r\n\r\n\r\nLatin Hypercube Sampling\r\nIn short, this method allows us to obtain random samples that are\r\nmore reflective of the underlying distribution of the dataset.\r\nMaximum entropy design\r\nThis approach aims to maximize the expected change in information,\r\nwhich maximizes the entropy of the observed responses at the point in\r\nthe design (Santner, Williams, and Notz\r\n2018).\r\nLater in this post, I will be showing the created grids under\r\ndifferent methods.\r\nDemonstration\r\nIn this demonstration, I will be using the employee\r\nattrition dataset from Kaggle.\r\nNevertheless, let’s begin the demonstration!\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'tidymodels', 'themis', 'doParallel',\r\n              'ggpubr')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nFor this demonstration, we will be using an R package called\r\ndials to create the list of hyperparameter\r\ncombinations.\r\nAfter that, we will use tune package in\r\ntidymodels to find the combinations of hyperparameters that\r\nwould give us the best model performance.\r\nImport the data\r\nFirst I will import the data into the environment.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-03-12-marketbasket/data/general_data.csv\") %>%\r\n  select(-c(EmployeeCount, StandardHours, EmployeeID))\r\n\r\n\r\n\r\nI will set the random seed for reproducibility.\r\n\r\n\r\nset.seed(1234)\r\n\r\n\r\n\r\nBuild a model\r\nFor simplicity, I will reuse the random forest model building code I\r\nwrote in my previous post.\r\nYou can refer to my previous post for\r\nthe explanations of the model building.\r\n\r\n\r\ndf_split <- initial_split(df, \r\n                          prop = 0.6, \r\n                          strata = Attrition)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\ndf_folds <- vfold_cv(df_train, strata = Attrition)\r\n\r\n\r\n\r\n\r\n\r\nranger_recipe <- \r\n  recipe(formula = Attrition ~ ., \r\n         data = df_train) %>%\r\n  step_impute_mean(NumCompaniesWorked,\r\n                   TotalWorkingYears) %>%\r\n  step_nzv(all_predictors()) %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_upsample(Attrition)\r\n\r\nranger_spec <- \r\n  rand_forest(trees = tune(),\r\n              mtry = tune()) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\") \r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(ranger_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\n\r\n\r\n\r\n\r\n\r\nCreate dials\r\nNext, I will start creating the different grids by using different\r\nmethods.\r\nRegular grid\r\nI will use grid_regular function to create the regular\r\ngrid of tuning parameters.\r\nTo do so, I have included the hyperparameters to be included in the\r\ngrid.\r\n\r\n\r\ndials_regular <- grid_regular(trees(),\r\n                              mtry(c(1, 10)),\r\n                              levels = 7)\r\n\r\n\r\n\r\nNote that the argument size is not an argument for\r\ngrid_regular function. Hence, I have used the argument\r\nlevel instead.\r\nNon-regular grid\r\nNext, I will create the non-regular grids before plotting them to see\r\nhow the values of the selected hyperparameters are different compared\r\nwith one another.\r\nRandom grid\r\nTo create a random grid, I will use grid_random\r\nfunction.\r\nNote that I have indicated that I would like to have 49 randomly\r\nsampled points by indicating the size to be 49.\r\n\r\n\r\ndials_random <- grid_random(trees(),\r\n                            mtry(c(1, 10)),\r\n                            size = 49)\r\n\r\n\r\n\r\nLatin hypercube sampling\r\nThen, grid_latin_hypercube function is used to create\r\nthe grid.\r\n\r\n\r\ndials_latin <- grid_latin_hypercube(trees(),\r\n                                     mtry(c(1, 10)),\r\n                                     size = 49)\r\n\r\n\r\n\r\nMaximum entropy design\r\nLastly, I will use grid_max_entropy function to create\r\nthe non-regular grid.\r\n\r\n\r\ndials_entropy <- grid_max_entropy(trees(),\r\n                                     mtry(c(1, 10)),\r\n                                     size = 49)\r\n\r\n\r\n\r\nRandom\r\ngrid - with fewer combinations than regular grid\r\nTo illustrate how a random grid can obtain a combination of\r\nhyperparameters with model performance “close enough” compared to the\r\nmodel performance of a regular grid, I will create a smaller random grid\r\nwith only contains 24 combinations.\r\n\r\n\r\ndials_random_less <- grid_random(trees(),\r\n                            mtry(c(1, 10)),\r\n                            size = 24)\r\n\r\n\r\n\r\nVisualize the points\r\nNext, I will plot out the hyperparameter grids by using different\r\napproaches.\r\n\r\n\r\nggarrange(\r\n  ggplot(dials_regular, aes(x = trees, y = mtry)) +\r\n    geom_point() +\r\n    labs(title = \"Regular Grid\"),\r\n  ggplot(dials_random, aes(x = trees, y = mtry)) +\r\n    geom_point() +\r\n    labs(title = \"Random grid\"),\r\n  ggplot(dials_random_less, aes(x = trees, y = mtry)) +\r\n    geom_point() +\r\n    labs(title = \"Random grid - Fewer Combinations\"),\r\n  ggplot(dials_latin, aes(x = trees, y = mtry)) +\r\n    geom_point() +\r\n    labs(title = \"Latin Hypercube Sampling\"),\r\n  ggplot(dials_entropy, aes(x = trees, y = mtry)) +\r\n    geom_point() +\r\n    labs(title = \"Maximum Entropy Design\"),\r\n  ncol = 2,\r\n  nrow = 3\r\n)\r\n\r\n\r\n\r\n\r\nAs what we can see in the graphs above,\r\nRegular grid creates a grid of hyperparameters that are equally\r\nspaced out\r\nThe grids created under Latin hypercube sampling and maximum\r\nentropy are more spread out than random grid\r\nTuning hyperparameters\r\nOnce the values of hyperparameters to be tuned are created, we can\r\nstart tuning the hyperparameters.\r\nIn this post, I will be using the grid search method in searching for\r\nthe best pairs of hyperparameters. As such, I will be using\r\ntune_grid function.\r\nRegular grid\r\nTo tune the parameters, I will pass the following objects into the\r\nfunction:\r\nWorkflow object\r\nThe cross-validation datasets\r\nSelected model metrics\r\nGrids of tuning parameters created in the earlier step\r\nNevertheless, let’s start the hyperparameter tuning.\r\nFirst, I will pass the created hyperparameter grids into the tuning\r\nfunction.\r\nI will also indicate AUC should be used as the model metric during\r\nthe hyperparameter tuning.\r\n\r\n\r\nranger_tune_grid <- tune_grid(ranger_workflow, \r\n                              resample = df_folds,\r\n                              metrics = metric_set(roc_auc),\r\n                              grid = dials_regular)\r\n\r\n\r\n\r\nAlso, as stated on the documentation\r\npage, if no metric set is provided, following will be computed:\r\n\r\n\r\nModels\r\n\r\n\r\nDefault Metrics\r\n\r\n\r\nRegression\r\n\r\n\r\nRMSE & R squared\r\n\r\n\r\nClassification\r\n\r\n\r\nROC curve & accuracy\r\n\r\n\r\nWe could extract the model performance by passing the\r\ntune_results object into collect_metrics\r\nfunction.\r\nI have also arranged the model performance in descending manner.\r\n\r\n\r\nranger_tune_grid %>%\r\n  collect_metrics() %>%\r\n  arrange(desc(mean))\r\n\r\n\r\n# A tibble: 49 x 8\r\n    mtry trees .metric .estimator  mean     n std_err .config         \r\n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>           \r\n 1     4  1000 roc_auc binary     0.969    10 0.00540 Preprocessor1_M~\r\n 2     4  2000 roc_auc binary     0.968    10 0.00520 Preprocessor1_M~\r\n 3     4  1333 roc_auc binary     0.968    10 0.00518 Preprocessor1_M~\r\n 4     4   667 roc_auc binary     0.968    10 0.00546 Preprocessor1_M~\r\n 5     4  1666 roc_auc binary     0.967    10 0.00546 Preprocessor1_M~\r\n 6     5   667 roc_auc binary     0.967    10 0.00565 Preprocessor1_M~\r\n 7     5  1000 roc_auc binary     0.967    10 0.00544 Preprocessor1_M~\r\n 8     5  1333 roc_auc binary     0.967    10 0.00522 Preprocessor1_M~\r\n 9     4   334 roc_auc binary     0.967    10 0.00497 Preprocessor1_M~\r\n10     5  2000 roc_auc binary     0.967    10 0.00554 Preprocessor1_M~\r\n# ... with 39 more rows\r\n\r\nTo illustrate the model results under different grids, we can pass\r\nthe object into autoplot function.\r\n\r\n\r\nautoplot(ranger_tune_grid)\r\n\r\n\r\n\r\n\r\nAs shown in the result above, the AUC results for all trees seem\r\nquite similar, except when tree = 1.\r\nThe AUC results also increase when the number of randomly selected\r\npredictors.\r\nNon-regular grid\r\nRandom grid\r\nNext, I will perform hyperparameter tuning based on the random\r\ngrid.\r\n\r\n\r\nranger_tune_random <- tune_grid(ranger_workflow, \r\n                              resample = df_folds,\r\n                              metrics = metric_set(roc_auc),\r\n                              grid = dials_random)\r\n\r\n\r\n\r\nLatin hypercube sampling\r\nSimilarly, I will pass the list of hyperparameter values generated by\r\nusing the Latin hypercube sampling method into the tuning function.\r\n\r\n\r\nranger_tune_latin <- tune_grid(ranger_workflow, \r\n                              resample = df_folds,\r\n                              metrics = metric_set(roc_auc),\r\n                              grid = dials_latin)\r\n\r\n\r\n\r\nMaximum entropy design\r\nLastly, I will perform tuning by using the grid created by using\r\nmaximum entropy design.\r\n\r\n\r\nranger_tune_entropy <- tune_grid(ranger_workflow, \r\n                              resample = df_folds,\r\n                              metrics = metric_set(roc_auc),\r\n                              grid = dials_entropy)\r\n\r\n\r\n\r\nRandom\r\ngrid - with fewer combinations than regular grid\r\nI will also perform tuning on the smaller random search grids to\r\ncheck whether the model performance would be affected if we use a\r\nsmaller grid.\r\n\r\n\r\nranger_tune_random_less <- tune_grid(ranger_workflow, \r\n                              resample = df_folds,\r\n                              metrics = metric_set(roc_auc),\r\n                              grid = dials_random_less)\r\n\r\n\r\n\r\nCompare results\r\nNext, I will compare the model performance under the different grids\r\ncreated under different methods.\r\nTo do so, I will first create an empty tibble, bind the model\r\nperformances and sort the results in descending manner.\r\n\r\n\r\ntibble() %>%\r\n  bind_rows(show_best(ranger_tune_grid, n = 1) %>%\r\n              mutate(dials_method = \"Regular Grid\")) %>%\r\n  bind_rows(show_best(ranger_tune_random, n = 1) %>%\r\n              mutate(dials_method = \"Random grid\")) %>%\r\n  bind_rows(show_best(ranger_tune_random_less, n = 1) %>%\r\n              mutate(dials_method = \"Smaller random grid\")) %>%\r\n  bind_rows(show_best(ranger_tune_latin, n = 1) %>%\r\n              mutate(dials_method = \"Latin Hypercube Sampling\")) %>%\r\n  bind_rows(show_best(ranger_tune_entropy, n = 1) %>%\r\n              mutate(dials_method = \"Maximum Entropy Design\")) %>%\r\n  arrange(desc(mean)) %>%\r\n  select(-c(std_err, .config, .metric, .estimator))\r\n\r\n\r\n# A tibble: 5 x 5\r\n   mtry trees  mean     n dials_method            \r\n  <int> <int> <dbl> <int> <chr>                   \r\n1     4  1517 0.969    10 Maximum Entropy Design  \r\n2     4   900 0.969    10 Latin Hypercube Sampling\r\n3     4  1000 0.969    10 Regular Grid            \r\n4     4  1266 0.968    10 Smaller random grid     \r\n5     6  1951 0.968    10 Random grid             \r\n\r\nFrom the result above, it seems like the model performances are more\r\nor less similar under the different methods of creating dials.\r\nAlso, despite I have reduced the number of hyperparameter\r\ncombinations of the smaller random grid by half, the AUC results between\r\nthis random grid and the regular grid are rather similar.\r\nThis illustrates the point that a random grid can find a “close\r\nenough” set of hyperparameters with fewer combinations to search. This\r\nmakes this approach more computation efficient than regular grid\r\nsearch.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Andrea\r\nPiacquadio\r\n\r\n\r\n\r\nBrownlee, Jason. 2019. “What Is the Difference Between a Parameter\r\nand a Hyperparameter?” https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/.\r\n\r\n\r\nKoehrsen, Will. 2018. “Intro to Model Tuning: Grid and Random\r\nSearch.” https://www.kaggle.com/code/willkoehrsen/intro-to-model-tuning-grid-and-random-search/notebook.\r\n\r\n\r\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. https://www.tmwr.org/grid-search.html.\r\n\r\n\r\nSantner, Thomas J, Brian J Williams, and William I Notz. 2018. The\r\nDesign and Analysis of Computer Experiments.\r\n\r\n\r\nZach. 2020. “What Is Latin Hypercube Sampling?” https://www.statology.org/latin-hypercube-sampling/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-05-grids/image/soundboard.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-03-partial-dependence/",
    "title": "Partial Dependence Plot (PDP)",
    "description": "If all else being equal, what is the effect of the selected variable?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-05-07",
    "categories": [
      "Machine Learning",
      "Model Explanability"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is a global\r\nmodel-agnostic method?\r\nPartial\r\nDependence\r\nHow does partial dependence\r\nwork?\r\nPros and\r\ncons\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the\r\ndata\r\nBuild a\r\nmodel\r\nPartial Dependence Plot\r\nCreate explainer objects\r\nExplaining\r\nhow the different variables affect the predictions\r\n\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring another popular model\r\nexplainability method, i.e. partial dependence.\r\n\r\n\r\n\r\nPhoto by Oleg\r\nMagni\r\nPartial dependence plots (PDP) show the dependence between the target\r\nresponse and a set of input features of interest, marginalizing over the\r\nvalues of all other input features (the ‘complement’ features).\r\nIntuitively, we can interpret the partial dependence as the expected\r\ntarget response as a function of the input features of interest (Scikit-learn).\r\nBefore we jump into the partial dependence plot, let’s take a look\r\nwhat is a global model-agnostic method.\r\nWhat is a global\r\nmodel-agnostic method?\r\nIn my previous post, I have discussed how lime method\r\ncan explain the model prediction, where lime is a local\r\nmodel-agnostic method.\r\nThe local model-agnostic method explains individual predictions,\r\ni.e. how the different variables affect the individual predicted\r\noutcome.\r\nInstead of focusing on individual predictions, the global\r\nmodel-agnostic method focuses on explaining the overall model\r\npredictions.\r\nUnlike the LIME method, partial dependence is a global XAI method.\r\nThe global method gives a comprehensive explanation of the entire data\r\nset, describing the impact of the features(s) on the target variable in\r\nthe context of the overall data (Kim\r\n2021).\r\nPartial Dependence\r\nHow does partial dependence\r\nwork?\r\nBelow is how partial dependence works (Baeder, Brinkmann, and Xu 2021):\r\nFor each level i, of the selected feature (continuous variables\r\nare binned):\r\nFor all observations, modify the value of the selected feature to\r\ni\r\nUsing the modified observations and the existing model, predict\r\nthe response variable value for every observation\r\nCalculate the average predicted values for all\r\nobservations\r\n\r\nPlot the average predicted values for each level (y-axis) against\r\nthe feature levels (x-axis)\r\nPros and cons\r\nSome of the advantages of partial dependence are (Molnar 2022):\r\nComputation of partial dependence plots is intuitive\r\nEasy to interpret the graph\r\nEasy to implement\r\nUnfortunately, partial dependence also comes with limitations. Below\r\nare some of the limitations discussed in (Molnar 2022):\r\nThe author argued that omitting the distribution can be\r\nmisleading\r\nThis method assumes the variables are not correlated with one\r\nanother, which is unlikely to be true\r\nHeterogeneous effects might be hidden in the PD plot as the plot\r\nis only showing marginal effects\r\nThe author gave an example of how the marginal effects could have\r\naveraged out by the different variable values, which can be\r\nmisleading\r\n\r\nIn the demonstration below, I will show some of the limitations of\r\npartial dependence.\r\nDemonstration\r\nIn this demonstration, I will be using the employee\r\nattrition dataset from Kaggle.\r\nNevertheless, let’s begin the demonstration!\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'tidymodels', 'DALEXtra', 'themis', \r\n              'ingredients', 'corrplot')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nFor this demonstration, we will be using an R package called\r\ningredients. This package allows producing partial\r\ndependence plots with a few lines of code.\r\nAlso, this package is part of the model explainability tools\r\ndeveloped by MI^2 DataLab. This allows us to use other model\r\nexplainability tools without many changes to the codes.\r\nImport the data\r\nFirst I will import the data into the environment.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-03-12-marketbasket/data/general_data.csv\") %>%\r\n  select(-c(EmployeeCount, StandardHours, EmployeeID))\r\n\r\n\r\n\r\nI will set the random seed for reproducability.\r\n\r\n\r\nset.seed(1234)\r\n\r\n\r\n\r\nBuild a model\r\nFor simplicity, I will reuse the random forest model building code I\r\nwrote in my previous post so that we can focus this post on how we apply\r\nPDP to interpret the machine learning model results.\r\nYou can refer to my previous post on\r\nthe explanations of the model building.\r\n\r\n\r\ndf_split <- initial_split(df, \r\n                          prop = 0.6, \r\n                          strata = Attrition)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\nranger_recipe <- \r\n  recipe(formula = Attrition ~ ., \r\n         data = df_train) %>%\r\n  step_impute_mean(NumCompaniesWorked,\r\n                   TotalWorkingYears) %>%\r\n  step_nzv(all_predictors()) %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_upsample(Attrition)\r\n\r\nranger_spec <- \r\n  rand_forest(trees = 1000) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\") \r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(ranger_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\nranger_fit <- ranger_workflow %>%\r\n  fit(data = df_train)\r\n\r\n\r\n\r\nPartial Dependence Plot\r\nNow, we will start using partial dependence to explain our model\r\npredictions!\r\nCreate explainer objects\r\nSimilarly to last post, I will first create the explainer object by\r\nusing explain_tidymodels function.\r\n\r\n\r\nranger_explainer <- explain_tidymodels(ranger_fit,\r\n                   data = select(df_train, -Attrition),\r\n                   y = df_train$Attrition,\r\n                   verbose = FALSE)\r\n\r\n\r\n\r\nAside from that, we need the following codes to ensure the right\r\nexplainers are being used (Lendway).\r\n\r\n\r\nmodel_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer\r\npredict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer\r\n\r\n\r\n\r\nOtherwise, the subsequent codes will not be able to run.\r\nExplaining\r\nhow the different variables affect the predictions\r\nNext, we will start explaining how the different variables affect the\r\noutcome.\r\nTo do so, we just need to indicate the interested variable in the\r\npartial_dependence function as shown below.\r\n\r\n\r\npdp_ranger <- partial_dependence(ranger_explainer,\r\n                                 variables = c(\"YearsAtCompany\"))\r\n\r\n\r\n\r\n\r\n\r\ngraph <- plot(pdp_ranger)\r\ngraph\r\n\r\n\r\n\r\n\r\nAs shown in the graph above, below are we can observe from the graph\r\nabove:\r\nEmployees who just recently just joined the company are more\r\nlikely to resign\r\nThe likelihood of resignation drop significantly when the years\r\nof service increase and stay flat around year 8\r\nThe likelihood will then increase again at around 20 years\r\nThis could well be the employees have reached the retirement\r\nage\r\n\r\nAs the graph object is a ggplot object, that allows us to modify the\r\ngraph object by using ggplot functions.\r\n\r\n\r\nclass(graph)\r\n\r\n\r\n[1] \"gg\"     \"ggplot\"\r\n\r\nBelow I have made modifications to the graph by using\r\nggplot functions:\r\n\r\n\r\ngraph + \r\n  guides(color = \"none\") +\r\n  theme(plot.tag = element_blank()) +\r\n  labs(title = \"Partial Dependence Profile\", \r\n       subtitle = NULL) +\r\n  theme_light()\r\n\r\n\r\n\r\n\r\nAlternatively, we could indicate the list of partial dependence plots\r\nto be produced by indicating the variable types under\r\nvariable_type argument.\r\n\r\n\r\npdp_ranger_num <- partial_dependence(ranger_explainer,\r\n                                 variable_type = \"numerical\")\r\n\r\n\r\nplot(pdp_ranger_num)\r\n\r\n\r\n\r\n\r\nFrom the graphs above, we could observe the following:\r\nIt seems like the predictive powers for some variables are low\r\nsince their partial dependence plots are rather flat\r\nThere is a spike in the likelihood of resignation for employees\r\nwho have recently had a much higher percentage of salary hike, which\r\nworthwhile to further investigate the reasons\r\nIt also seems like employees who previously worked for more\r\ncompanies in the past are more likely to resign\r\nAs mentioned in the earlier section, one of the assumptions of\r\npartial dependence is the variables are not correlated with one\r\nanother.\r\nTo check this assumption, I will plot the correlation matrix of the\r\nnumeric variables by using the corrplot function.\r\n\r\n\r\ndf_num <- df %>%\r\n  select_if(is.numeric)\r\n\r\ncorrplot(cor(df_num, use=\"pairwise.complete.obs\"), \r\n         method = \"number\", \r\n         type = \"upper\", \r\n         tl.cex = 0.65, \r\n         number.cex = 0.65, \r\n         diag = FALSE)\r\n\r\n\r\n\r\n\r\nFrom the correlation chart above, it is clear that the variables are\r\nnot independent of one another, which in practice is quite unlikely the\r\npredictors are independent of one another.\r\n(Molnar 2022)\r\ndiscussed the issue of using PDP plot when the variables are not\r\nindependent of one another. When the variables are correlated, we create\r\ndata points in areas of the feature distribution where the actual\r\nprobability is very low.\r\nFor example, from the correlation matrix above, Age and\r\nTotalWorkingYears are positively correlated. This makes sense as in\r\ngeneral, we could expect older employees would have more working\r\nexperience.\r\nBut in the PDP calculation, as we permute the data points over\r\ndifferent combinations, we could have data points that might not make\r\nsense. For example, the algorithm could generate a profile with age = 20\r\nand total working years > 20.\r\nAs the partial dependence algorithm is unable to differentiate these\r\ndata points from the rest, these unlikely data points will be used in\r\nthe average feature effect curve in the partial dependence plot as\r\nwell.\r\nIn the future post, I will be exploring other methods that help us to\r\novercome this.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Jenna\r\nHamra\r\n\r\n\r\n\r\nBaeder, Larry, Peggy Brinkmann, and Eric Xu. 2021. Interpretable\r\nMachine Learning for Insurance. https://www.soa.org/resources/research-reports/2021/interpretable-machine-learning/.\r\n\r\n\r\nKim, Seungjun (Josh). 2021. “Explainable AI (XAI) Methods Part 1 —\r\nPartial Dependence Plot (PDP).” https://towardsdatascience.com/explainable-ai-xai-methods-part-1-partial-dependence-plot-pdp-349441901a3d.\r\n\r\n\r\nLendway, Lisa. “Interpretable Machine Learning: This Tutorial\r\nFocuses on Local Interpretation.” https://advanced-ds-in-r.netlify.app/posts/2021-03-31-imllocal/.\r\n\r\n\r\nMolnar, Christopher. 2022. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/pdp.html.\r\n\r\n\r\nScikit-learn. “4.1. Partial Dependence and Individual Conditional\r\nExpectation Plots.” https://scikit-learn.org/stable/modules/partial_dependence.html#mathematical-definition.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-03-partial-dependence/image/city.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-16-lime/",
    "title": "Local Interpretable Model-Agnostic Explanations (LIME)",
    "description": "Don't worry, you won't feel any sourness while using the method",
    "author": [
      {
        "name": "Jasper Lok",
        "url": "https://jasperlok.netlify.app/"
      }
    ],
    "date": "2022-04-16",
    "categories": [
      "Machine Learning",
      "Model Explanability"
    ],
    "contents": "\r\n\r\nContents\r\nLIME\r\nWhat is LIME?\r\nPros and\r\ncons\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the\r\ndata\r\nBuild a model\r\nSplit data\r\nRandom\r\nForest\r\n\r\nLocal\r\nInterpretable Model-agnostic Explanations (LIME)\r\nCreate explainer objects\r\nExplaining predictions\r\n\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring local interpretable model-agnostic\r\nexplanations (LIME), i.e. one of the model explainability methods.\r\n\r\n\r\n\r\nPhoto by\r\nVino\r\nLi on\r\nUnsplash\r\nThis technique was first introduced back in 2016. The title of the\r\npaper is rather interesting, i.e. “Why should I trust you?” Explaining\r\nthe predictions of any classifier.\r\nYou may refer to the paper under this link.\r\nLIME\r\nOne interesting point highlighted by the authors of LIME paper was\r\n“if the users do not trust a model or a prediction, they will not use\r\nit.” (Ribeiro, Singh, and\r\nGuestrin 2016).\r\nBased on my practical experience, this is so true.\r\nMost of the time the model metrics (eg. confusion matrix, accuracy,\r\netc) alone are not sufficient to give comfort to the users, especially\r\nthe non-technical users, comfort that they could trust the model\r\nresults.\r\nBelow are some of the questions I have encountered so far:\r\nHow do I know the model predictions are reasonable?\r\nAm I able to explain the model reach the predicted outcome to my\r\nstakeholders?\r\nIf yes, how could I explain the results in a layman’s manner?\r\n\r\nIf the models cannot be explained, it would be seen as a black\r\nbox.\r\nThe authors of the LIME paper also outlined 4 criteria for\r\nexplanations that must be satisfied (purva91 2021):\r\nThe explanations for the predictions should be understandable,\r\ni.e. interpretable by the target demographic.\r\nWe should be able to explain individual predictions. The authors\r\ncall this local fidelity\r\nThe method of explanation should apply to all models. This is\r\ntermed by the authors as the explanation being model-agnostic\r\nAlong with the individual predictions, the model should be\r\nexplainable in its entirety, i.e. global perspective should be\r\nconsidered\r\nBut fear not! There is research in helping data scientists in\r\nexplaining the model results.\r\nIn this post, I will be exploring LIME, i.e. one of the common\r\nmethods used in explaining the model results.\r\n\r\n\r\n\r\nPhoto by Govinda Valbuena from Pexels\r\nDon’t worry, you won’t feel any sourness while using the method in\r\nexplaining the predictions!\r\nWhat is LIME?\r\nLIME is a local surrogate model that is used to explain the\r\nindividual predictions of the black-box machine learning models (Molnar 2022).\r\nOne key assumption made in LIME method is that at the local scale,\r\nthe model can be approximated by a simple linear model (Molnar 2022).\r\nBelow is the graph shown:\r\n\r\n\r\n\r\nIn the graph, the black dotted line is the simple model that attempts\r\nto explain the predicted value of the selected model point (ie. the\r\nblack cross) from the complex classification model (ie. the orange and\r\ngreen area).\r\nPros and cons\r\nSome of the advantages of LIME method are (Biecek and Burzykowski 2021):\r\nLIME method is model agnostic, i.e. it does not imply any\r\nassumptions about the black-box structure\r\nThis method also offers an interpretable representation of the\r\ncomplex model\r\nThis method provides local fidelity, i.e. the explanations are\r\nlocally well-fitted to the black-box model\r\nAside from tabular data, it also works for text and picture\r\ndata.\r\nFollowing are some of the flaws of LIME method (Molnar 2022):\r\nAt the moment, there is no correct definition of the neighborhood\r\nwhen using LIME with tabular data\r\nThe author suggests checking whether explanations make sense when we\r\ntry the different kernel setting\r\n\r\nThe explanations of two close points could vary significantly,\r\nwhich could raise concern on how much we could trust the model\r\nexplainability results\r\nDemonstration\r\nIn this demonstration, I will be using the employee\r\nattrition dataset from Kaggle.\r\nNevertheless, let’s begin the demonstration!\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'tidymodels', 'DALEXtra', 'themis', \r\n              'lime')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nFor this demonstration, we will be using a R package called\r\nDALEXtra.\r\nThis package provides an interface to different implementations of\r\nLIME method. Refer to the documentation\r\npage for the list of LIME package this function\r\nsupports.\r\nAs this package is acting as a wrapper, so I will be importing the\r\nLIME package into the environment as well.\r\nImport the data\r\nFirst I will import the data into the environment.\r\n\r\n\r\ndf <- read_csv(\"https://raw.githubusercontent.com/jasperlok/my-blog/master/_posts/2022-03-12-marketbasket/data/general_data.csv\")\r\n\r\n\r\n\r\nI will set the random seed for reproducability.\r\n\r\n\r\nset.seed(1234)\r\n\r\n\r\n\r\nBuild a model\r\nFor simplicity, I will build a random forest and attempt to explain\r\nthe predicted results by using LIME method.\r\nSplit data\r\nNext, I will split the data into training and testing datasets.\r\n\r\n\r\ndf_split <- initial_split(df, \r\n                          prop = 0.6, \r\n                          strata = Attrition)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nRandom Forest\r\nFirst I will define the recipe for the machine learning models.\r\n\r\n\r\nranger_recipe <- \r\n  recipe(formula = Attrition ~ ., \r\n         data = df_train) %>%\r\n  step_impute_mean(NumCompaniesWorked,\r\n                   TotalWorkingYears) %>%\r\n  step_nzv(all_predictors()) %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_upsample(Attrition)\r\n\r\n\r\n\r\nIn the recipe above, I have done the following:\r\nImpute the missing values with the mean of the relevant\r\ncategories\r\nRemove any predictors with near-zero variance\r\nConvert all the nominal predictors to dummy variables\r\nAs the target variable is highly imbalanced, I have used\r\nupsampling to resolve the imbalanced issue in the target\r\nvariable\r\nNext, I will define the model specs for the machine learning I will\r\nbe building.\r\n\r\n\r\nranger_spec <- \r\n  rand_forest(trees = 1000) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\") \r\n\r\n\r\n\r\nNext, I will build the workflow for the model building.\r\n\r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(ranger_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\n\r\n\r\nFinally, I will start fitting the model.\r\n\r\n\r\nranger_fit <- ranger_workflow %>%\r\n  fit(data = df_train)\r\n\r\n\r\n\r\nLocal\r\nInterpretable Model-agnostic Explanations (LIME)\r\nNow, we will start using LIME to explain our model predictions!\r\nCreate explainer objects\r\nTo use DALEXtra package to use LIME method\r\nto explain predictions, we will use explain_tidymodels\r\nfunction to create the explainer object.\r\n\r\n\r\nranger_explainer <- explain_tidymodels(ranger_fit,\r\n                   data = select(df_train, -Attrition),\r\n                   y = df_train$Attrition,\r\n                   verbose = FALSE)\r\n\r\n\r\n\r\nAccording to the documentation,\r\nthis package also supports the models\r\nAside from that, we need the following codes to ensure the right\r\nexplainers are being used (Lendway).\r\n\r\n\r\nmodel_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer\r\npredict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer\r\n\r\n\r\n\r\nOtherwise, the subsequent codes will not be able to run.\r\nExplaining predictions\r\nIn this demonstration, I will attempt to explain the top 3 predicted\r\nattrition by using lime method.\r\nTo do so, I will use predict function to generate the\r\npredictions from testing dataset. I will also indicate the probability\r\nof attrition should be generated by indicating prob in the\r\ntype argument.\r\nThen, I will slice the data points with top 3 predicted attrition by\r\nusing slice_head function.\r\n\r\n\r\ntop_3_obs <- predict(ranger_fit, \r\n        df_test, \r\n        type = \"prob\") %>%\r\n  bind_cols(df_test) %>%\r\n  arrange(desc(.pred_Yes)) %>%\r\n  slice_head(n = 3)\r\n\r\ntop_3_obs\r\n\r\n\r\n# A tibble: 3 x 26\r\n  .pred_No .pred_Yes   Age Attrition BusinessTravel    Department     \r\n     <dbl>     <dbl> <dbl> <chr>     <chr>             <chr>          \r\n1  0.00594     0.994    31 Yes       Travel_Frequently Research & Dev~\r\n2  0.0128      0.987    31 Yes       Travel_Frequently Sales          \r\n3  0.0134      0.987    37 Yes       Travel_Frequently Sales          \r\n# ... with 20 more variables: DistanceFromHome <dbl>,\r\n#   Education <dbl>, EducationField <chr>, EmployeeCount <dbl>,\r\n#   EmployeeID <dbl>, Gender <chr>, JobLevel <dbl>, JobRole <chr>,\r\n#   MaritalStatus <chr>, MonthlyIncome <dbl>,\r\n#   NumCompaniesWorked <dbl>, Over18 <chr>, PercentSalaryHike <dbl>,\r\n#   StandardHours <dbl>, StockOptionLevel <dbl>,\r\n#   TotalWorkingYears <dbl>, TrainingTimesLastYear <dbl>, ...\r\n\r\nAs it seems like the new observation must be in the same data format\r\nand cannot contain additional columns, I will drop the two prediction\r\nfields.\r\n\r\n\r\ntop_3_obs <- top_3_obs %>%\r\n  select(-c(.pred_No, .pred_Yes))\r\n\r\n\r\n\r\nOkay, let’s start to explain the predictions!\r\nFor this, I will be using predict_surrogate function\r\nfrom DALEXtra package.\r\n\r\n\r\nlime_rf_top_3 <- predict_surrogate(explainer = ranger_explainer,\r\n                  new_observation = top_3_obs %>% \r\n                    select(-Attrition),\r\n                  n_features = 8,\r\n                  type = \"lime\")\r\n\r\n\r\n\r\nNote that we will need to remove the target variable from the data\r\nwhen running the predict_surrogate function, otherwise the\r\ncode will return an error (Lendway).\r\nOne cool thing to note is the output from the plot\r\nfunction above is an object is a ggplot object.\r\nThis would allow us to modify the graph by using the different ggplot\r\nrelated functions.\r\nFor example, I would like to change the color to grey and make the\r\nbar color a bit more transparent.\r\nTo do so, I will add on scale_fill_manual function to\r\nthe plot object as shown below to modify the ggplot graph.\r\n\r\n\r\nplot(lime_rf_top_3 %>% filter(case == 2)) +\r\n  labs(title = \"Before Modification\")\r\n\r\n\r\n\r\nplot(lime_rf_top_3 %>% filter(case == 2)) +\r\n  scale_fill_manual(values = alpha(c(\"grey\", \"black\"), 0.6)) +\r\n  labs(title = \"After Modification\")\r\n\r\n\r\n\r\n\r\nNevertheless, let’s start analyzing the results!\r\nAs the graph can be very cluttered, so I will use the for loop to\r\nplot the different observations separately.\r\n\r\n\r\nfor (i in 1:3){\r\n  print(\r\n    plot(lime_rf_top_3 %>% filter(case == i))\r\n    )\r\n}\r\n\r\n\r\n\r\n\r\nThe graphs above shows us the top variables that are important in the\r\nlocal model.\r\nThe graph also shows on each variable is “contributing” to the\r\nprediction. The higher the positive weight, the higher effect that the\r\nvariable has on the prediction.\r\nAside from the “explanation” breakdown of three predictions, the\r\ngraph also contains the predicted output and explanation fit.\r\nThe values under explanation fit indicate how well the LIME method\r\nexplains the prediction of the relevant data point (Adyatama 2019).\r\nFrom the graph, we can see that the explanation fit is rather poor\r\nsince the values range between 35-55%.\r\nThis implies the LIME model is only able to explain about 35-55% of\r\nour fitted model, which is not very ideal.\r\nTo resolve this, we can pass in additional arguments to tune the\r\nparameters in lime.\r\nRefer to the lime\r\ndocumentation page for the list of available arguments under\r\nlime package.\r\n\r\n\r\nlime_rf_top_3 <- predict_surrogate(explainer = ranger_explainer,\r\n                  new_observation = top_3_obs %>% \r\n                    select(-Attrition),\r\n                  n_features = 8,\r\n                  dist_fun = \"manhattan\",\r\n                  kernel_width = 2,\r\n                  type = \"lime\")\r\n\r\nfor (i in 1:3){\r\n  print(\r\n    plot(lime_rf_top_3 %>% filter(case == i))\r\n    )\r\n}\r\n\r\n\r\n\r\n\r\nAs shown above, the values on explanation fit increase after I have\r\nchanged the dist_fun to “manhattan” and\r\nkernel_width to 2.\r\nFrom the graph, it seems like the staffs with higher attrition have\r\nfollowing common characteristics:\r\nSingle, i.e. not yet married\r\nHave been working in the company less than 3 years\r\nAlternatively, lime package offers an option to plot a\r\ncondensed overview of all explanations. This would help identify the\r\ncommon features that influence the observations.\r\n\r\n\r\nplot_explanations(lime_rf_top_3)\r\n\r\n\r\n\r\n\r\nSimilarly to the graphs earlier, we can see that the top 3 employees\r\nare all single and works in the current company for less than 3\r\nyears.\r\nConclusion\r\nThat’s all for the day!\r\nThis post has demonstrated how we could use lime package\r\nin explaining the predictions.\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Wagner Soares from Pexels\r\n\r\n\r\n\r\nAdyatama, Arga. 2019. “Interpreting Classification Model with\r\nLIME.” https://algotech.netlify.app/blog/interpreting-classification-model-with-lime/.\r\n\r\n\r\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory\r\nModel Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/.\r\n\r\n\r\nLendway, Lisa. “Interpretable Machine Learning: This Tutorial\r\nFocuses on Local Interpretation.” https://advanced-ds-in-r.netlify.app/posts/2021-03-31-imllocal/.\r\n\r\n\r\nMolnar, Christoph. 2022. 9.2 Local Surrogate (LIME). https://christophm.github.io/interpretable-ml-book/lime.html.\r\n\r\n\r\npurva91. 2021. “ML Interpretability Using LIME in r.” https://www.analyticsvidhya.com/blog/2021/01/ml-interpretability-using-lime-in-r/.\r\n\r\n\r\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\r\n“‘Why Should i Trust You?’ Explaining the Predictions\r\nof Any Classifier.” https://arxiv.org/pdf/1602.04938.pdf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-16-lime/image/lime.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-12-marketbasket/",
    "title": "Association Rules",
    "description": "Revealing the hidden \"rules\" within the data",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2022-03-12",
    "categories": [
      "Unsupervised Learning",
      "Association Rules"
    ],
    "contents": "\r\n\r\nContents\r\nMarket Basket Analysis\r\nApplication\r\nfor Market Basket Analysis in Insurance\r\nSupport\r\nConfidence\r\nLift\r\n\r\nDemonstration\r\nSetup the environment\r\nImport the\r\ndata\r\nFind the association rules\r\nVisualisation\r\nStatic Graph\r\nInteractive\r\nGraph\r\nMethod 1: Use plot\r\nfunction\r\nMethod 2: Use\r\nvisNetwork function\r\n\r\n\r\n\r\nConclusion\r\n\r\nRecently while surfing the net, I happened to come across this\r\nemployee attrition dataset from Kaggle.\r\nThis has sparked my interest in understanding how we could perform\r\nrule mining to mine hidden relationships.\r\n\r\n\r\n\r\nPhoto by Joshua Miranda from Pexels\r\nMarket Basket Analysis\r\nIn my previous\r\npost on clustering, I have briefly touched on the definition of\r\nassociation.\r\nMarket basket analysis is used in discovering how the different\r\n“items” in the dataset are related to one another.\r\nThis is commonly used in the retail sector to help companies to\r\nidentify upsell and cross-sell opportunities.\r\nFor example, the common example used in illustrating this technique\r\nis how the sales of beer and the sales of diapers are correlated in the\r\nsupermarket (Swoyer 2016).\r\nThis has sparked the discussion on why the sales of these two different\r\nproducts are correlated.\r\nApplication\r\nfor Market Basket Analysis in Insurance\r\nMeanwhile, this technique can also be used in the insurance context\r\nas well.\r\nFor instance, we could apply this technique in detecting the common\r\ncharacteristics of high churners. It also can be used to understand the\r\nprofiles of customers with the high claim amounts.\r\nApart from that, this would allow the insures to formulate different\r\ncustomer strategies to target different customer segments.\r\nFor example, insurers could think of ways to win back their\r\nprofitable customers that have recently churned their policies or even\r\nstrategies to reduce the churn rate for their profitable segments.\r\nSimilar to clustering, it is always good to think about how we will\r\nbe using the results from the rule mining.\r\nBefore jumping into the analysis, let’s take a look at the definition\r\nof some of the key metrics under market basket analysis.\r\nSupport\r\nSupport refers to the fraction of transactions that contain both\r\nitems A and B.\r\nSupport tells us about the frequency of an item or a combination of\r\nitems bought (Ahamed 2021).\r\n\\(Support = \\frac{freq(A,\r\nB)}{N}\\)\r\nConfidence\r\nConfidence tells us how frequently items A and B are bought together\r\n(Ahamed 2021).\r\n\\(Confidence = \\frac{freq(A,\r\nB)}{freq(A)}\\)\r\nIn other words, confidence looks at the conditional probability of B\r\nbeing bought given that A is bought as well.\r\nLift\r\nLift measures the strength of association rules over the random\r\noccurrence of A and B (Ahamed 2021).\r\n\\(Lift =\r\n\\frac{Support(A,B)}{Support(A)*Support(B)}\\)\r\nIn other words, lift measures the strength of the “rules”. The higher\r\nthe lift, the stronger the association rules.\r\nNevertheless, let’s start with the demonstration of market basket\r\nanalysis!\r\nDemonstration\r\nIn this demonstration, I will be using a Kaggle\r\ndataset on the employee resignation dataset.\r\n\r\n\r\n\r\nPhoto by Anna Shvets from Pexels\r\nI will be using the following datasets to perform association rule\r\nmining:\r\nGeneral data: the main dataset which contains the info\r\non the employees, including whether the employees have resigned from the\r\ncompany\r\nEmployee survey data\r\nManager survey data\r\nSetup the environment\r\nFirst, I will call the relevant packages for the analysis later.\r\n\r\n\r\npackages = c('tidyverse', 'arules', 'arulesViz', 'igraph', 'visNetwork')\r\n\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n    }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\nI will be following R packages for the following purposes:\r\narules\r\npackage: Mine association rules in the data\r\narulesViz\r\npackage: Visualizing the aossication rules\r\nigraph\r\npackage: Visualize the network\r\nvisNetwork\r\npackage: Allow interactivity when visualizing the network\r\nImport the data\r\nNext, I will import the data into the environment.\r\n\r\n\r\ndf_main <- read_csv(\"data/general_data.csv\") %>%\r\n  select(-c(EmployeeCount,\r\n            Over18,\r\n            StandardHours))\r\n\r\n\r\n\r\nAs EmployeeCount, Over18, and StandardHours only contain one single\r\nvalue, I will remove these variables before mining the rules.\r\nAs we are not told whether the employee ID shown in the survey\r\nbelongs to the employees or managers, hence I will assume the employee\r\nID shown in both survey data belong to the employees.\r\n\r\n\r\ndf_employee <- read_csv(\"data/employee_survey_data.csv\") %>%\r\n  rename_with(~paste0(\"employee_\", .x)) %>%\r\n  rename(\"EmployeeID\" = \"employee_EmployeeID\")\r\n\r\ndf_manager <- read_csv(\"data/manager_survey_data.csv\") %>%\r\n  rename_with(~paste0(\"manager_\", .x)) %>%\r\n  rename(\"EmployeeID\" = \"manager_EmployeeID\")\r\n\r\n\r\n\r\nNext, I will join the different datasets together through\r\nleft_join junction.\r\n\r\n\r\ndf <- df_main %>%\r\n  left_join(df_employee, by = \"EmployeeID\") %>%\r\n  left_join(df_manager, by = \"EmployeeID\") \r\n\r\n\r\n\r\nI will also group some categories with relatively low counts under\r\nYearsWithCurrManager and TotalWorkingYears as separate categories.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(YearsWithCurrManager_recoded = case_when(YearsWithCurrManager > 7 ~ \"7+\",\r\n                                                  TRUE ~ as.character(YearsWithCurrManager)),\r\n         TotalWorkingYears_recoded = case_when(TotalWorkingYears > 15 ~ \"15+\",\r\n                                               TRUE ~ as.character(TotalWorkingYears))) %>%\r\n  select(-c(YearsWithCurrManager,\r\n            TotalWorkingYears))\r\n\r\n\r\n\r\nFind the association rules\r\nOnce the data is processed, we will proceed to mine the association\r\nrules within the dataset.\r\nOver here, I will be using apriori function to perform\r\nthe task.\r\n\r\n\r\nrules <- apriori(df_1,\r\n                 parameter = list(supp = 0.055,\r\n                                  confidence = 0.1,\r\n                                  target = \"rules\"),\r\n                 appearance = list(default = \"lhs\",\r\n                                   rhs=c(\"Attrition=Yes\",\"Attrition=No\")),\r\n                 control = list(verbose = F))\r\n\r\n\r\n\r\nIn the parameters, I have indicated the following:\r\nMin support should be 0.055\r\nMin confidence should be 0.1\r\nI would mine the rules from the data by indicating the target as\r\nrules\r\nAs I am also interested in how the different attributes of the\r\nemployees affect their attrition, I will perform what is known as\r\n‘target rules mining’ by specifying the rules I am interested in.\r\nAlso, in general, we are interested in the rules that have the\r\nrelatively stronger association. With that, I will filter out the rules\r\nwith lift below a specified amount.\r\n\r\n\r\nrules_sub <- subset(rules, lift > 1.16)\r\n\r\n\r\n\r\nThen I will sort the rules in a descending manner.\r\n\r\n\r\nrules_sub <- sort(rules_sub, by = \"lift\", decreasing = TRUE)\r\n\r\n\r\n\r\narulesViz package contains inspectDT\r\nfunction that allows the users to illustrate the rules in interactive\r\ndata manner.\r\n\r\n\r\ninspectDT(rules_sub)\r\n\r\n\r\n\r\n\r\nVisualisation\r\nTo make it easier to understand the rules, we could plot out the\r\nrules in graphic format.\r\nStatic Graph\r\nplot function from arulesViz package could\r\nbe used to illustrate the rules.\r\nAccording to the documentation\r\npage, the plot function supports different plotting\r\nengines, including ggplot, base and so on.\r\nIf we don’t indicate the engine, the default plotting engine will be\r\nggplot.\r\n\r\n\r\nstatic_graph_ggplot <- plot(rules_sub, \r\n                                method = \"graph\")\r\n\r\nstatic_graph_ggplot\r\n\r\n\r\n\r\n\r\nOver here, I will plot the rules by using igraph engine.\r\nThis would allow us to visualize the rules in a static plot.\r\n\r\n\r\nstatic_graph_igraph <- plot(rules_sub, \r\n                                method = \"graph\", \r\n                                engine = \"igraph\")\r\n\r\n\r\n\r\n\r\nInteractive Graph\r\nOne issue with the static graph is that graph becomes very cluttered\r\nwhen there are too many rules.\r\nTo resolve this, we could either reduce the number of rules to be\r\nillustrated or plot an interactive graph that allows us to zoom,\r\nhighlight or filter the graphs.\r\nTo plot the interactive graph, we could either use the\r\nplot function from arulesViz package or\r\nvisNetwork function from visNetwork\r\npackage.\r\nMethod 1: Use plot\r\nfunction\r\n\r\n\r\nstatic_graph_visNetwork <- plot(rules_sub, \r\n                                method = \"graph\", \r\n                                engine = \"visNetwork\",\r\n                                control = list(degree_highlight = 2))\r\n\r\nstatic_graph_visNetwork\r\n\r\n\r\n\r\n\r\nMethod 2: Use\r\nvisNetwork function\r\nThe interesting thing about visNetwork is that it allows\r\nus to pass additional arguments to further define the nodes and edges\r\n(eg. the value of the mode, the color of the edge etc).\r\nTo do so, I will convert the static igraph object into a list by\r\nusing as_data_frame function from igraph\r\npackage as shown below.\r\n\r\n\r\nint_graph <- as_data_frame(static_graph_igraph, \r\n                           what = \"both\")\r\n\r\n\r\n\r\nI have also specified ‘both’ under the what argument so that the\r\nobject would contain both edges and vertices.\r\n\r\n\r\nclass(int_graph)\r\n\r\n\r\n[1] \"list\"\r\n\r\nOnce that is done, we can pull the necessary info from the list and\r\nform a data frame by using data.frame function as shown\r\nbelow.\r\n\r\n\r\nnodes <- data.frame(id = int_graph$vertices$name,\r\n                    value = int_graph$vertices$lift,\r\n                    color.background = int_graph$vertices$support,\r\n                    int_graph$vertices)\r\n\r\nedges <- data.frame(int_graph$edges)\r\n\r\n\r\n\r\nThen, the created data frame can be passed into\r\nvisNetwork function.\r\n\r\n\r\nvisNetwork(nodes, edges) %>%\r\n  visEdges(arrows =\"to\") %>%\r\n  visNodes(color = list(background = int_graph$vertices$support)) %>%\r\n  visOptions(highlightNearest = list(enabled = TRUE, \r\n                                     degree = 2), \r\n             nodesIdSelection = TRUE)\r\n\r\n\r\n\r\n\r\nThe cool thing about such graphs is we could interact with the graph,\r\nwhich is something we couldn’t do on the static graph.\r\nIf we were to highlight the nodes associated with attrition = yes,\r\nbelow are the characteristics of employees with relatively higher\r\nattrition:\r\nThey tend to be younger, i.e. within the age of 18 - 32\r\nThe year of service in the company tend to be shorter as well,\r\ni.e. within 0 - 4 years in the company\r\nAs a result, the years under their current manager and the year\r\nsince the last promotion tends to be the shortest among all\r\n\r\nStrangely enough, this group of employees also have a rather good\r\nperformance rating from their manager since their performance rating is\r\nbetween 3 and 4\r\nAccording to the data dictionary, the performance ratings of 3 and 4\r\nrefer to excellent and outstanding\r\n\r\n\r\n\r\n\r\nWith the insights gathered, the HR staff could gather more insights\r\non the reasons for the relatively higher attrition of the younger\r\nstaff.\r\nThis would allow the company to have a more targeted approach to\r\nresolve the issues faced by different segments of employees, improving\r\nthe overall workplace staff retention.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\nkrakenimages\r\non\r\nUnsplash\r\n\r\n\r\n\r\nAhamed, Fazil. 2021. “Market-Basket Analysis.” https://medium.com/analytics-vidhya/market-basket-analysis-127c73f353d7.\r\n\r\n\r\nSwoyer, Steve. 2016. “Beer and Diapers: The Impossible\r\nCorrelation.” https://tdwi.org/articles/2016/11/15/beer-and-diapers-impossible-correlation.aspx.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-12-marketbasket/image/rules.jpg",
    "last_modified": "2022-12-27T22:23:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-12-point-pattern-analysis/",
    "title": "Point Pattern Analysis",
    "description": "How do you know whether the objects are randomly dispersed within an area?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2022-02-12",
    "categories": [
      "Geospatial Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nSpatial\r\nAnalysis\r\nPoint Pattern Analysis\r\nApplications of Point\r\nPattern Analysis\r\nComplete Spatial Randomness\r\nTypes of distributions\r\n\r\nG function\r\nDemonstration\r\nWhat is\r\na shapefile?\r\nData Source\r\nSetup the environment\r\nImport the\r\nData\r\nPlotting the\r\nMap\r\nStatistical\r\nTest on all the bus stops in the entire Singapore\r\nStatistical\r\nTest on all the bus stops in the selected area\r\n\r\nConclusion\r\n\r\nIn this sharing, I will be exploring point pattern analysis.\r\nThis is one of the spatial topics I didn’t manage to find out more\r\nabout when I was pursuing my master degree.\r\n\r\n\r\n\r\nPhoto by\r\nTimo\r\nWielink on\r\nUnsplash\r\nBefore jumping right into the technique, let’s take a look what is\r\nspatial analysis.\r\nSpatial Analysis\r\nSpatial analysis is a type of geographical analysis which seeks to\r\nexplain patterns of human behavior and its spatial expression in terms\r\nof mathematics and geometry, that is, locational analysis (Dartmouth College Library).\r\nPoint Pattern Analysis\r\nPoint pattern analysis is one of the fundamental concepts for spatial\r\nanalysis.\r\nSpatial Point Pattern Analysis is the evaluation of the pattern, or\r\ndistribution, of a set of points on a surface (Kam).\r\nIn other words, this technique studies how the items are distributed\r\nwithin the study area.\r\nApplications of Point\r\nPattern Analysis\r\nPoint pattern analysis can be used in many areas, including (Bivand, Pebesma, and Gómez-Rubio\r\n2013):\r\nEcology (eg. How are the different trees distributed? Are there\r\nare any competitions between the trees?)\r\nEpidemiology (eg. Is the disease clustered within the\r\narea?)\r\nOf course, similar could be applied in the insurance context as\r\nwell.\r\nFor example, one could study whether there are any underlying\r\ngeographical patterns within the location on where claims occur.\r\nComplete Spatial Randomness\r\nComplete spatial randomness (CSR) studies whether the events are\r\ndistributed independently at random and uniformly over the study area\r\n(Bivand, Pebesma, and\r\nGómez-Rubio 2013).\r\nTypes of distributions\r\nIn general, we can classify the distributions into three types (National\r\nConservation Training Center 2010):\r\n\r\n\r\nTypes of Distributions\r\n\r\n\r\nDescriptions\r\n\r\n\r\nRandom\r\n\r\n\r\nAny point is equally likely to occur at any location and the position of\r\nany point is not affected by the position of any other point\r\n\r\n\r\nUniform\r\n\r\n\r\nEvery point is as far from all of its neighbors as possible\r\n\r\n\r\nClustered\r\n\r\n\r\nMany points are concentrated close together, and large areas that\r\ncontain very few, if any, points\r\n\r\n\r\nUsually, clustered pattern happens when there is an attraction\r\nbetween the points, whilst the regular patterns occur when there is\r\ninhibition among the points (Bivand, Pebesma, and Gómez-Rubio\r\n2013).\r\nG function\r\nThere are other statistical tests (eg. F function, K function, L\r\nfunction, etc) that could help us to test whether the objects are\r\nrandomly distributed.\r\nTo keep the post short and sweet, I will only explore the G function\r\nin this post.\r\nG function measures the distribution of the distances from an\r\narbitrary event to its nearest event (Bivand, Pebesma, and Gómez-Rubio\r\n2013).\r\nG function is defined as following:\r\n\\(\\hat{G}(r) = \\frac {\\# \\{ d_i : d_i\\le r,\r\n\\forall i \\}}{n}\\)\r\nwhere:\r\nthe numerator is the number of elements in the set of distances\r\nthat are lower than or equal to d\r\nd is \\(min_j \\{ d_{ij}^{*}, \\forall j\r\n\\ne i \\}\\)\r\nn is the total number of points\r\nDemonstration\r\nBefore jumping straight into the geospatial technique, let’s\r\nunderstand the common data files (i.e. shapefiles) used for the\r\ngeospatial purposes.\r\nWhat is a shapefile?\r\nA shapefile is a simple, nontopological format for storing the\r\ngeometric location and attribute information of geographic features\r\n(esri a).\r\nUsually the shapefiles will contain at least .shp, .shx, .dbf, and\r\n.prj (esri\r\nb).\r\nInstead of our usual import function (eg. read_csv,\r\nread_excel etc), we will use the function that is specially\r\ndesigned to handle shapefiles.\r\nData Source\r\nI will be using Singapore Bus Stop Location dataset for this\r\ndemonstration.\r\n\r\n\r\n\r\nPhoto by\r\nHu\r\nChen on\r\nUnsplash\r\nThe dataset can be downloaded from LIA Data\r\nMall.\r\nTo download the data files, below are the steps to download the data\r\nfrom the Data Mall:\r\nGo to the ‘Static Datasets’ section\r\nThen, choose ‘Road Infrastructure’\r\nFilter the search by entering ‘Bus Stop Location’ in the search\r\nbar\r\n\r\n\r\n\r\nI will also download the Singapore planning zone map from Data.gov.sg.\r\nSetup the environment\r\nOkay, let’s set up the environment for the demonstration later.\r\n\r\n\r\npackages = c('sf', 'maptools', 'spatstat', 'tmap')\r\n\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n    }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\nThese are the documentation pages for sf, tmap, maptools,\r\nand spatstat.\r\nImport the Data\r\nFirst, I will import the data into the environment.\r\nAs the file is a shapefile, I will use st_read function\r\nto read in the shapefiles.\r\n\r\n\r\ndf <- st_read(dsn = \"data/BusStopLocation\", layer = \"BusStop\")\r\n\r\n\r\nReading layer `BusStop' from data source \r\n  `C:\\Users\\Jasper Lok\\Documents\\2_Data Science\\my-blog\\_posts\\2022-02-12-point-pattern-analysis\\data\\BusStopLocation' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 5156 features and 3 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: 4427.938 ymin: 26482.1 xmax: 48282.5 ymax: 52983.82\r\nProjected CRS: SVY21\r\n\r\nOver here, I have indicated the folder name under dsn\r\nargument and file names under layer argument.\r\nI will also import the planning zone info into the environment for\r\nlater purposes.\r\n\r\n\r\nmpsz <- st_read(dsn = \"data/SG Map\", layer = \"MP14_SUBZONE_WEB_PL\")\r\n\r\n\r\nReading layer `MP14_SUBZONE_WEB_PL' from data source \r\n  `C:\\Users\\Jasper Lok\\Documents\\2_Data Science\\my-blog\\_posts\\2022-02-12-point-pattern-analysis\\data\\SG Map' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 323 features and 15 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\r\nProjected CRS: SVY21\r\n\r\nOne common check before the analysis is to ensure all the data is\r\nusing the same projection system.\r\nTo check this, we can use st_crs function to extract the\r\ninformation of the geospatial data.\r\n\r\n\r\nst_crs(df)\r\n\r\n\r\nCoordinate Reference System:\r\n  User input: SVY21 \r\n  wkt:\r\nPROJCRS[\"SVY21\",\r\n    BASEGEOGCRS[\"WGS 84\",\r\n        DATUM[\"World Geodetic System 1984\",\r\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\r\n                LENGTHUNIT[\"metre\",1]],\r\n            ID[\"EPSG\",6326]],\r\n        PRIMEM[\"Greenwich\",0,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\r\n    CONVERSION[\"unnamed\",\r\n        METHOD[\"Transverse Mercator\",\r\n            ID[\"EPSG\",9807]],\r\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8801]],\r\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8802]],\r\n        PARAMETER[\"Scale factor at natural origin\",1,\r\n            SCALEUNIT[\"unity\",1],\r\n            ID[\"EPSG\",8805]],\r\n        PARAMETER[\"False easting\",28001.642,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8806]],\r\n        PARAMETER[\"False northing\",38744.572,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8807]]],\r\n    CS[Cartesian,2],\r\n        AXIS[\"(E)\",east,\r\n            ORDER[1],\r\n            LENGTHUNIT[\"metre\",1,\r\n                ID[\"EPSG\",9001]]],\r\n        AXIS[\"(N)\",north,\r\n            ORDER[2],\r\n            LENGTHUNIT[\"metre\",1,\r\n                ID[\"EPSG\",9001]]]]\r\n\r\nThe geodetic CRS from this geospatial data is SVY21 and ellipsoid is\r\nWGS 84.\r\nSimilarly, I will check the information of the mpsz spatial objects\r\nto ensure they are projecting on the same projection system as the\r\nspatial objects of the bus stop.\r\n\r\n\r\nst_crs(mpsz)\r\n\r\n\r\nCoordinate Reference System:\r\n  User input: SVY21 \r\n  wkt:\r\nPROJCRS[\"SVY21\",\r\n    BASEGEOGCRS[\"SVY21[WGS84]\",\r\n        DATUM[\"World Geodetic System 1984\",\r\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\r\n                LENGTHUNIT[\"metre\",1]],\r\n            ID[\"EPSG\",6326]],\r\n        PRIMEM[\"Greenwich\",0,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\r\n    CONVERSION[\"unnamed\",\r\n        METHOD[\"Transverse Mercator\",\r\n            ID[\"EPSG\",9807]],\r\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8801]],\r\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\r\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\r\n            ID[\"EPSG\",8802]],\r\n        PARAMETER[\"Scale factor at natural origin\",1,\r\n            SCALEUNIT[\"unity\",1],\r\n            ID[\"EPSG\",8805]],\r\n        PARAMETER[\"False easting\",28001.642,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8806]],\r\n        PARAMETER[\"False northing\",38744.572,\r\n            LENGTHUNIT[\"metre\",1],\r\n            ID[\"EPSG\",8807]]],\r\n    CS[Cartesian,2],\r\n        AXIS[\"(E)\",east,\r\n            ORDER[1],\r\n            LENGTHUNIT[\"metre\",1,\r\n                ID[\"EPSG\",9001]]],\r\n        AXIS[\"(N)\",north,\r\n            ORDER[2],\r\n            LENGTHUNIT[\"metre\",1,\r\n                ID[\"EPSG\",9001]]]]\r\n\r\nPlotting the Map\r\nI use tmap package as this package allows us in creating\r\ninteractive maps.\r\nTo use tmap, we will start with tmap_mode\r\nfunction. Since I want to plot an interactive graph, I will choose\r\n‘view’ option.\r\nThen I will indicate the shape object in the tmap_shape\r\nfunction. After that, we will need to indicate how we would like the\r\nplotting function to draw the symbols.\r\n\r\n\r\ntmap_mode('view')\r\ntm_shape(df) + \r\n  tm_dots()\r\n\r\n\r\n\r\n\r\nOne cool thing about this package is that the API of this package is\r\nbased on the grammar of graphics. Hence, the syntax is pretty similar to\r\nggplot package.\r\nAnother fantastic part of this package is it also allows us to choose\r\nleaftlet layer to change the maps.\r\nTo do so, we can hover over the “stacking” button below the “minus”\r\nbutton in the graph. The list will pop up as shown below.\r\n\r\n\r\n\r\nAlternatively, we can indicate the interested layer through\r\ntm_basemap function as shown below. This will provide users\r\nwith an alternative method to choose from a wider range of layers.\r\n\r\n\r\ntmap_mode('view')\r\ntm_basemap(\"Esri.DeLorme\") +\r\ntm_shape(df) + \r\n  tm_dots()\r\n\r\n\r\n\r\n\r\nYou can find other different layers in the following link.\r\nFrom the map, there are 5 bus stops outside of Singapore! These are\r\nvalid points since there are buses going back and forth to Johor Bahru\r\nevery day.\r\nStatistical\r\nTest on all the bus stops in the entire Singapore\r\nNext, we will attempt to find out whether the bus stops are randomly\r\ndistributed by running a hypothesis test.\r\n\\(H_{0}:\\) The bus stops in\r\nSingapore is randomly distributed\r\n\\(H_{\\alpha}:\\) The bus stops in\r\nSingapore is not randomly distributed\r\nTo run the Monte Carlo test, we will first need to convert the\r\nspatial object into a point pattern object.\r\nThe simulation function, envelope function can only\r\naccept point pattern or a fitted point process model as stated in the documentation\r\npage.\r\nFirst, I will convert the simple feature data frame into spatial\r\npoints by using as_Spatial function.\r\n\r\n\r\nbusstop_sp <- as_Spatial(df)\r\n\r\n\r\n\r\nThen, I will convert the spatial points to spatial points in\r\nspastat package. This is to allow us to perform the\r\nnecessary test later.\r\nmaptools package will provide some functions to convert\r\nbetween ppp objects representing two-dimensional points\r\npatterns between spatstat package and\r\nSpatialPoints classes\r\n\r\n\r\nbusstop_ppp <- as(busstop_sp, \"ppp\")\r\n\r\n\r\n\r\nOnce the conversion is done, we can pass the spatial object into\r\nenvelope function.\r\nenvelope function will randomly simulate many point\r\npatterns so that summary function is computed for all of them (Bivand, Pebesma, and Gómez-Rubio\r\n2013).\r\n\r\n\r\nG_sg_csr <- envelope(busstop_ppp, \r\n                     Gest, \r\n                     nsim = 1000)\r\n\r\n\r\nGenerating 1000 simulations of CSR  ...\r\n1, 2, 3, ......10.........20.........30.........40.........50.........60\r\n.........70.........80.........90.........100.........110.........120\r\n.........130.........140.........150.........160.........170.........180\r\n.........190.........200.........210.........220.........230.........240\r\n.........250.........260.........270.........280.........290.........300\r\n.........310.........320.........330.........340.........350.........360\r\n.........370.........380.........390.........400.........410.........420\r\n.........430.........440.........450.........460.........470.........480\r\n.........490.........500.........510.........520.........530.........540\r\n.........550.........560.........570.........580.........590.........600\r\n.........610.........620.........630.........640.........650.........660\r\n.........670.........680.........690.........700.........710.........720\r\n.........730.........740.........750.........760.........770.........780\r\n.........790.........800.........810.........820.........830.........840\r\n.........850.........860.........870.........880.........890.........900\r\n.........910.........920.........930.........940.........950.........960\r\n.........970.........980.........990......... 1000.\r\n\r\nDone.\r\n\r\nI will also indicate the nsim should be 1,000, so that the function\r\nwill simulate over 1,000 times.\r\nFollowing is how to interpret the graph:\r\nIf the line for observations (i.e. the solid line) is outside and\r\nabove the grey region, this indicates that the points are exhibiting a\r\nclustered pattern\r\nIf the line is outside and below the grey region, this indicates\r\nthe points are exhibiting a regular pattern (i.e. the reverse of\r\nclustered pattern)\r\nIf the line falls within the grey region, there is statistical\r\nevidence that the points are randomly distributed\r\n\r\n\r\nplot(G_sg_csr)\r\n\r\n\r\n\r\n\r\nAs shown above, we can see the solid line lies above the grey area.\r\nHence we reject the null hypothesis and conclude that the bus points\r\nexhibits clustered pattern.\r\nStatistical\r\nTest on all the bus stops in the selected area\r\nInstead of performing a statistical test for the entire area, we\r\ncould segment out the area we are interested in and perform the\r\nnecessary statistical test.\r\nFirst, I will list out the planning areas within the Singapore\r\nmap.\r\n\r\n\r\nsort(unique(mpsz$PLN_AREA_N))\r\n\r\n\r\n [1] \"ANG MO KIO\"              \"BEDOK\"                  \r\n [3] \"BISHAN\"                  \"BOON LAY\"               \r\n [5] \"BUKIT BATOK\"             \"BUKIT MERAH\"            \r\n [7] \"BUKIT PANJANG\"           \"BUKIT TIMAH\"            \r\n [9] \"CENTRAL WATER CATCHMENT\" \"CHANGI\"                 \r\n[11] \"CHANGI BAY\"              \"CHOA CHU KANG\"          \r\n[13] \"CLEMENTI\"                \"DOWNTOWN CORE\"          \r\n[15] \"GEYLANG\"                 \"HOUGANG\"                \r\n[17] \"JURONG EAST\"             \"JURONG WEST\"            \r\n[19] \"KALLANG\"                 \"LIM CHU KANG\"           \r\n[21] \"MANDAI\"                  \"MARINA EAST\"            \r\n[23] \"MARINA SOUTH\"            \"MARINE PARADE\"          \r\n[25] \"MUSEUM\"                  \"NEWTON\"                 \r\n[27] \"NORTH-EASTERN ISLANDS\"   \"NOVENA\"                 \r\n[29] \"ORCHARD\"                 \"OUTRAM\"                 \r\n[31] \"PASIR RIS\"               \"PAYA LEBAR\"             \r\n[33] \"PIONEER\"                 \"PUNGGOL\"                \r\n[35] \"QUEENSTOWN\"              \"RIVER VALLEY\"           \r\n[37] \"ROCHOR\"                  \"SELETAR\"                \r\n[39] \"SEMBAWANG\"               \"SENGKANG\"               \r\n[41] \"SERANGOON\"               \"SIMPANG\"                \r\n[43] \"SINGAPORE RIVER\"         \"SOUTHERN ISLANDS\"       \r\n[45] \"STRAITS VIEW\"            \"SUNGEI KADUT\"           \r\n[47] \"TAMPINES\"                \"TANGLIN\"                \r\n[49] \"TENGAH\"                  \"TOA PAYOH\"              \r\n[51] \"TUAS\"                    \"WESTERN ISLANDS\"        \r\n[53] \"WESTERN WATER CATCHMENT\" \"WOODLANDS\"              \r\n[55] \"YISHUN\"                 \r\n\r\nNext, I will segment out the area of interest. In this example, I\r\nwill segment “DOWNTOWN CORE” out from the planning zone as shown\r\nfollowing.\r\n\r\n\r\ndowntown <- mpsz %>%\r\n  filter(PLN_AREA_N == \"DOWNTOWN CORE\")\r\n\r\n\r\n\r\nSimilarly, we can pass the spatial object to tmap function to see the\r\nselected area on the map.\r\n\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(downtown) +\r\n  tm_polygons()\r\n\r\n\r\n\r\n\r\nWe will then convert the spatial object into spatial polygons as\r\nshown below.\r\nNext, we need to convert the object into an owin object.\r\nOwin is defined as the “observation window” of a point pattern (R\r\ndocumentation).\r\n\r\n\r\ndowntown_owin <- as(as_Spatial(downtown), \"owin\")\r\n\r\n\r\n\r\nIf we call the owin object, it will tell us the polygon boundary of\r\nthe selected area.\r\n\r\n\r\ndowntown_owin\r\n\r\n\r\nwindow: polygonal boundary\r\nenclosing rectangle: [28896.262, 31528.047] x [27914.19, 31714.21] \r\nunits\r\n\r\nOnce that is done, we could “cut” the observation location as shown\r\nbelow.\r\n\r\n\r\nbusstop_ppp_downtown <- busstop_ppp[downtown_owin]\r\n\r\n\r\n\r\nFinally, we will pass the point pattern object into the G function to\r\nperform the necessary statistical test.\r\nBelow is the hypothesis test:\r\n\\(H_{0}:\\) The bus stops in Downtown\r\ncore is randomly distributed\r\n\\(H_{\\alpha}:\\) The bus stops in\r\nDowntown core is not randomly distributed\r\n\r\n\r\nG_downtown_csr <- envelope(busstop_ppp_downtown, \r\n                     Gest, \r\n                     correction = \"best\",\r\n                     nsim = 1000)\r\n\r\n\r\nGenerating 1000 simulations of CSR  ...\r\n1, 2, 3, ......10.........20.........30.........40.........50.........60\r\n.........70.........80.........90.........100.........110.........120\r\n.........130.........140.........150.........160.........170.........180\r\n.........190.........200.........210.........220.........230.........240\r\n.........250.........260.........270.........280.........290.........300\r\n.........310.........320.........330.........340.........350.........360\r\n.........370.........380.........390.........400.........410.........420\r\n.........430.........440.........450.........460.........470.........480\r\n.........490.........500.........510.........520.........530.........540\r\n.........550.........560.........570.........580.........590.........600\r\n.........610.........620.........630.........640.........650.........660\r\n.........670.........680.........690.........700.........710.........720\r\n.........730.........740.........750.........760.........770.........780\r\n.........790.........800.........810.........820.........830.........840\r\n.........850.........860.........870.........880.........890.........900\r\n.........910.........920.........930.........940.........950.........960\r\n.........970.........980.........990......... 1000.\r\n\r\nDone.\r\n\r\nLastly, I will plot out the simulated result.\r\n\r\n\r\nplot(G_downtown_csr)\r\n\r\n\r\n\r\n\r\nInteresting!\r\nThe result is showing us a different outcome when we zoom into one of\r\nthe planning areas.\r\nThe bus stops in the selected area are neither clustered nor equally\r\ndistributed.\r\nThis makes sense since it’s very unlikely the location of bus stops\r\nare concentrated in one area or have equal distance away from each\r\nother.\r\nHowever, when we look at the bus stop locations for the entire\r\nSingapore, the majority of the bus stops will be located at the\r\nresidency areas or where there are more activities.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\nshawnanggg\r\non\r\nUnsplash\r\n\r\n\r\n\r\nBivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013.\r\nApplied Spatial Data Analysis with r. Springer.\r\n\r\n\r\nDartmouth College Library. “Spatial Analysis &\r\nModelling.” https://researchguides.dartmouth.edu/gis/spatialanalysis#:~:text=Spatial%20analysis%20is%20a%20type,neighbor%20analysis%20and%20Thiessen%20polygons.\r\n\r\n\r\nesri. a. “Shapefiles.” https://desktop.arcgis.com/en/arcmap/10.3/manage-data/shapefiles/what-is-a-shapefile.htm.\r\n\r\n\r\n———. b. “Shapefiles.” https://doc.arcgis.com/en/arcgis-online/reference/shapefiles.htm#:~:text=A%20shapefile%20is%20an%20Esri,and%20attributes%20of%20geographic%20features.&text=Shapefiles%20often%20contain%20large%20features,desktop%20applications%20such%20as%20ArcMap.\r\n\r\n\r\nKam, Tin Seong. “Lesson 4: Spatial Point Patterns\r\nAnalysis.” https://geodsa.netlify.app/topics.html#lesson-4-spatial-point-patterns-analysis.\r\n\r\n\r\nNational Conservation Training Center. 2010. “Spatial Point\r\nPatterns.” https://training.fws.gov/courses/references/tutorials/geospatial/CSP7304/documents/Lecture12_PointPat1.pdf.\r\n\r\n\r\nR documentation. “Class Owin.” https://search.r-project.org/CRAN/refmans/spatstat.geom/html/owin.object.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-12-point-pattern-analysis/image/map.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-topic-modeling/",
    "title": "Topic Modeling",
    "description": "Discovering the topics within the text data",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Text Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nCategorizing documents\r\nWhat is topic modeling?\r\nApplication of topic\r\nmodeling\r\nMethods to perform topic\r\nmodeling\r\nLatent Dirichlet Allocation\r\n(LDA)\r\nWhat is Dirichlet\r\ndistribution?\r\n\r\nR package\r\nDemonstration\r\nSetup the environment\r\nDocument-feature matrix\r\nWord Cloud\r\n\r\nN-gram\r\nWord Cloud\r\n\r\nLDA\r\n\r\nConclusion\r\n\r\nI have discussed the basics of text analytics and the pre-processing\r\nfor text data in the previous post.\r\nTherefore, in this post, I will explore topic modeling, i.e. one of\r\nthe techniques under text analytics.\r\n\r\n\r\n\r\nPhoto by\r\nPrateek\r\nKatyal on\r\nUnsplash\r\nCategorizing documents\r\nCategorizing documents is one of the commonly used techniques in text\r\nanalytics.\r\n(Blumenau 2021)\r\nsummarizes the suitable technique to categorize the documents depending\r\non the situation.\r\nIn general, the two considerations of which methods are appropriate\r\nto be used to categorize the documents are as follows:\r\nAre the categories of the documents known?\r\nDo we know the rules of categorizing the documents?\r\n\r\n\r\n\r\nHowever, the categories within the documents may not be so\r\nstraightforward some of the time. For example, how granular should we\r\nsplit the actuarial topics within the documents? Should I split by the\r\nbroader topics within the documents? How do we define the broader\r\ntopic?\r\nThis is where the topic modeling technique becomes very handy.\r\nWhat is topic modeling?\r\nTopic modeling is a statistical method for identifying words in a\r\ncorpus of documents that tend to co-occur together and as a result,\r\nshare some sort of semantic relationship (Jockers and Thalken 2020).\r\nIn other words, topic modeling can also be thought of as an exercise\r\nto perform clustering on the documents based on the keywords in the\r\ndocuments.\r\nApplication of topic\r\nmodeling\r\nIn a report by Milliman (Pouget et al. 2021), the authors\r\nprovided a use case on how topic modeling could help in claim\r\nmanagement. In the use case, the author mentioned that the topic\r\nmodeling technique can be used to categorize the emails into different\r\ntopics. This could greatly reduce the number of variables for the\r\nclassification tasks.\r\nApart from that, the following are the other possible applications of\r\ntopic modeling (Valput 2020):\r\nText summarization\r\nQuery expansion\r\nSentiment analysis\r\nRecommender systems\r\nNext, we will look at how to perform topic modeling.\r\nMethods to perform topic\r\nmodeling\r\n(Sarkar 2016)\r\nThere are three methods in extracting the topics from the text data:\r\nLatent Semantic Indexing (LSA)\r\nLatent Dirichlet Allocation (LDA)\r\nNon-negative Matrix Factorization (NNMF)\r\nOver here, I will be using the second method, where it is one of the\r\npopular methods to perform topic modeling.\r\nLatent Dirichlet Allocation\r\n(LDA)\r\nAfter reading through a few different materials, personally, I think\r\nis below explanation is easier to understand.\r\n(Bansal 2016) LDA\r\nassumes documents are produced from a mixture of topics. Those topics\r\nthen generate words based on their probability distribution. Given a\r\ndataset of documents, LDA backtracks and tries to figure out what topics\r\nwould create those documents in the first place.\r\n\r\n\r\n\r\nThis method is quite similar to LSA. The main difference between LSA\r\nand LDA is that LDA assumes that the distribution of topics in a\r\ndocument and the distribution of words in topics are Dirichlet\r\ndistributions (Pascual\r\n2019).\r\nWhat is Dirichlet\r\ndistribution?\r\nThe Dirichlet distribution Dir(α) is a family of continuous\r\nmultivariate probability distributions parameterized by a vector α of\r\npositive reals (Liu 2019). It is a\r\nmultivariate generalization of the Beta distribution. Dirichlet\r\ndistributions are commonly used as prior distributions in Bayesian\r\nstatistics.\r\nThe author also provided the explanation of using Dirichlet\r\ndistribution used as a prior distribution in Bayesian statistics,\r\ni.e. this distribution is the conjugate before the categorical\r\ndistribution and multinomial distribution, and making it the prior would\r\nsimplify the maths.\r\nR package\r\nJust to recap how the different R packages work together with\r\ntidytext package (Silge and Robinson 2021):\r\n\r\n\r\n\r\nScreenshot from Chapter 6 of Text Mining with R book\r\nI will be using topicmodels package and\r\ntidytext package to perform topic modeling.\r\nDemonstration\r\nIn this demonstration, I will be using the same dataset as the\r\nprevious post. This is the link to download the dataset.\r\nThe data consists of a publicly available set of question and\r\nsentence pairs from an open domain question.\r\n\r\n\r\n\r\nPhoto by Olya Kobruseva from Pexels\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', \r\n              'ggwordcloud', 'lexicon', 'topicmodels')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nI will also be using the pre-processing steps as previous\r\npost. I will be also using the output from lemmatization to perform\r\ntopic modeling.\r\nHence, I will skip the pre-processing steps in this post so that this\r\npost is not too lengthy.\r\nDocument-feature matrix\r\nTo perform topic modeling, first, we will need to convert the token\r\ninto the document-feature matrix (dfm).\r\nAccording to the quanteda documentation\r\npage, dfm refers to documents in rows and “features” as\r\ncolumns, where we can understand the tokens as the “features” in this\r\ncontext.\r\n\r\n\r\ntext_df_lemma_1 <- text_df_lemma %>%\r\n  dfm()\r\n\r\n\r\n\r\nI will also trim away the terms that appear 10 times or less across\r\nthe document since it is unlikely the terms will carry much meaning to\r\nthe analysis.\r\n\r\n\r\ntext_df_lemma_1 <- text_df_lemma_1  %>%\r\n  dfm_trim(min_termfreq = 10)\r\n\r\n\r\n\r\nThen, topfeatures function is used to extract the top 20\r\nfeatures from the dfm object.\r\n\r\n\r\ntopfeatures(text_df_lemma_1, 20)\r\n\r\n\r\n                    state        unite      include     american \r\n        8588         2782         1842         1416         1304 \r\n        year        world          war         time        large \r\n        1132         1087         1032          927          910 \r\n        film         make       series       system         city \r\n         832          767          759          755          730 \r\n        form united state         term      country         call \r\n         702          694          686          681          667 \r\n\r\nWord Cloud\r\nNext, I will illustrate the result in word cloud format so that it’s\r\neasier to visualize how frequently the words appear in the documents\r\nrelative to one another.\r\nTo do so, first I will convert the dfm object to a tidy object by\r\nusing tidy function.\r\n\r\n\r\ntext_df_lemma_1_tidy <- tidy(text_df_lemma_1)\r\n\r\n\r\n\r\nNext, I will perform group_by to sum up the total count\r\nof the different words in the documents.\r\n\r\n\r\ntext_df_lemma_1_tidy_count <- text_df_lemma_1_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\n\r\n\r\nOnce that is done, I will use ggplot function to\r\nvisualize the word cloud.\r\n\r\n\r\ntext_df_lemma_1_tidy_count %>%\r\n  filter(tot_count >= 500) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"square\") +\r\n  scale_size_area(max_size = 500) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nFrom the graph, it seems like some unigram tokens may not carry much\r\nmeaning. The algorithm has treated each word as a token by itself.\r\nHowever, sometimes we need multiple words to convey the message.\r\nFor example, if we refer to the graph, we will note “unite”, where\r\n“unite” could mean a lot of things. If we were to refer to the raw data,\r\nwe will realize that “unite” is from the “united states”.\r\nN-gram\r\nTo overcome this, instead of performing a uni-gram, I will specify\r\n2:3 in the function so that both bi-gram and tri-gram will be\r\ngenerated.\r\n\r\n\r\ntext_df_lemma_3 <- text_df_lemma %>%\r\n  tokens_ngrams(n = 2:3, concatenator = \" \") %>%\r\n  dfm()\r\n\r\n\r\n\r\ntopfeatures function is being used to extract the top\r\nfeatures from the term frequency.\r\n\r\n\r\ntopfeatures(text_df_lemma_3, 20)\r\n\r\n\r\n          unite state             world war         unite kingdom \r\n                 1538                   228                   178 \r\n        north america            super bowl                war ii \r\n                  157                   138                   121 \r\n         world war ii             civil war           los angeles \r\n                  119                   110                    96 \r\n            york city            video game       president unite \r\n                   90                    88                    80 \r\npresident unite state          unite nation    federal government \r\n                   80                    76                    75 \r\n        academy award     television series    united state state \r\n                   73                    72                    68 \r\n    metropolitan area          sell million \r\n                   68                    66 \r\n\r\nNote that the term “unite state” appears much more frequent than the\r\nrest of the words. After some trial and error, it seems like the topic\r\nmodeling result seems to be more satisfying after I drop the word “unite\r\nstate”.\r\nHence in the dfm_trim function, I have specified I would\r\nlike to drop the term that has more than 1000 counts.\r\nI will be dropping terms with a count of 30 or less, otherwise, the\r\ndfm will be too huge.\r\n\r\n\r\ntext_df_lemma_3 <- text_df_lemma_3 %>%\r\n  dfm_trim(min_termfreq = 30,\r\n           max_termfreq = 1000, \r\n           termfreq_type = \"count\")\r\n\r\n\r\n\r\ndfm_trim function also supports other trimming methods,\r\neg. drop by proportion, rank and, quantile. Refer to this documentation\r\npage for the different trimming methods.\r\nWord Cloud\r\nNext, I will convert the dfm object into a tidy object so that I\r\ncould use ggplot function to visualize the word cloud.\r\n\r\n\r\ntext_df_lemma_3_tidy <- tidy(text_df_lemma_3)\r\n\r\ntext_df_lemma_3_tidy_count <- text_df_lemma_3_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\ntext_df_lemma_3_tidy_count %>%\r\n  filter(tot_count >= 45) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"square\") +\r\n  scale_size_area(max_size = 14) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nLDA\r\nOnce that is done, I will pass the dfm object into LDA\r\nfunction. I have specified that I would like to have 5 topics in the\r\nresult. Also, I will stick with the default fitting method, which is\r\n“VEM”.\r\nBefore that, I will perform a check to ensure there is no presence of\r\nall 0 rows within the dfm object. Otherwise, the algorithm would not\r\nwork.\r\n\r\n\r\nrowTotals <- apply(text_df_lemma_3, 1, sum)\r\n\r\ntext_df_lemma_3_new <- text_df_lemma_3[rowTotals > 0, ]\r\n\r\n\r\n\r\nOnce the check is done, I will pass the updated dfm object into the\r\nLDA function.\r\n\r\n\r\ndf_lda_lemma_3 <- LDA(text_df_lemma_3_new, \r\n              k = 5, \r\n              control = list(seed = 0,\r\n                             nstart = 1))\r\n\r\n\r\n\r\nIf we were to call the LDA object, you could see that we have fitted\r\na LDA model with VEM fitting method. There are 5 topics within the\r\nfitted model.\r\n\r\n\r\ndf_lda_lemma_3\r\n\r\n\r\nA LDA_VEM topic model with 5 topics.\r\n\r\nTo visualize the result, I will use tidy function to\r\nconvert the object into tidy data format so that ggplot\r\nfunction can be used.\r\nThis is where we can use the tidy function within\r\ntidytext package to help us to convert the dfm object into\r\na tidy object. Note that I have not passed in another argument, so the\r\ndefault matrix would be “beta” as shown in the result below.\r\n\r\n\r\ndf_lda_lemma_3_tidy <- tidy(df_lda_lemma_3)\r\n\r\ndf_lda_lemma_3_tidy\r\n\r\n\r\n# A tibble: 545 x 3\r\n   topic term                   beta\r\n   <int> <chr>                 <dbl>\r\n 1     1 african american    0.0215 \r\n 2     2 african american    0.00110\r\n 3     3 african american    0.00125\r\n 4     4 african american    0.00285\r\n 5     5 african american    0.00140\r\n 6     1 fictional character 0.0109 \r\n 7     2 fictional character 0.0109 \r\n 8     3 fictional character 0.00563\r\n 9     4 fictional character 0.00187\r\n10     5 fictional character 0.00489\r\n# ... with 535 more rows\r\n\r\nRecall beta is the parameter for per-topic word distribution. The way\r\nwe could understand the beta is the probability of being generated from\r\na necessary topic.\r\nOnce the results are being converted into tidy data format, I will\r\nfind the top 10 terms under each topic. I will also sort the term based\r\non their beta values in descending order.\r\n\r\n\r\ndf_lda_lemma_3_tidy_terms <- df_lda_lemma_3_tidy %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n = 10) %>% \r\n  ungroup() %>%\r\n  arrange(topic, -beta)\r\n\r\n\r\n\r\nThen, I will pass the result to ggplot function to\r\nvisualize the result in the word cloud.\r\n\r\n\r\ndf_lda_lemma_3_tidy_terms %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(beta, term, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~topic, scales = \"free\") +\r\n  scale_y_reordered() +\r\n  theme_minimal() +\r\n  theme(text = element_text(size = 20))\r\n\r\n\r\n\r\n\r\n\r\n\r\nFrom the result, below are some of the interesting insights:\r\nTopic 1 has some words that are related to football\r\nThere are few words related to countries or areas within topic 2,\r\neg. united states, united kingdom and, so on\r\nTopic 4 seems to be related to the super bowl\r\nTopic 3 & 5 seems to be related to war since there are a few\r\nwords related to wars\r\nAlthough both topic 3 & 5 seem to be related to wars, topic 3\r\nseems to be related to civil war and topic 5 relates to world\r\nwar\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by\r\nKinga\r\nCichewicz on\r\nUnsplash\r\n\r\n\r\n\r\nBansal, Shivam. 2016. “Beginners Guide to Topic Modeling in\r\nPythons.” https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/.\r\n\r\n\r\nBlumenau, Jack. 2021. “Day 11: Topic Models: Me314: Introduction\r\nto Data Science and Machine Learning.” https://lse-me314.github.io/lecturenotes/ME314_day11.pdf.\r\n\r\n\r\nJockers, Matthew L., and Rosamond Thalken. 2020. Text Analysis with\r\nr. Springer.\r\n\r\n\r\nLiu, Sue. 2019. “Dirichlet Distribution: Motivating LDA.”\r\nhttps://towardsdatascience.com/dirichlet-distribution-a82ab942a879.\r\n\r\n\r\nPascual, Federico. 2019. “Topic Modeling: An Introduction.”\r\nhttps://monkeylearn.com/blog/introduction-to-topic-modeling/.\r\n\r\n\r\nPouget, Fanny, Remi Bellina, Mehdi Echchelh, Karol Maciejewski,\r\nChristoph Krischanitz, Bartosz Gaweda, Dominik Sznajder, et al. 2021.\r\n“The Use of Artificial Intelligence and Data Analytics in Life\r\nInsurance.” Milliman.\r\n\r\n\r\nSarkar, Dipanjan. 2016. Text Analytics with Python a Practical Real-\r\nWorld Approach to Gaining Actionable Insights from Your Datal.\r\nApress.\r\n\r\n\r\nSilge, Julia, and David Robinson. 2021. “6 Topic Modeling.”\r\nhttps://www.tidytextmining.com/topicmodeling.html.\r\n\r\n\r\nValput, Damir. 2020. “Topic Modelling: Interpretability and\r\nApplications.” https://datascience.aero/topic-modelling/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-topic-modeling/image/books.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-02-text-analytics/",
    "title": "Text Analytics 101",
    "description": "Finding the \"gold\" within the text",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [
      "Text Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nText\r\nAnalytics\r\nPotential\r\nOpportunities in Text Analytics\r\nText\r\npre-processing\r\nImport of Raw Text and\r\nFormating\r\nRemoving\r\nunwanted characters (eg. special characters and symbols)\r\nMisspelling\r\n\r\nConverting into lowercases\r\nTokenization\r\nStopwords\r\nPart of Speech Tagging\r\nStemming & lemmatization\r\n\r\nFeature\r\nengineering techniques for text analytics\r\nBag of words\r\n(BoW)\r\nTerm frequency &\r\ninverse document frequency\r\nWord\r\nembedding\r\n\r\nHow different R packages\r\nwork together\r\nDemonstration\r\nSetup the environment\r\nImport data\r\nClean text\r\nConvert to lower letters\r\nRemove stopwords &\r\nunwanted characters\r\nReplace\r\nwords\r\nStemming\r\nLemmatization\r\n\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring different text analytics.\r\n\r\n\r\n\r\nPhoto by Leah Kelley from Pexels\r\nThis is one of the topics I wanted to explore while I was pursuing my\r\nmaster degree.\r\nWhile I was reading through the different materials on text analytics\r\nand natural language processing, I realized the more I read about text\r\nanalytics, the more I realized it is not as simple as what I initially\r\nimagined so.\r\nMind blownTaken from giphy\r\nHence I have spent quite a fair bit of time reading through different\r\nbooks and articles to understand more about text analytics.\r\nHope this would help you in understanding more about text\r\nanalytics.\r\nText Analytics\r\nAs the name suggested, this technique focuses on cleaning and drawing\r\ninsights out from the text data. Often, the text data are not\r\n‘structured’ in a way that could be easily understood by the\r\nmodels/algorithms.\r\nOnce the texts are processed, further techniques can be applied to\r\nthe pre-processed text data to draw insights or even build a machine\r\nlearning model on the text data.\r\nLet’s look at how text analytics could add value to the insurance\r\nbusiness.\r\nPotential\r\nOpportunities in Text Analytics\r\nThere are articles on how text data can be used in the insurance\r\ncontext. (Rodriguez\r\n2019) discussed the benefits unstructured data (eg. text\r\ndata) could provide, including how text data could assist insurers in\r\nimproving decision making and so on.\r\nThe author also provided an example of how insurers could tap onto\r\ntext data is how the various text analytics and natural language\r\nprocessing can be used to process the volume of the texts, assisting\r\nclaim adjusters potentially to make a more accurate decision with a\r\nshorter amount of time spent.\r\nApart from using text data in claim analysis, (Duncan 2016) also listed some of the\r\nfinancial service use cases where text analytics is being applied.\r\nWhile the actual usage of text data in insurance day-to-day operation\r\nis somewhat lower than expected, the insurers seem to remain optimistic\r\non the potential of using text data (McGrath 2021).\r\nText pre-processing\r\n(Ferrario and Naegelin\r\n2020) The typical steps of text pre-processing include the\r\nfollowing:\r\nImport of raw text and formatting\r\nConversion of text to lowercase\r\nTokenization, i.e. split of all strings of text into\r\ntokens\r\nStopwords removal\r\nPart-of-speech (POS) tagging of tokenized text\r\nStemming or lemmatization\r\nLet’s look at what does each step does in text analytics.\r\nImport of Raw Text and\r\nFormating\r\nAfter importing the text into the environment, it is common that we\r\nwill perform some levels of “text cleaning” to remove or correct some of\r\nthe texts before performing the analysis.\r\nFollowing are some considerations while performing “text\r\ncleaning”:\r\nRemoving\r\nunwanted characters (eg. special characters and symbols)\r\nWithin the text, we might have characters we wanted to remove from\r\nthe analysis. For example, we might want to exclude the reference link\r\nfrom the text analysis.\r\nMost of the modern packages would have included some pre-defined\r\nfunctions to remove the unwanted characters within the text data. For\r\nexample, below is the function to tokenize text data in\r\nquanteda package:\r\n\r\n\r\n\r\nScreen shot from quanteda\r\ndocumentation page\r\nAs shown in the screenshot above, there are options for the users to\r\nindicate whether they would like to remove the necessary text within the\r\ndata.\r\nHowever, there are scenarios where the pre-defined functions are\r\nunable to “clean” the data. For example, we might have the same words\r\nbut spell differently within the data (eg. color vs colour).\r\nThis is where regular expression (a.k.a. regex) comes in very handy.\r\nRegex can be used to extract, remove, replace or even find a string\r\nwithin the text.\r\nPersonally, I find this regex\r\nwebsite is very helpful. It helps me to visualize whether the regex\r\nis working (i.e. finding the relevant words) as intended.\r\nBelow are some of the common syntax in regex:\r\n\r\n\r\n\r\nExtracted from this\r\nwebsite\r\nThis is another great resource of\r\nthe different regex syntax to extract the necessary info from the\r\ntext.\r\nMisspelling\r\nOften the texts are unlikely to be clean. Misspelling of words is one\r\nof the common problems to tackle when analyzing texts.\r\nTo fix the misspelling, there are two approaches. One is to use\r\nexisting packages to fix the typos.\r\nAnother method is to create a manual listing of typos. The downside\r\nof such an approach is that we are limited to correct words we have\r\nobserved within the dataset.\r\nConverting into lowercases\r\nAs the usual text analytics algorithm is case-sensitive, the same\r\nwords with different lower cases and/or upper cases will be treated as\r\ndifferent words. For example, “TEXT”, “Text” and “text” will be treated\r\nas different words.\r\nHence, the usual approach is to convert all the words into lowercase\r\nso that the algorithm would not treat the same words as different words\r\ndue to the difference in letter cases.\r\nTokenization\r\nTokenization is the process of chopping character streams into tokens\r\n(Manning, Raghavan, and\r\nSchütze 2009). The authors also further explained that a\r\ntoken is an instance of a sequence of characters in some particular\r\ndocument that is grouped as a useful semantic unit for processing.\r\nIn general, tokenization consists of following two types and their\r\nrelevant descriptions (Sarkar 2016):\r\n\r\n\r\nTypes\r\n\r\n\r\nDescriptions\r\n\r\n\r\nSentence Tokenization\r\n\r\n\r\nSplit text into sentences\r\n\r\n\r\nWord Tokenization\r\n\r\n\r\nSplit text into words\r\n\r\n\r\nStopwords\r\nStopwords are the most common words in any natural language (Singh 2019). Often these stopwords do\r\nnot carry much value in helping us in understanding the context of the\r\ndocuments/articles.\r\nIn general, the stopwords can be categorized into following groups\r\n(Hvitfeldt and Silge\r\n2021a):\r\n\r\n\r\nGrouping\r\n\r\n\r\nDescriptions\r\n\r\n\r\nRecommendation\r\n\r\n\r\nGlobal stopwords\r\n\r\n\r\nWords that are almost always low in meaning in a given language\r\n\r\n\r\nUse pre-made stopwords lists to remove them\r\n\r\n\r\nSubject-specific stopwords\r\n\r\n\r\nWords that are uninformative for a given a subject area\r\n\r\n\r\nMay improve performance if we have domain expertise to create a good\r\nlist to remove them. But likely require us to create the list manually\r\nas these words are generally not considered as stopwords in a given\r\nlanguage\r\n\r\n\r\nDocument-level stopwords\r\n\r\n\r\nWords that do not provide any or much information for a given document\r\n\r\n\r\nDifficult to classify and would not be worth the effort to identify\r\n\r\n\r\nWhile the common approach is to remove stopwords while performing\r\ntext analytics, removal stopwords would not be appropriate in some of\r\nthe NLP tasks such as machine translation and so on.\r\nThis is because the meaning of the text could change when we remove\r\nstopwords. (Schumacher\r\n2019) provided an example to illustrate a scenario where\r\nremoving stopwords is a bad idea.\r\nConsidering the following example by (Schumacher 2019):\r\nOriginal statement: Stacy gave her doll to the puppy\r\nAfter removing stopwords: Stacy gave doll puppy\r\nWithout the stopwords, we are unable to interpret whether the doll\r\nwas given to the puppy or the puppy was given to the doll. Hence, this\r\nillustrates that the importance of stopwords in this context.\r\nPart of Speech Tagging\r\nParts of speech (POS) are specific lexical categories to which words\r\nare assigned based on their syntactic context and role (Sarkar 2016). In other words, the words\r\nin the text are being analyzed and tagged to a role in a sentence (eg.\r\nis the word a noun or a verb).\r\nSome of the natural language processing tasks (eg. named entity\r\nrecognition) would require the tokens to be tagged first.\r\nStemming & lemmatization\r\nOften within the texts, there might be similar words. For example,\r\nthe text might contain words like ‘cave’ and ‘caves’, where both of the\r\nwords are referring to the same thing.\r\nHence, it would be better for us to “clean” the words before\r\nperforming any analysis. Otherwise, the algorithm will be treating these\r\nwords as separate words. (Hvitfeldt and Silge 2021b) The author\r\nalso mentioned through this process, we reduce the sparsity of the test\r\ndata, which can be very helpful when training models.\r\nFollowing is the comparison of stemming and lemmatization (bitext 2021):\r\n\r\n\r\nCategory\r\n\r\n\r\nDescriptions\r\n\r\n\r\nStemming\r\n\r\n\r\nRule-based\r\nWork by cutting off the end or the beginning of the words\r\n\r\n\r\nLemmatization\r\n\r\n\r\nLinguistics-based\r\nNormalize the words based on language structure and how words are\r\nused in their context\r\nTake the morphological analysis of the words into\r\nconsiderations\r\nGenerally, longer runtime than stemming\r\n\r\n\r\nFeature\r\nengineering techniques for text analytics\r\nAs the machine learning algorithm is unable to work with the text\r\ndirectly, different feature engineering techniques can be used in\r\npreparing the text data before passing it into the algorithm.\r\nFollowing are some of the feature engineering techniques for text\r\nanalytics:\r\nBag of words (BoW)\r\n(Brownlee 2019)\r\nexplained that BoW is a representation of text that describes the\r\noccurrence of words within a document. Essentially we will split the\r\ntexts into either individual text or a group of texts.\r\nAlso, this technique is called ‘bag of words’ as under this\r\ntechnique, the sequence of the words does not matter.\r\nTerm frequency &\r\ninverse document frequency\r\n(Stecanella\r\n2019) explained that term frequency is calculating the\r\nfrequency of the words in a document, where the inverse document\r\nfrequency is measuring how common or a rare a word is in the entire\r\ndocument.\r\nAlternatively, the inverse document frequency can be thought as a\r\n“penalty function” imposed on term frequency to measure how frequently\r\nthe words appear in the document. If the words appear more frequently in\r\nthe document, the higher the “penalty” is.\r\nWord embedding\r\nThis is a more advanced technique, which I won’t be covering in this\r\npost. For more info, please refer to this\r\nlink or this\r\nlink.\r\nHow different R packages\r\nwork together\r\nIn R, many packages work with text analytics and natural language\r\nprocessing.\r\nIn this post, I will be exploring the different R packages mentioned\r\nin the Text Mining with R book.\r\nFollowing are how the different text analytics R packages could work\r\ntogether (Silge and Robinson\r\n2021):\r\n\r\n\r\n\r\nScreenshot from Chapter 6 of Text Mining with R book\r\nDemonstration\r\nIn this demonstration, I will be using a dataset from Microsoft. This\r\ndataset contains a publicly available set of question and sentence\r\npairs.\r\nThis is the link to download the dataset.\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', \r\n              'ggwordcloud', 'lexicon')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport data\r\nNext, I will import the dataset into the environment.\r\nNote that the dataset is in tsv format, hence I will be using\r\nread_tsv function to import the dataset into the\r\nenvironment.\r\n\r\n\r\ndf <- read_tsv(\"data/WikiQA.tsv\",\r\n               quote = \"\\t\")\r\n\r\n\r\n\r\nClean text\r\nOnce the data is imported into the environment, tokens\r\nfunction is used to tokenize the words.\r\nMeanwhile, I have also indicated that punctuation, numbers, symbols,\r\nand separators should be removed when the words are being tokenized.\r\n\r\n\r\ntext_df <- tokens(df$Sentence, \r\n                  remove_punct = TRUE,\r\n                  remove_numbers = TRUE,\r\n                  remove_symbols = TRUE,\r\n                  remove_separators = TRUE,\r\n                  split_hyphens = FALSE)\r\n\r\n\r\n\r\nConvert to lower letters\r\nAs discussed in the post earlier, the common text cleaning also\r\ninvolves converting the token into the lower case as the algorithm is\r\ncase-sensitive. Same words with the different cases will be treated as\r\ndifferent words.\r\nHence, I have used tokens_tolower function to convert\r\nall the words to lower case.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_tolower()\r\n\r\n\r\n\r\nRemove stopwords &\r\nunwanted characters\r\nNext, I will remove the stopwords from the text data. To do so, I\r\nwrap stopwords function with tokens_remove\r\nfunction.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_remove(stopwords(language = \"en\", source = \"smart\"), padding = FALSE)\r\n\r\n\r\n\r\nNote that I have also indicated that padding to be false\r\nso that the function will not replace the stopwords with an empty\r\nstring. In other words, the stopwords will be dropped from the\r\ndataset.\r\nAlso, note that I have indicated the source for stopwords should be\r\nsmart, where the list of stopwords within this source can\r\nbe found under this\r\nlink.\r\nThere are also other sources for the English stopwords. Following are\r\nthe different sources that are currently supported:\r\n\r\n\r\nstopwords::stopwords_getsources()\r\n\r\n\r\n[1] \"snowball\"      \"stopwords-iso\" \"misc\"          \"smart\"        \r\n[5] \"marimo\"        \"ancient\"       \"nltk\"          \"perseus\"      \r\n\r\nNext, I will perform a quick check on the cleaned text data. I have\r\nnoted a few issues within the text data.\r\nFor example, there are some digits with alphabets as shown below.\r\n\r\n\r\ntext_df[[51]][2]\r\n\r\n\r\n[1] \"21a\"\r\n\r\nIf we were to go back to the original text data, we will realize this\r\nstring seems to be the robot version number, which may not add much\r\nvalue/insight to the analysis later.\r\n\r\n\r\ndf$Sentence[51]\r\n\r\n\r\n[1] \"The Beretta 21A Bobcat is a small pocket-sized semi-automatic pistol designed by Beretta in Italy.\"\r\n\r\nHence, I will remove this string from the text data. To do so, I have\r\nused tokens_replace and the relevant regex to remove the\r\nwords.\r\nNote that in the regex, I have indicated that I want to find all the\r\nmatching strings that start with a digit and continue by letters. The\r\nasterisk means there are 0 or more letters after the digits.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_replace(pattern = c(\"\\\\d\\\\w*\"), replacement = c(\"\"), valuetype = \"regex\")\r\n\r\n\r\n\r\nBelow is another issue found in the text data. Somehow there is a\r\nhyphen before some of the words.\r\n\r\n\r\ndf$Sentence[2735]\r\n\r\n\r\n[1] \"The bar was a staple of the Chicago -based company for some seven decades.\"\r\n\r\nTherefore, during the tokenization, I have indicated that the words\r\nshould be split by hyphens. According to the documentation,\r\nhyphens will be treated as a separate token.\r\nTo remove the hyphens after splitting the words, I will use regex to\r\nremove the hyphens.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_replace(pattern = c(\"\\\\-\"), replacement = c(\"\"), valuetype = \"regex\")\r\n\r\n\r\n\r\nThese are common issues one might face when cleaning the text data.\r\nTherefore, it is always a good practice to double the output to check\r\nwhether the output is as expected based on the pre-processing steps\r\nindicated earlier.\r\nReplace words\r\nSome words may have different spelling. Without any cleaning, the\r\nalgorithm will these words with a different spelling as different\r\nwords.\r\nFor example, within the text data, I noted that “united state” can\r\nalso be spelled as “united st” or “u.s” although both of the words refer\r\nto “united state”.\r\nTherefore, I will list the words to be replaced within\r\ntokens_replace function.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_replace(pattern = c(\"united st\", \"u.s\"), \r\n                 replacement = c(\"united state\", \"united state\"), \r\n                 valuetype = \"fixed\")\r\n\r\n\r\n\r\nNote that as I am finding the exact match of the words, hence the\r\nvaluetype should be indicated as “fixed”.\r\nStemming\r\nAs mentioned in the earlier section, often we may perform some\r\nfurther cleaning to find the root words, otherwise the algorithm will\r\ntreat these words as separate words by themselves.\r\nFor example, if I were to extract the text that contains “tree”, we\r\nwill note that “trees” are just the plural form of “tree” and we may not\r\nwant to differentiate “tree” and “trees” in the analysis.\r\n\r\n\r\ntemp <- text_df %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  filter(term != \"\") %>%\r\n  filter(str_detect(term, \"\\\\b(tree)\") == TRUE)\r\n\r\nunique(temp$term)\r\n\r\n\r\n[1] \"tree\"      \"trees\"     \"treedome\"  \"treener\"   \"treehouse\"\r\n\r\nTherefore, one of the common approaches is to use stemming to find\r\nthe root words.\r\nTo do so, I will use tokens_wordstem function to perform\r\nstemming on the words.\r\n\r\n\r\ntext_df_stem <- text_df %>%\r\n   tokens_wordstem(language = \"english\")\r\n\r\n\r\n\r\nNext, I will dfm function to create a document-feature\r\nmatrix so that later I could use tidy function to convert\r\nthe object to conform to tidy data format.\r\nAlso, I will trim away all the term frequency that is lesser than\r\n8.\r\n\r\n\r\ntext_df_stem_1 <- text_df_stem %>%\r\n  dfm() %>%\r\n  dfm_trim(min_termfreq = 8)\r\n\r\ntext_df_stem_1_tidy <- tidy(text_df_stem_1) %>%\r\n  filter(term != \"\")\r\n\r\n\r\n\r\nOnce the object is converted into tidy data format, we could use our\r\nusual dplyr to transform for the necessary analysis.\r\nI will perform a frequency count on each word in the entire dataset,\r\nregardless of which documents they appear on.\r\n\r\n\r\ntext_df_stem_1_tidy_count <- text_df_stem_1_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\n\r\n\r\nI will further sort the count descending. From the results, we can\r\nsee that some of the words appear more frequently than the rest.\r\n\r\n\r\ntext_df_stem_1_tidy_count %>%\r\n  arrange(desc(tot_count))\r\n\r\n\r\n# A tibble: 5,398 x 2\r\n   term     tot_count\r\n   <chr>        <dbl>\r\n 1 state         2810\r\n 2 unit          2076\r\n 3 includ        1416\r\n 4 american      1305\r\n 5 world         1267\r\n 6 year          1154\r\n 7 nation        1047\r\n 8 war           1037\r\n 9 time           928\r\n10 film           862\r\n# ... with 5,388 more rows\r\n\r\nHowever, it would be quite difficult to compare the frequency count\r\nof different words by looking at the data above.\r\nHence, I will pass the output into ggwordcloud to\r\nvisualize the output in wordcloud. As there are too many unique words\r\nwithin the data and the graph will be cluttered with different words if\r\nwe were to visualize all the words in wordcloud, the graph will be\r\ncluttered with words.\r\nHence, to overcome this, I will filer out those words with less than\r\n350 count to lower the number of word counts.\r\n\r\n\r\ntext_df_stem_1_tidy_count %>%\r\n  filter(tot_count >= 350) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"circle\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nWhile this stemming could assist us in finding the root words, but it\r\nalso fails to identify the root words for some of the words.\r\n\r\n\r\ntext_df_stem_1_tidy_count %>%\r\n  filter(str_detect(term, \"(young)\") == TRUE)\r\n\r\n\r\n# A tibble: 3 x 2\r\n  term     tot_count\r\n  <chr>        <dbl>\r\n1 young          123\r\n2 younger         24\r\n3 youngest        24\r\n\r\nLemmatization\r\nRecall in the earlier section, I have discussed that lemmatization is\r\nanother method to find the root words.\r\nThis method is linguistics-based and could potentially overcome the\r\nissue we face when using stemming.\r\nTo perform lemmatization, I have referred to this Stack\r\nOverflow post on how to perform lemmatization by using\r\ntokens_replace function.\r\n\r\n\r\ntext_df_lemma <- text_df %>%\r\n  tokens_replace(pattern = lexicon::hash_lemmas$token, \r\n                 replacement = lexicon::hash_lemmas$lemma)\r\n\r\n\r\n\r\nOnce the lemmatization is done, let’s us check whether lemmatization\r\nhas successfully finding the root words of “younger” and “youngest”.\r\n\r\n\r\ntext_df_lemma %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  filter(term != \"\") %>%\r\n  filter(str_detect(term, \"(young)\") == TRUE) %>%\r\n  group_by(term) %>%\r\n  tally()\r\n\r\n\r\n# A tibble: 3 x 2\r\n  term           n\r\n  <chr>      <int>\r\n1 young        163\r\n2 young's        4\r\n3 youngblood     1\r\n\r\nAt least now the original words for “younger” & “youngest” are\r\nfound through lemmatization. The rest of the words which contain “young”\r\nare unlikely to have “young” as the root word.\r\nNext, I will follow similar steps to create the document-feature\r\nmatrix and convert the data into tidy data format.\r\n\r\n\r\ntext_df_lemma_1 <- text_df_lemma %>%\r\n  dfm() %>%\r\n  dfm_trim(min_termfreq = 8)\r\n\r\ntext_df_lemma_1_tidy <- tidy(text_df_lemma_1) %>%\r\n  filter(term != \"\")\r\n\r\ntext_df_lemma_1_tidy_count <- text_df_lemma_1_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\n\r\n\r\nOnce the frequency count is computed, I will pass the data into word\r\ncloud function to visualize the words in the word cloud format.\r\n\r\n\r\ntext_df_lemma_1_tidy_count %>%\r\n  filter(tot_count >= 350) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"square\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Emre Can Acer from Pexels\r\n\r\n\r\n\r\nbitext. 2021. “What Is the Difference Between Stemming and\r\nLemmatization?” https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/.\r\n\r\n\r\nBrownlee, Jason. 2019. “A Gentle Introduction to the Bag-of-Words\r\nModel.” https://machinelearningmastery.com/gentle-introduction-bag-words-model/.\r\n\r\n\r\nDuncan, Alan D. 2016. “Is It Time for Text Analytics in Financial\r\nServices?” https://blogs.gartner.com/alan-duncan/2016/09/21/time-come-text-analytics-financial-services/.\r\n\r\n\r\nFerrario, Andrea, and Mara Naegelin. 2020. “The Art of Natural\r\nLanguage Processing: Classical, Modern and Contemporary Approaches to\r\nText Document Classification.” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887.\r\n\r\n\r\nHvitfeldt, Emil, and Julia Silge. 2021a. “Chapter 3 Stop\r\nWords.” https://smltar.com/stopwords.html#stopwordssummary.\r\n\r\n\r\n———. 2021b. “Chapter 4 Stemming.” https://smltar.com/stemming.html#how-to-stem-text-in-r.\r\n\r\n\r\nManning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. 2009.\r\nAn Introduction to Information Retrieval. Cambridge University\r\nPress.\r\n\r\n\r\nMcGrath, Liam. 2021. “Mining for Gold: Text Analytics in\r\nInsurance: Natural Language Processing Provides Insurers Tools for Using\r\nText Data.” https://www.willistowerswatson.com/en-US/Insights/2021/03/mining-for-gold-text-analytics-in-insurance.\r\n\r\n\r\nRodriguez, Jason. 2019. “Decoding the Hidden Value of Unstructured\r\nText Data.” https://www.willistowerswatson.com/en-US/Insights/2019/02/decoding-the-hidden-value-of-unstructured-text-data.\r\n\r\n\r\nSarkar, Dipanjan. 2016. Text Analytics with Python a Practical Real-\r\nWorld Approach to Gaining Actionable Insights from Your Datal.\r\nApress.\r\n\r\n\r\nSchumacher, Alex. 2019. “When (Not) to Lemmatize or Remove Stop\r\nWords in Text Preprocessing.” https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html.\r\n\r\n\r\nSilge, Julia, and David Robinson. 2021. “6 Topic Modeling.”\r\nhttps://www.tidytextmining.com/topicmodeling.html.\r\n\r\n\r\nSingh, Shubham. 2019. “NLP Essentials: Removing Stopwords and\r\nPerforming Text Normalization Using NLTK and spaCy in Python.” https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/.\r\n\r\n\r\nStecanella, Bruno. 2019. “Understanding TF-ID: A Simple\r\nIntroduction.” https://monkeylearn.com/blog/what-is-tf-idf/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-02-text-analytics/image/typewriter.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-30-k-means/",
    "title": "K-Means Clustering",
    "description": "Why don't we call this algorithm as k-average algorithm? \n\n\nBecause it's mean.",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-10-30",
    "categories": [
      "Machine Learning",
      "Unsupervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nK-means\r\nclustering\r\nPros and Cons of K-means\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Checking & Wrangling\r\nTreatment on Magic Values\r\nHandling\r\nOutlier\r\nStandardization\r\nFilter out Non-numeric\r\nvariables\r\nCorrelation Checking\r\n\r\nRunning clustering\r\nalgorithm\r\nElbow curve\r\nChecking on clustering\r\nresults\r\nCheck\r\ncluster size\r\nCheck average values for\r\neach cluster\r\n\r\nParallel Coordinate Plot\r\n\r\nConclusion\r\n\r\nIn the previous post, I have discussed the basic concept of\r\nclustering and what are considerations when performing any clustering\r\nexercise.\r\nTherefore, I will be using K-means to demonstrate how a clustering\r\nalgorithm works and how to interpret the clustering results in this\r\npost.\r\nK-means clustering\r\n\r\n\r\n\r\nIllustration by Allison Horst\r\nK-means clustering is one of the most common basic algorithms one\r\nwould learn when they embark on their machine learning journey.\r\nIn short, K-means clustering attempts to partition a dataset into K\r\ndistinct, non-overlapping clusters (James et al. 2021).\r\nAllison\r\nHorst has nice illustrations on how K-means clustering work in\r\ngraphic format, making it super easy to understand.\r\nFollowing are the illustrations on how K-means clustering work:\r\n\r\n\r\n\r\nIllustration by Allison Horst\r\nPros and Cons of K-means\r\n(Google Developers\r\n2020) has listed a few pros and cons of the K-means\r\nclustering algorithm.\r\nBelow are the pros and cons extracted from the website:\r\nPros\r\nRelatively simple to implement\r\nScales to large data sets\r\nGuarantees convergence\r\nCan warm-start the positions of centroids\r\nEasily adapts to new examples\r\nGeneralizes to clusters of different shapes and sizes, such as\r\nelliptical clusters\r\nCons\r\nChoosing k manually\r\nBeing dependent on initial values\r\nClustering data of varying sizes and density\r\nClustering outliers\r\nScaling with number of dimensions\r\nDemonstration\r\nFor this demonstration, I will be using bank\r\nmarketing data from UCI Machine Learning Repository.\r\nThis dataset contains data points from past direct marketing\r\ncampaigns of a Portuguese banking institution.\r\n\r\n\r\n\r\nPhoto by Monstera from Pexels\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I\r\nneed for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidymodels', 'plotly', \r\n              'corrplot', 'GGally')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport Data\r\nNext, I will import the data into the environment and perform some\r\ndata wrangling for the analysis later.\r\n\r\n\r\nset.seed(12345)\r\n\r\ndf <- read_delim(\"data/bank-full.csv\", delim = \";\") %>%\r\n  mutate(across(where(is.character), as.factor)) %>%\r\n  mutate(month = ordered(month, levels = c(\"jan\",\r\n                                          \"feb\",\r\n                                          \"mar\",\r\n                                          \"apr\",\r\n                                          \"may\",\r\n                                          \"jun\",\r\n                                          \"jul\",\r\n                                          \"aug\",\r\n                                          \"sep\",\r\n                                          \"oct\",\r\n                                          \"nov\",\r\n                                          \"dec\"))) %>%\r\n  sample_frac(size = 0.5)\r\n\r\n\r\n\r\nNote that I will perform clustering on a sample of the data points.\r\nThis approach is commonly used when the dataset is too huge for us to\r\nperform data exploratory.\r\nOnce we have finished the clustering analysis, we could re-run on the\r\nfull dataset although the results are unlikely to change significantly\r\nwhen we change from subset of dataset to full dataset. If the results\r\nhave changed significantly, this could indicate that the subset of data\r\nused for clustering is not representative of the full dataset.\r\nData Checking & Wrangling\r\nAs part of the usual analysis, I will start by checking the data\r\nquality.\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n22606\r\nNumber of columns\r\n17\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n10\r\nnumeric\r\n7\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\njob\r\n0\r\n1\r\nFALSE\r\n12\r\nblu: 4871, man: 4665, tec: 3747, adm:\r\n2626\r\nmarital\r\n0\r\n1\r\nFALSE\r\n3\r\nmar: 13633, sin: 6433, div: 2540\r\neducation\r\n0\r\n1\r\nFALSE\r\n4\r\nsec: 11611, ter: 6566, pri: 3522, unk:\r\n907\r\ndefault\r\n0\r\n1\r\nFALSE\r\n2\r\nno: 22193, yes: 413\r\nhousing\r\n0\r\n1\r\nFALSE\r\n2\r\nyes: 12550, no: 10056\r\nloan\r\n0\r\n1\r\nFALSE\r\n2\r\nno: 18955, yes: 3651\r\ncontact\r\n0\r\n1\r\nFALSE\r\n3\r\ncel: 14648, unk: 6495, tel: 1463\r\nmonth\r\n0\r\n1\r\nTRUE\r\n12\r\nmay: 6893, jul: 3454, aug: 3130, jun:\r\n2648\r\npoutcome\r\n0\r\n1\r\nFALSE\r\n4\r\nunk: 18496, fai: 2500, oth: 870, suc:\r\n740\r\ny\r\n0\r\n1\r\nFALSE\r\n2\r\nno: 19968, yes: 2638\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n41.01\r\n10.64\r\n18\r\n33.00\r\n39\r\n48\r\n94\r\n▅▇▃▁▁\r\nbalance\r\n0\r\n1\r\n1347.74\r\n2897.13\r\n-3372\r\n71.00\r\n443\r\n1430\r\n71188\r\n▇▁▁▁▁\r\nday\r\n0\r\n1\r\n15.83\r\n8.32\r\n1\r\n8.00\r\n16\r\n21\r\n31\r\n▇▆▇▆▆\r\nduration\r\n0\r\n1\r\n259.57\r\n258.82\r\n0\r\n103.25\r\n180\r\n321\r\n3785\r\n▇▁▁▁▁\r\ncampaign\r\n0\r\n1\r\n2.78\r\n3.16\r\n1\r\n1.00\r\n2\r\n3\r\n58\r\n▇▁▁▁▁\r\npdays\r\n0\r\n1\r\n39.79\r\n99.67\r\n-1\r\n-1.00\r\n-1\r\n-1\r\n871\r\n▇▁▁▁▁\r\nprevious\r\n0\r\n1\r\n0.57\r\n2.57\r\n0\r\n0.00\r\n0\r\n0\r\n275\r\n▇▁▁▁▁\r\n\r\nTreatment on Magic Values\r\nIn the data dictionary, we are told that if the customer is not being\r\ncontacted, the pdays will be recorded as -1.\r\nIn Google\r\nMachine Learning Course, the author mentioned it is not ideal to mix\r\n“magic” values with actual data.\r\nTherefore, I will separate the “magic” values from the actual data by\r\nincluding an indicator to indicate whether the pdays were captured in\r\nthe dataset and changing the magic value to 0.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(pdays_supplied = case_when(pdays == -1 ~ \"No\",\r\n                                    TRUE ~ \"Yes\"),\r\n         pdays_supplied = as.factor(pdays_supplied),\r\n         pdays_recoded = case_when(pdays == -1 ~ 0,\r\n                                   TRUE ~ pdays)) %>%\r\n  dplyr::select(-pdays)\r\n\r\n\r\n\r\nHandling Outlier\r\nAs K-means clustering is finding the centroid within each cluster,\r\nhence the algorithm can sensitive to outliers. Hence, to overcome this,\r\nwe can either take the following actions:\r\nRemove outliers before running the clustering algorithm\r\nUse other algorithms that are less sensitive to outliers (eg.\r\nk-medians clustering)\r\nIn this post, I will remove the outliers from the dataset before\r\nperforming clustering analysis.\r\n\r\n\r\ndf_1 <- df_1 %>%\r\n filter(previous < 100)\r\n\r\n\r\n\r\nStandardization\r\nIn the data quality checking, we can see that the values are very\r\nskewed.\r\nAs K-means clustering is very sensitive to the range of the variable,\r\nhence standardization should be performed to avoid variables with larger\r\nvariance to dominate the effect of clustering.\r\n\r\n\r\ndf_1_scale <- df_1 %>%\r\n  mutate(across(where(is.numeric), scale))\r\n\r\n\r\n\r\nFilter out Non-numeric\r\nvariables\r\nThere are non-numeric variables within the dataset. However, K-means\r\nclustering algorithm is unable to handle non-numeric variables.\r\nThe common approach to get around with this issue is to convert the\r\nnon-numeric variables into dummy variables through a one-hot encoding\r\nmethod. (IBM Support 2020)\r\nmentioned that the clustering results are unlikely to be satisfactory by\r\nusing binary data. As the naming of the algorithm suggested, the\r\nalgorithm attempts to find the clusters through finding the “mean”\r\n(a.k.a. the centroid), where finding the average is not meaningful for\r\nbinary data.\r\nAs such, we can either perform clustering only on the numeric\r\nvariables or use other algorithms (e.g. K-proto) that allow us to\r\nperform clustering on the mixed data type. I will explore this\r\nalternative clustering algorithm in my future post.\r\nIn this post, I will use the first method, which is to filter out all\r\nthe non-numeric variables for clustering.\r\n\r\n\r\ndf_1_scale_num <- df_1_scale %>%\r\n  select_if(is.numeric)\r\n\r\n\r\n\r\nCorrelation Checking\r\nI will also check the correlation before I proceed and run K-means\r\nclustering. This is because highly correlated variables may affect the\r\nclustering results, causing clustering results to overlap with one\r\nanother.\r\n\r\n\r\ncorrplot(cor(df_1_scale_num, use=\"pairwise.complete.obs\"), \r\n         method = \"number\", \r\n         type = \"upper\", \r\n         tl.cex = 0.65, \r\n         number.cex = 0.65, \r\n         diag = FALSE)\r\n\r\n\r\n\r\n\r\nFrom the correlation results, it does not seem like we have any\r\nhighly correlated variables. This suggests that we could use all the\r\nvariables for clustering purposes.\r\nRunning clustering algorithm\r\nLet’s start our analysis in K-means clustering.\r\nAs mentioned earlier, we would need to upfront pre-define how many\r\nclusters we need for the K-means clustering exercise. However, we are\r\nunable to know what is the optimal number of clusters with the given\r\ndataset. Hence, the common approach is to run the algorithms over a\r\npre-defined range number of clusters.\r\nTherefore, to do so, I have referenced the code from Exploratory\r\nclustering shared on Tidymodels documentation page.\r\n\r\n\r\nkclusts_1_scale <- \r\n  tibble(k = 3:10) %>%\r\n  mutate(\r\n    kclust = map(k, ~kmeans(df_1_scale_num, algorithm=\"Lloyd\", .x)),\r\n    tidied = map(kclust, tidy),\r\n    glanced = map(kclust, glance),\r\n    augmented = map(kclust, augment, df_1_scale_num)\r\n  )\r\n\r\n\r\n\r\nNext, I would use unnest function to flatten the\r\nselected columns into regular columns.\r\n\r\n\r\nclusters_1_scale <- \r\n  kclusts_1_scale %>%\r\n  unnest(cols = c(tidied))\r\n\r\nassignments_1_scale <- \r\n  kclusts_1_scale %>% \r\n  unnest(cols = c(augmented))\r\n\r\nclusterings_1_scale <- \r\n  kclusts_1_scale %>%\r\n  unnest(cols = c(glanced))\r\n\r\n\r\n\r\nThe beauty of such approach above is the output from the codes\r\nfollows tidy data concept, which allows us to join different functions\r\ntogether without needing many transformations.\r\nElbow curve\r\nOnce the columns are flattened, I will plot the elbow curve to find\r\nthe optimal number of clusters by using ggplot\r\nfunction.\r\nIn general, the total within-cluster variation (a.k.a. tot.withinss\r\nin the algorithm/graph) decreases when the number of clusters increases.\r\nThis is expected since when the number of clusters increases, there\r\nwould be more centroids within the dataset, which decreases the distance\r\nbetween each data point and the centroids.\r\nAlternatively, we can understand that when all the data points are\r\nindividual clusters by themselves, the total within-cluster variation\r\nwould be zero. Hence, the total within-cluster variation would decrease\r\nwhen we increase the number of clusters.\r\nAs such, below is the elbow curve for the given dataset:\r\n\r\n\r\nggplot(clusterings_1_scale, aes(k, tot.withinss)) +\r\n  geom_line() +\r\n  geom_point()\r\n\r\n\r\n\r\n\r\nBased on the elbow curve above, it seems like 8 clusters is a good\r\ncutoff.\r\nNote that it is not ideal to choose the cluster that gives the lowest\r\ntotal within-cluster variation. Merely choosing the number of clusters\r\nthat give us the lowest total within-cluster variation would not work as\r\nthe data point is a cluster on its own would give us the lowest total\r\nwithin-cluster variation. This would defeat the purpose of the\r\nclustering exercise.\r\nTherefore, it would be beneficial to consider how should we select\r\nthe clustering results.\r\nDo refer to my previous\r\npost where I have discussed the different considerations while\r\nperforming clustering analysis.\r\nWith that, I will pull out the clustering results by using\r\npull function and pluck function.\r\n\r\n\r\nkclusts_result_1_scale <- kclusts_1_scale %>%\r\n  pull(kclust) %>%\r\n  pluck(6)\r\n\r\n\r\n\r\nChecking on clustering\r\nresults\r\nCheck cluster size\r\nAs discussed in my previous\r\npost, it is also quite crucial to check the number of data points\r\nwithin each cluster to ensure there is a sizable count within each\r\ncluster. The usual rule of thumb used in practice is to have at least 5\r\ndata points within each cluster.\r\nLet’s pull out the number of data points in each cluster.\r\n\r\n\r\nkclusts_result_1_scale$size\r\n\r\n\r\n[1] 3419 1428  653 3370 5490 2868 4842  535\r\n\r\nFrom the results shown above, there are at least 5 data points within\r\neach cluster.\r\nCheck average values for\r\neach cluster\r\nObserving the average values of each cluster could give us a glimpse\r\nof the different characteristics of each cluster, allowing us to\r\nformulate different strategies for different groups of customers.\r\nThis is because “not all the customers are the same”.\r\nNote that we have previously performed standardization on the dataset\r\nbefore using it to run the clustering algorithm. Hence, instead of using\r\nthe average values in the clustering results, I will compute the average\r\nvalues of each cluster by using the original dataset (i.e. the dataset\r\nbefore performing any standardization).\r\nTo do so, I will first insert back which cluster each data point\r\nbelongs to back to the dataset.\r\n\r\n\r\ndf_1_num <- df_1 %>%\r\n  select_if(is.numeric)\r\n\r\ndf_1_num$cluster <- kclusts_result_1_scale$cluster\r\n\r\n\r\n\r\nThen, I will extract out all the column names.\r\n\r\n\r\nvar_list <- df_1_num %>%\r\n  names()\r\n\r\n\r\n\r\nOnce that is done, I will use loop function to calculate\r\nthe average values for each cluster.\r\n\r\n\r\ncluster <- c(1,2,3,4,5,6,7,8)\r\nresult <- as.data.frame(cluster)\r\n\r\nfor(i in var_list){\r\n  temp_result <- df_1_num %>%\r\n    group_by(cluster) %>%\r\n    summarise({{i}} := mean(!!sym(i))) %>%\r\n    dplyr::select(-cluster)\r\n\r\n  result <- cbind(result, temp_result)\r\n\r\n}\r\n\r\nresult\r\n\r\n\r\n  cluster      age    balance       day duration  campaign\r\n1       1 52.16993  1114.4109  8.808424 212.3963  2.255338\r\n2       2 40.58403  1287.3011 16.013305 991.6078  2.435574\r\n3       3 40.58806   923.0766 22.781011 136.0888 16.464012\r\n4       4 52.70475  1247.9864 22.788724 192.7899  2.700000\r\n5       5 34.07341   904.1894 22.950638 203.8022  2.498361\r\n6       6 39.55788  1148.5764 13.542887 239.3720  2.120990\r\n7       7 33.57951   798.6838  8.224081 217.6338  2.214168\r\n8       8 43.73271 14736.7645 15.986916 238.4766  2.542056\r\n     previous pdays_recoded\r\n1 0.109388710     7.1275227\r\n2 0.213585434    16.7436975\r\n3 0.009188361     0.6324655\r\n4 0.153115727    10.9842730\r\n5 0.079963570     6.1910747\r\n6 3.628312413   265.2771967\r\n7 0.080132177     4.4258571\r\n8 0.474766355    29.5140187\r\n\r\nFollowing are some observations from the results above and possible\r\nactions we could take:\r\nAll customers under Cluster 7 seems to be new customers since their\r\naverage value of previous variable and pdays_recoded variable is 0\r\nTheir average balance is on the lower end\r\nSince we have just recently contacted this group of customers\r\n(i.e. the lead is still warm), maybe we could extract more info on this\r\ngroup of customers to attempt to do any up-selling or cross-selling to\r\nincrease the ‘balance’ amount\r\n\r\nInterestingly enough, Cluster 2 has the highest average last contact\r\nduration (i.e. the duration variable)\r\nAlthough they have the highest average last contract duration, the\r\naverage values of other features are not very different from other\r\nclusters\r\nPerhaps we could check with the business team to understand why this\r\ngroup of customers has the highest average last contract duration\r\n\r\nThe average balance for Cluster 6 is much higher than the rest of\r\nthe group. It is about 12 times more than the average balance of the\r\nentire customer base\r\nCustomer under Cluster 2 seems to be quite similar to Cluster 6 as\r\nthe average values for the features are quite similar (except for\r\nbalance and duration)\r\nIs there anything we can learn from Cluster 6 so that we could\r\n“shift” the customers in Cluster 2 to Cluster 6?\r\n\r\nThe customers under Cluster 3 were contacted quite sometimes ago\r\n(about 8 months ago)\r\nWe could run campaigns to “warm-up” and recapture this group of\r\ncustomers\r\n\r\nParallel Coordinate Plot\r\nAlternatively, a good way to illustrate the clustering results is to\r\nuse a parallel coordinate plot. This approach allows us to use the\r\nvisualization effectively to compare the average value of the\r\nclusters.\r\nTo make the graph less clustered, I will plot out 1% of the\r\nclustering results so that it is easier for us to make comparisons\r\nacross different clusters.\r\n\r\n\r\ndf_1_num$cluster <- as.factor(kclusts_result_1_scale$cluster)\r\n\r\nkclusts_result_1_scale_graph <- df_1_num %>%\r\n  sample_frac(size = 0.01) %>%\r\n  ggparcoord(columns = c(1:7), groupColumn = \"cluster\", scale = \"center\")\r\n\r\nggplotly(kclusts_result_1_scale_graph)\r\n\r\n\r\n\r\n\r\nFor example, if we filter the interactive graph by only focusing on\r\nCluster 5 & 8, we can see the average age under Cluster 8 is younger\r\nthan Cluster 5.\r\nNote that we can select and un-select the cluster in the graph by\r\nclicking the cluster number in the legend.\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Nareeta Martin on Unsplash\r\n\r\n\r\n\r\nGoogle Developers. 2020. “Clustering in Machine Learning.”\r\nhttps://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages.\r\n\r\n\r\nIBM Support. 2020. “Clustering Binary Data with k-Means (Should Be\r\nAvoided).” https://www.ibm.com/support/pages/clustering-binary-data-k-means-should-be-avoided.\r\n\r\n\r\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\r\n2021. An Introduction to Statistical Learning with Applications in\r\nr. Springer.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-30-k-means/image/kmeans_1.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-17-clustering/",
    "title": "Clustering",
    "description": "Mirror, mirror on the wall, which data are similar to one another?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-10-17",
    "categories": [
      "Machine Learning",
      "Unsupervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nUnsupervised Learning\r\nApplication of Clustering\r\nConsideration when\r\nPerforming Clustering\r\nPurpose of Clustering\r\nAnalysis\r\nAppropriateness\r\nof Features in Separating Policies into Clusters\r\nNumber of Data Points\r\nwithin Each Cluster\r\nCluster\r\nSelection\r\nLimitation of\r\nSelected Clustering Algorithm\r\n\r\nConclusion\r\n\r\nDuring the Q&A session in my last August sharing, there were some\r\nquestions on different unsupervised learning methods. It has triggered\r\nmy thought to do a sharing on unsupervised learning methods, in\r\nparticular clustering.\r\nIt is one of the basic topics one would learn when he/she learns data\r\nscience.\r\nInstead of having the explanations and demonstration in the same\r\npost, I have decided to split up the post into two so that each post is\r\nnot too long to read.\r\nIn this post, I will focus on what one should know before perform\r\nclustering analysis.\r\nThe demonstration on clustering will be included in the next\r\npost.\r\nNevertheless, let’s understand a bit more about clustering.\r\n\r\n\r\n\r\nPhoto by v2osk on Unsplash\r\nUnsupervised Learning\r\nIn general, the classical machine learning models can be further\r\nsplit into two broad categories, which are supervised and unsupervised\r\nlearning methods.\r\nUnsupervised learning is a machine learning method to discover hidden\r\npatterns or data groupings without human intervention (IBM Cloud Education 2020).\r\nIn other words, this method aims to model the underlying structure,\r\naffinities, or distribution in the data in order to learn more about its\r\nintrinsic characteristics (Ivo 2018).\r\nThe unsupervised learning method comprises clustering, association,\r\nand dimension reduction as shown in the graph below.\r\n\r\n\r\n\r\nExtracted from vas3k blog\r\nBelow are the descriptions of each unsupervised learning methods:\r\n\r\n\r\nTechniques\r\n\r\n\r\nDescriptions\r\n\r\n\r\nClustering\r\n\r\n\r\nClustering is one of the basic algorithms any beginners would learn\r\nwhen they started their data science journey.\r\nThe clustering algorithms include k-means, hierarchical clustering,\r\nlatent class analysis, and so on. It is commonly used in many\r\napplications as shown following:\r\nFind the different customer segments so that one could derive a\r\nmore targeted strategy to boost the sales\r\nGroup the customers into different groups based on their\r\nbehaviors\r\n\r\n\r\nAssociation\r\n\r\n\r\nApriori is one of the common algorithms used to mine the association\r\nrules between the different items. In other words, this method allows\r\none to discover how the items are “associated” with one\r\nanother.\r\n\r\nFollowing are some of the common examples of how the association\r\nrules are used:\r\nProduct bundling by bundling products with higher profit margin\r\nand closely associated to boost the sales\r\nPerform cross-selling and upselling products\r\nArranging the closely associated products together to increase\r\nthe sales\r\n\r\nThe more advanced technique includes taking time into considerations\r\nwhile performing association algorithms. This would allow one to\r\nunderstand whether there is a change in the customer purchasing\r\nbehaviors.\r\n\r\n\r\nDimension Reduction\r\n\r\n\r\nAs the data increases, often one would face the “curve of\r\ndimensionality”. This is an issue while building machine learning models\r\nas the data points in high-dimensional space are sparse, resulting in a\r\nless desirable machine learning model.\r\nTherefore, the common way to resolve this is to perform principal\r\ncomponent analysis (PCA). The key idea of PCA is the algorithm attempts\r\nto find a low-dimensional representation of a dataset that contains as\r\nmuch as possible of the variation (James et al. 2021).\r\nHowever, as the principal components derived from the algorithm are not\r\ndirectly observable from the dataset, hence it is often more challenging\r\nto explain or understand the results.\r\n\r\n\r\nIn this series, I will focus on clustering and leave the other two\r\nunsupervised learning methods for future posts.\r\nApplication of Clustering\r\n(Google Developers\r\n2020) has listed a list of common applications for\r\nclustering, which includes:\r\nmarket segmentation\r\nsocial network analysis\r\nsearch result grouping\r\nmedical imaging\r\nimage segmentation\r\nanomaly detection\r\nConsideration when\r\nPerforming Clustering\r\nWhile clustering could be a useful tool to discover insights within\r\nthe dataset, below are some of the important considerations when\r\nperforming clustering:\r\nPurpose of Clustering\r\nAnalysis\r\n\r\n\r\n\r\nPhoto by Mark Fletcher-Brown on Unsplash\r\n(Logan and Pentimonti\r\n2016) recommended the users to think about how the clustering\r\nresults will be used. For example, what is the business problem we be\r\nsolving through the clustering results? The required dataset could defer\r\ndepending on the business problem we are solving.\r\nNevertheless, the answer to this question will guide users in\r\nselecting the appropriate variables for the clustering analysis,\r\ndeciding the appropriate clustering algorithms, and so on.\r\nAppropriateness\r\nof Features in Separating Policies into Clusters\r\nAs the name unsupervised machine learning suggested, the clustering\r\nalgorithm has no mechanism for differentiating relevant or irrelevant\r\nvariables. Also, the clustering results can be very dependent on the\r\nvariables included in the analysis (Kam\r\n2019). Therefore, to obtain meaningful clustering results,\r\nthe selected variables should be able to reflect the inherent\r\ndifferences between the different clusters.\r\nFor example, if we know there are the fundamental structural\r\ndifference between the different types of customers/businesses, the\r\nrelevant parameters should be used in clustering so that the clusters\r\nare less likely to overlap with one another.\r\n\r\n\r\n\r\nPhoto by David Rotimi on Unsplash\r\nOf course, the variables that could effectively separate the dataset\r\nmay not be very apparent sometimes. Often, it requires some level of\r\nunderstanding of the underlying data or even the context of the\r\ndata.\r\nAs the old saying goes, garbage in garbage out. Without thinking\r\nthrough and selecting the appropriate variables, the clustering results\r\nmay not be meaningful.\r\nBesides, the number of variables used in clustering is also quite\r\nimportant. The rule of thumb recommended by Goodman is to have at least\r\nthree variables in the clustering analysis (Goodman 1974). The clustering results\r\nmay not be meaningful if insufficient variables are used in\r\nclustering.\r\nMeanwhile, having too many variables may introduce too much noise,\r\nresulting in less meaningful clustering results. Therefore, only the\r\nvariables that could help us in separating the different characteristics\r\nof the model points should be included in the analysis.\r\nNumber of Data Points\r\nwithin Each Cluster\r\n\r\n\r\n\r\nPhoto by Monstera from Pexels\r\nThe general rule of thumb is the number of data points within each\r\ncluster should not be less than 5. The idea is that the cluster is\r\nunlikely to be meaningful when the number of data points contained in\r\nthe cluster is relatively low.\r\nFor example, imagine the company is leveraging the clustering results\r\nto set up the different operations to serve different groups based on\r\nthe values customers bring in. The company would be better off choosing\r\na lower number of clusters that contain a decent number of data points\r\nwithin the clusters even though the higher number of clusters might be\r\nproducing a better clustering result. The cost of having an additional\r\nheadcount or separate process for a cluster without a decent size is\r\nlikely to be hard to justify.\r\nCluster Selection\r\n\r\n\r\n\r\nPhoto by Edu Grande on Unsplash\r\nFor clustering, the number of clusters is a parameter that needs to\r\nbe defined upfront before clustering analysis. However, this poses a\r\nchallenge - how do we know what is the optimal number of clusters for\r\nthe given dataset?\r\nThe common approach is to run the algorithm under a different number\r\nof clusters and use some measurements to compare the clustering results,\r\nproviding us with a more objective method to determine the optimal\r\nnumber of clusters.\r\nFor example, the elbow curve is one of the common approaches to find\r\nthe optimal number of clusters. This approach plots the within-cluster\r\nsum of the square against the number of clusters. Within-cluster sum of\r\nsquare measures the variability within the cluster. With the same\r\ndataset, as the number of clusters increases, the within-cluster sum of\r\nsquares tends to decrease.\r\nFollowing is an example of how the elbow curve graph looks like,\r\nwhere the x-axis represents the number of clusters and the y-axis\r\nrepresents the total withinness between clusters.\r\n\r\n\r\n\r\nExample of elbow curve graph\r\nHowever, having too many clusters could introduce noises into the\r\nclustering results, increasing the within-cluster sum of squares. The\r\nrule of thumb is to choose the point that the decrease in the\r\nwithin-cluster sum of squares would be insignificant when the number of\r\nclusters increases. This is because a simpler model is preferred if the\r\nsimpler model can differentiate the segments well.\r\nAlso, note that one could compare the clustering results only if the\r\nclustering results are run based on the same set of data. This is\r\nbecause supervised learning methods have an explicit target, so\r\ndifferent accuracy measurements can be used to measure the performance\r\nof the different methods.\r\nOn the other hand, unlike the supervised learning method, the\r\nfundamental idea of the unsupervised learning method is to use the\r\nalgorithms to find the various underlying patterns within the dataset.\r\nHence, the unsupervised learning method does not have an explicit target\r\nfor the algorithm to learn and measure on.\r\nHence, it is not meaningful to compare clustering results if the\r\nunderlying dataset used in the clustering is different. However, we\r\ncould compare the cluster results qualitatively to see which cluster\r\nmodels provide a more meaningful result. Following are some\r\nconsiderations in comparing the cluster results qualitatively:\r\nDoes each cluster have a reasonable amount of data points within\r\neach cluster (eg. not less than 5 data points within each\r\ncluster)?\r\nIs there any overlapping between the clusters? A good cluster\r\nshould be able to show the differences between the data points\r\nLimitation of\r\nSelected Clustering Algorithm\r\n\r\n\r\n\r\nPhoto by lucas souza from Pexels\r\nEach clustering algorithm has its limitations, hence appropriate data\r\ntransformation might require before the analysis.\r\nFor example, the K-means clustering algorithm can only accept numeric\r\nvariables. Hence the non-numeric variables will need to be transformed\r\nbefore they can be used for clustering purposes.\r\nAlso, the common clustering algorithm assumes the data is\r\ntime-independent. This assumption is not appropriate when we are dealing\r\nwith sequential data, such as time-series data, speech data, and so on.\r\nHence, other clustering methods (eg. dynamic time warping) that take\r\ntime into considerations when performing clustering would be more\r\nappropriate. Do check out my previous poster on how we could use dynamic\r\ntime warping to analyze time-series data in this\r\npost.\r\n\r\n\r\n\r\nPoster of my project on Dynamic Time Warping\r\nHence, it is important to understand the limitations of the selected\r\nclustering algorithm before the analysis to ensure meaningful\r\nresults.\r\nConclusion\r\nThat’s all for the day!\r\nAbove are some of the important considerations while performing\r\nclustering analysis.\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\n\r\n\r\n\r\nGoodman, L. A. 1974. “Exploratory Latent Structure Analysis Using\r\nBoth Identifiable and Unidentifiable Models.”\r\nBiometrika.\r\n\r\n\r\nGoogle Developers. 2020. “Clustering in Machine Learning.”\r\nhttps://developers.google.com/machine-learning/clustering/overview.\r\n\r\n\r\nIBM Cloud Education. 2020. “Unsupervised Learning.” https://www.ibm.com/cloud/learn/unsupervised-learning.\r\n\r\n\r\nIvo, Dinov D. 2018. Data Science and Predictive Analytics:\r\nBiomedical and Health Applications Using r. Springer.\r\n\r\n\r\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\r\n2021. An Introduction to Statistical Learning with Applications in\r\nr. Springer.\r\n\r\n\r\nKam, T. Seong, ed. 2019. “Isss602 Data Analytics Lab\r\nWeek 4 Lesson Slides - Cluster Analysis: Concepts, Algorithms and\r\nMethods.”\r\n\r\n\r\nLogan, Jessica AR., and Jill M. Pentimonti. 2016. Introduction to\r\nLatent Class Analysis for Reading Fluency Research. Edited by Kelli\r\nD. Cummings and Yaacov Petscher. Springer.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-17-clustering/image/categorize.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-17-transfer-learning-with-cnn/",
    "title": "Transfer Learning with Convolution Neural Network",
    "description": "Why learn from scratch when you can leverage the existing work?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-09-17",
    "categories": [
      "Tensorflow/Keras",
      "Deep Learning",
      "Image Recognition",
      "Transfer Learning"
    ],
    "contents": "\r\n\r\nContents\r\nHow is image recognition relevant for insurance?\r\nShorten operation process\r\nAdditional Pricing Parameters\r\nAdditional Parameters in Claim Estimation\r\n\r\nWhat is ‘Transfer Learning?’\r\nPre-trained Models\r\nConsiderations when using Pre-trained Models\r\nData similarity: Is the current dataset similar to the dataset used to train pre-trained model?\r\nSize of the data set: How big is our dataset?\r\nHow are we training the model?\r\n\r\nModel Building Steps\r\nDemonstration\r\nSetup the environment\r\nImport data\r\nImport the Pre-trained model\r\nAdd on Classifier Layer\r\nImage Augmentation\r\nModel Preparation\r\nModel Fitting\r\nVisualizing the Model Results\r\n\r\n\r\nConclusion\r\n\r\nRecently I happened to come across this post by (Chollet and Allaire 2017) on RStudio AI Blog and inspired me to give it a try to build a convolution neural network (CNN) model to solve an image classification problem.\r\nBefore jumping into the discussion, let’s take a look at how image recognition can be used in the insurance context.\r\nHow is image recognition relevant for insurance?\r\nPapers are suggesting how the image recognition technology can be used in the insurance context.\r\nIn the ‘Applying Image Recognition to Insurance’ report, (Shang 2018) explored a few examples of how insurers could leverage image recognition.\r\nShorten operation process\r\nImage recognition can be utilized at different stages of the insurance stage (eg. policy inception, update policy information, policy claim), where such use cases in this area can be observed in some countries.\r\nFor example, instead of manually inputting necessary info into the different documents at the point of policy inception, insurers could build an algorithm that would capture the details in the images uploaded. This could potentially shorten the turnaround time, resulting in an improvement in customer satisfaction.\r\nAdditional Pricing Parameters\r\n(Shang 2018) also discussed how these techniques can be used for insurance pricing. For example, in property insurance, pictures can be used to understand the riskiness of the property, providing additional parameters for pricing and underwriting purpose.\r\nNevertheless, this could allow the insurers in implementing dynamic pricing on the relevant business lines, improving the profitability of the business.\r\nAdditional Parameters in Claim Estimation\r\n(Shang 2018) also provided an example of how image recognition could be applied in real-time risk monitoring and risk management. Insurers can use the information extracted from the image to predict claim count and claim amount.\r\nNevertheless, in this exercise, instead of building the model from scratch, I will use what is known as “transfer learning” to speed up the model-building problem.\r\n\r\n\r\n\r\nPhoto by cottonbro from Pexels\r\nWhat is ‘Transfer Learning?’\r\n(Brownlee 2019a) explained that transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\r\n(Google 2021) also explained that the intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\r\nPre-trained Models\r\nThere are different types of pre-trained models that are trained to solve different types of problems. Some are trained to classify images, some are trained to handle text-related problems, and so on.\r\n\r\n\r\n\r\nDo check out this link to know about the different pre-trained models in TensorFlow.\r\nFor this analysis, I will focus the pre-trained models on the image classification problem. I will use VGG16 model to build a multi-class classification for the dataset. The relevant paper of this paper can be found in this link.\r\nConsiderations when using Pre-trained Models\r\nThe considerations of using pre-trained models can be summarized as the following diagram:\r\n\r\n\r\n\r\nDiagram on Model Tuning Considerations (Gupta 2017)\r\nData similarity: Is the current dataset similar to the dataset used to train pre-trained model?\r\nThese pre-trained models are usually commonly trained by using image data from ImageNet, where ImageNet is an image database with more than 14 million images. As such, this makes the pre-trained models generalized models.\r\nSo, before using the pre-trained models, we should ask ourselves how similar is between our dataset and the dataset used in pre-trained model. If our images are very different from the image data used in training the pre-trained models, the accuracy of our model is likely to be low.\r\nSize of the data set: How big is our dataset?\r\nAnother important consideration is how big is our dataset. The model is unlikely to perform well if the dataset is not large enough to train the models. Therefore, pre-trained models can be used as an alternative if our dataset is not large enough.\r\nHow are we training the model?\r\nThese pre-trained models typically come with the model weights that are derived by fitting the models with images from ImageNet. Hence, when we are using pre-trained models, one of the key questions we should answer is whether we should be using the model weights in the pre-trained models.\r\nThe usual approach is to freeze all the layers in the pre-trained model, except the few top layers. The few top unfreeze layers will be fine-tuned together with the classifier layer.\r\n\r\n\r\n\r\nCNN structure (Saha 2018)\r\nDo note that the more layers we unfreeze, the longer it would take to fit the model.\r\nModel Building Steps\r\nIn general, the model building steps can be summarized as follows:\r\nSplit the dataset into train and validation dataset\r\nImport pre-trained model into the environment and exclude the classifier layer from the pre-trained model\r\nAdd on classifier layer on top of the pre-trained model\r\nFreeze the weights of the pre-trained model, except for a few top layers to be tuned together with the classifier layer\r\nTrain the unfreeze top layers and classifier layer to make the model more relevant for the specific task\r\nDemonstration\r\nI will be using this image dataset from Kaggle. There are about 28k medium-quality images in this data. These photos are from 10 different categories of animals, which consist of dog, cat, horse, spider, butterfly, chicken, sheep, cow, squirrel, and elephant.\r\nBelow are some of the extracted images from the dataset:\r\n\r\n\r\n\r\nI have chosen these as the examples as I thought they looks funny.\r\nSetup the environment\r\nAs usual, I will start by calling all the relevant packages I would need in the analysis later on.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'keras')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport data\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\n\r\nAs mentioned in the post earlier, there are 10 different categories in the dataset. Hence this is a multi-class classification problem. Since the photos are saved under the respective categories, I will use dir function to extract out the list of categories, where train_dir is the link to the folder I stored the image dataset.\r\n\r\n\r\nlabel_list <- dir(train_dir)\r\n\r\n\r\n\r\nNext, length function is used to perform a count on the number of categories.\r\n\r\n\r\noutput_n <- length(label_list)\r\n\r\n\r\n\r\nNext, I will define the input size to be passed into the CNN model later. I have also defined the channel to be 3 as the photos are colored photos.\r\n\r\n\r\nwidth <- 224\r\nheight<- 224\r\nrgb <- 3 \r\n\r\ntarget_size <- c(width, height)\r\n\r\n\r\n\r\nImport the Pre-trained model\r\nAs discussed earlier, I would use the pre-trained model to classify the image. Over here, I will be using VGG16 as shown in the code chunk below. As I won’t be re-trained the entire network, I will indicate the model weights should follow the derived weights when the model is trained by using ImageNet dataset.\r\nAlso, to include our classifier layer, I will indicate the include_top argument to be false so that the pre-trained model is imported without the classifier layer.\r\n\r\n\r\nconv_base <- application_vgg16(\r\n  weights = \"imagenet\",\r\n  include_top = FALSE,\r\n  input_shape = c(width, height, rgb)\r\n)\r\n\r\n\r\n\r\nFor other pre-trained models, do refer to the ‘Applications’ section under either Keras documentation page or TensorFlow documentation page for more information.\r\nNext, summary function can be used to visualize how the pre-trained model looks like.\r\n\r\n\r\nsummary(conv_base)\r\n\r\n\r\nModel: \"vgg16\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\ninput_1 (InputLayer)           [(None, 224, 224, 3)]       0          \r\n______________________________________________________________________\r\nblock1_conv1 (Conv2D)          (None, 224, 224, 64)        1792       \r\n______________________________________________________________________\r\nblock1_conv2 (Conv2D)          (None, 224, 224, 64)        36928      \r\n______________________________________________________________________\r\nblock1_pool (MaxPooling2D)     (None, 112, 112, 64)        0          \r\n______________________________________________________________________\r\nblock2_conv1 (Conv2D)          (None, 112, 112, 128)       73856      \r\n______________________________________________________________________\r\nblock2_conv2 (Conv2D)          (None, 112, 112, 128)       147584     \r\n______________________________________________________________________\r\nblock2_pool (MaxPooling2D)     (None, 56, 56, 128)         0          \r\n______________________________________________________________________\r\nblock3_conv1 (Conv2D)          (None, 56, 56, 256)         295168     \r\n______________________________________________________________________\r\nblock3_conv2 (Conv2D)          (None, 56, 56, 256)         590080     \r\n______________________________________________________________________\r\nblock3_conv3 (Conv2D)          (None, 56, 56, 256)         590080     \r\n______________________________________________________________________\r\nblock3_pool (MaxPooling2D)     (None, 28, 28, 256)         0          \r\n______________________________________________________________________\r\nblock4_conv1 (Conv2D)          (None, 28, 28, 512)         1180160    \r\n______________________________________________________________________\r\nblock4_conv2 (Conv2D)          (None, 28, 28, 512)         2359808    \r\n______________________________________________________________________\r\nblock4_conv3 (Conv2D)          (None, 28, 28, 512)         2359808    \r\n______________________________________________________________________\r\nblock4_pool (MaxPooling2D)     (None, 14, 14, 512)         0          \r\n______________________________________________________________________\r\nblock5_conv1 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_conv2 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_conv3 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_pool (MaxPooling2D)     (None, 7, 7, 512)           0          \r\n======================================================================\r\nTotal params: 14,714,688\r\nTrainable params: 14,714,688\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nAdd on Classifier Layer\r\nNext, I will add on the classifier layer as discussed in the earlier post. As this is a multi-class classification problem, hence softmax is selected to be the last activation function.\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  conv_base %>% \r\n  layer_flatten() %>% \r\n  layer_dense(units = 256, activation = \"relu\") %>% \r\n  layer_dense(units = output_n, activation = \"softmax\")\r\n\r\n\r\n\r\nI will call the summary function to check on the model before fitting the model.\r\n\r\n\r\nsummary(model)\r\n\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nvgg16 (Functional)             (None, 7, 7, 512)           14714688   \r\n______________________________________________________________________\r\nflatten (Flatten)              (None, 25088)               0          \r\n______________________________________________________________________\r\ndense_1 (Dense)                (None, 256)                 6422784    \r\n______________________________________________________________________\r\ndense (Dense)                  (None, 10)                  2570       \r\n======================================================================\r\nTotal params: 21,140,042\r\nTrainable params: 21,140,042\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nImage Augmentation\r\nImage augmentation is a commonly used technique to ensure the model is not overfit.\r\n(Brownlee 2019b) explained this technique allows one to artificially created new training data from existing data. The author further explained that the reason for using such a method is this method will aid the modern deep learning algorithms to learn the different features that are similar, but not entirely the same as the training photo.\r\nFor example, it would be a problem if the objects always appear on the same side of the photo in our dataset. The algorithm may not be able to identify the objects if the objects do not appear on the same side as what is observed under the training dataset in the new photo.\r\nSo, the code chunk below will perform image augmentation on the training data.\r\n\r\n\r\ntrain_datagen = image_data_generator(\r\n  rescale = 1/255,\r\n  rotation_range = 40,\r\n  width_shift_range = 0.2,\r\n  height_shift_range = 0.2,\r\n  shear_range = 0.2,\r\n  zoom_range = 0.2,\r\n  horizontal_flip = TRUE,\r\n  fill_mode = \"nearest\",\r\n  validation_split = 0.2\r\n)\r\n\r\n\r\n\r\nApart from that, 20% of the training dataset is being held back as the validation dataset.\r\nAlso, one important note to take note of while performing image augmentation is the validation & test dataset should not be augmented.\r\n\r\n\r\nvalidation_datagen <- image_data_generator(rescale = 1/255)  \r\n\r\n\r\n\r\nModel Preparation\r\nTo leverage on transfer learning, typically we will freeze the base model and only unfreeze the connecting layers so that we can fine-tune the layers together with the classifier layer.\r\n\r\n\r\nfreeze_weights(conv_base)\r\n\r\nunfreeze_weights(conv_base, from = \"block5_conv1\")\r\n\r\n\r\n\r\nNext, I will generate batches of data in the code chunk below.\r\n\r\n\r\ntrain_generator <- flow_images_from_directory(\r\n  train_dir,                  # Target directory  \r\n  train_datagen,              # Data generator\r\n  target_size = target_size,  # Resizes all images to 150 × 150\r\n  class_mode = \"categorical\"       # binary_crossentropy loss for binary labels\r\n)\r\n\r\nvalidation_generator <- flow_images_from_directory(\r\n  train_dir,\r\n  validation_datagen,\r\n  target_size = target_size,\r\n  class_mode = \"categorical\"\r\n)\r\n\r\n\r\n\r\nLastly, I will define all the different model components before fitting the model.\r\nI will be using optimizer_sgd (ie. stochastic gradient descent optimizer) to fit the model. Do check out the documentation page on the different optimizers.\r\nAs this is a multi-class classification problem, hence I will be using categorical_accuracy as the performance metric.\r\n\r\n\r\nmodel %>% compile(\r\n  loss = \"binary_crossentropy\",\r\n  optimizer = optimizer_sgd(),\r\n  metrics = c(\"categorical_accuracy\")\r\n)\r\n\r\n\r\n\r\nModel Fitting\r\nOnce the different model components are being defined, I will start the model fitting as shown below.\r\n\r\n\r\nhistory <- model %>% fit_generator(\r\n  train_generator,\r\n  steps_per_epoch = 100,\r\n  epochs = 10,\r\n  validation_data = validation_generator,\r\n  validation_steps = 50\r\n)\r\n\r\n\r\n\r\nVisualizing the Model Results\r\nOnce the model is fit, the history from the model fitting is passed into plot function to visualize how the results change when the epochs increase.\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\n\r\nYay! Based on the accuracy shown above, it shows that the built model does have some levels of predictability, ie. the model will perform better than a random guess on the image category. Indeed, as the epoch increases, the model accuracy increases as well.\r\nAlso, as we can see in the graph above, the accuracy increases when the epoch increases. This suggests the accuracy could increase further if we increase the number of epoch.\r\nConclusion\r\nThat’s all for today!\r\nThanks for reading the post until the end.\r\nDo check out on the Keras documentation page or TensorFlow documentation page if you want to find out more on deep learning.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nPhoto by Александар Цветановић from Pexels\r\n\r\n\r\n\r\nBrownlee, Jason. 2019a. “Machine Learning Mastery: A Gentle Introduction to Transfer Learning for Deep Learning.” https://machinelearningmastery.com/transfer-learning-for-deep-learning/.\r\n\r\n\r\n———. 2019b. “Machine Learning Mastery: How to Configure Image Data Augmentation in Keras.” https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/.\r\n\r\n\r\nChollet, François, and J. J. Allaire. 2017. “RStudio AI Blog: Image Classification on Small Datasets with Keras.” https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/.\r\n\r\n\r\nGoogle. 2021. “TensorFlow: Transfer Learning and Fine-Tuning.” https://www.tensorflow.org/tutorials/images/transfer_learning.\r\n\r\n\r\nGupta, Dishashree. 2017. “Analytics Vidhya: Transfer Learning and the Art of Using Pre-Trained Models in Deep Learning.” https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/.\r\n\r\n\r\nSaha, Sumit. 2018. “Towards Data Science: A Comprehensive Guide to Convolutional Neural Networks — the Eli5 Way.” https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.\r\n\r\n\r\nShang, Kailan, ed. 2018. “Applying Image Recognition to Insurance.” Society of Actuaries. https://www.soa.org/globalassets/assets/Files/resources/research-report/2018/applying-image-recognition.pdf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-17-transfer-learning-with-cnn/image/image_on_wall.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-31-naive-bayes/",
    "title": "Naive Bayes Classifier",
    "description": "When the \"naive\" one outperform the conventional",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-09-04",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nDiscriminative Model\r\nvs Generative Model\r\nNaive Bayes Classifier\r\nDiscrim R\r\nPackage\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Checking & Wrangling\r\nData\r\nCleaning\r\nModel Building\r\nLogistic\r\nRegression\r\nNaive Bayes Classifier\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Shiva Smyth from Pexels\r\nConventionally, logistic regression is often used to solve\r\nclassification problems.\r\nIn this post, I will explore an alternative popular machine learning\r\nalgorithms, i.e. naive bayes classifier to solve classification\r\nproblems.\r\nBefore diving into naive bayes classifier, let’s understand the\r\ndifference between discriminative model and generative model.\r\nDiscriminative Model\r\nvs Generative Model\r\nDiscriminative models directly model the posterior probability \\(Pr(Y|X)\\). One good example of such model\r\nis logistic regression models.\r\nFor generative models, instead of directly model the posterior\r\nprobability, this type of model will model the join distribution of X\r\n& Y before predicting the posterior probability given as \\(Pr(Y|X)\\) (Ottesen 2017). Naive bayes classifier is\r\nan example of such model.\r\nNaive Bayes Classifier\r\nIn this algorithm, the prior knowledge is being included when making\r\npredictions. Essentially the idea for this algorithm is given the\r\nfeatures we observed in the predictors, what is likelihood the ‘y’\r\nbelong to the particular class?\r\nFollowing is the mathematical expression for naive bayes classifier\r\nobtained from Scikit-learn (scikit-learn):\r\n\\[\\hat{y} = arg max_y P(y)\\prod_{i=1}^n\r\nP(x_i|y)\\]\r\nOne of the key assumptions in Naive Bayes classifier is all the\r\npredictors are assumed to be independent from one another (Kuhn and Johnson 2013). This assumption\r\nhas effectively simplified the calculations, allowing one to multiply\r\nthe conditional probabilities together as shown in the mathematical\r\nexpression above.\r\nAlthough the predictors are unlikely to be independent from one\r\nanother, this model has a record of decent model performance.\r\nNevertheless, as what Dr Brownlee suggested in his post of tactics to\r\ncombat imbalanced classes (Brownlee, n.d.), it is probably\r\nworthwhile to try different machine learning algorithms, instead of\r\nalways sticking to our favorite algorithm. This would enable us in\r\nselecting the algorithm has a higher accuracy.\r\nDiscrim R Package\r\nIn this demonstration, naive bayes wrapper function\r\nfrom discrim\r\npackage will be used.\r\nDiscrim packages contains various discriminant\r\nanalysis models, including linear discriminant models and Naive Bayes\r\nClassifier.\r\nTo contrast the model performance, I will be using\r\nlogistic_reg function from parnsip\r\npackage to build logistic regression models.\r\nDemonstration\r\nFor the dataset, I will be using a travel\r\ninsurance dataset from Kaggle.\r\n\r\n\r\n\r\nPhoto by\r\nAnnie\r\nSpratt on\r\nUnsplash\r\nSetup the environment\r\nAs usual, I will start by calling all the relevant packages I would\r\nneed in the analysis later on.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidymodels', 'discrim', \r\n              'naivebayes', 'glmnet', 'tictoc', 'vip', 'shapr', \r\n              'DALEXtra', 'funModeling', 'plotly', 'readxl', 'ggmosaic')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nI will import the dataset obtained from Kaggle website.\r\n\r\n\r\ndf <- read_csv(\"data/travel insurance.csv\") %>%\r\n  rename(\"Commission\" = \"Commision (in value)\")\r\n\r\n\r\n\r\nData Checking & Wrangling\r\nAs usual, I will start with some basic data quality checking.\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n63326\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n7\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nAgency\r\n0\r\n1.00\r\n3\r\n3\r\n0\r\n16\r\n0\r\nAgency Type\r\n0\r\n1.00\r\n8\r\n13\r\n0\r\n2\r\n0\r\nDistribution Channel\r\n0\r\n1.00\r\n6\r\n7\r\n0\r\n2\r\n0\r\nProduct Name\r\n0\r\n1.00\r\n9\r\n36\r\n0\r\n26\r\n0\r\nClaim\r\n0\r\n1.00\r\n2\r\n3\r\n0\r\n2\r\n0\r\nDestination\r\n0\r\n1.00\r\n4\r\n42\r\n0\r\n149\r\n0\r\nGender\r\n45107\r\n0.29\r\n1\r\n1\r\n0\r\n2\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nDuration\r\n0\r\n1\r\n49.32\r\n101.79\r\n-2\r\n9\r\n22.00\r\n53.00\r\n4881.0\r\n▇▁▁▁▁\r\nNet Sales\r\n0\r\n1\r\n40.70\r\n48.85\r\n-389\r\n18\r\n26.53\r\n48.00\r\n810.0\r\n▁▇▁▁▁\r\nCommission\r\n0\r\n1\r\n9.81\r\n19.80\r\n0\r\n0\r\n0.00\r\n11.55\r\n283.5\r\n▇▁▁▁▁\r\nAge\r\n0\r\n1\r\n39.97\r\n14.02\r\n0\r\n35\r\n36.00\r\n43.00\r\n118.0\r\n▁▇▂▁▁\r\n\r\nGender\r\nThere is excessive missing value for Gender, i.e. only about 28.8% of\r\nthe data contains values in Gender column. Hence, I will drop this\r\ncolumns since this variable is unlikely to be going to be meaningful in\r\nexplaining the target variable.\r\nDestinations\r\nThere are about 149 unique values under destinations. This could be\r\nan issue when we use this feature to build machine learning model as\r\nthere are too many unique categories.\r\nTo further group the destinations, I have extracted the mapping\r\nbetween destinations and continents from internet and imported the\r\nmapping results into the environment. This allows me to join with the\r\nexisting dataset.\r\nBelow is code chunk I have imported into the environment and\r\nperformed a left join with the original dataset:\r\n\r\n\r\ndestination_state <- read_excel(\"data/Destination_Continent_Mapping.xlsx\")\r\n\r\ndf <- df %>%\r\n  left_join(destination_state, by = c(\"Destination\" = \"Destination\"))\r\n\r\n\r\n\r\nNext, I use bar chart to plot the claim proportion by continent.\r\n\r\n\r\nggplot(df, aes(Continent, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion by Continents\")\r\n\r\n\r\n\r\n\r\nAs the proportion of claim policies is rather low, it is hard to\r\ncompare the claim proportion across different continents.\r\n\r\n\r\ndf %>%\r\n  group_by(Continent, Claim) %>%\r\n  summarise(count = n()) %>%\r\n  pivot_wider(names_from = Claim, names_prefix = \"Claim_\", values_from = count) %>%\r\n  mutate(total = Claim_No + Claim_Yes,\r\n         Claim_perc = Claim_Yes / total * 100)\r\n\r\n\r\n# A tibble: 6 x 5\r\n# Groups:   Continent [6]\r\n  Continent     Claim_No Claim_Yes total Claim_perc\r\n  <chr>            <int>     <int> <int>      <dbl>\r\n1 Africa             298         5   303      1.65 \r\n2 Asia             49596       771 50367      1.53 \r\n3 Europe            4991        63  5054      1.25 \r\n4 North America     3041        45  3086      1.46 \r\n5 Oceania           4234        42  4276      0.982\r\n6 South America      239         1   240      0.417\r\n\r\nProduct Name\r\nI will plot the claim proportion by product names.\r\n\r\n\r\nggplot(df, aes(x = `Product Name`, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion by Product Name\")\r\n\r\n\r\n\r\n\r\nThere are too many categories under product name. This would affect\r\nthe performance of machine learning models if we use this variable to\r\nbuild models.\r\nHence, I will further group the product names so that this feature\r\ndoes not have too many unique categories.\r\nAs I am unable to find any further information on the product, hence\r\nI have kept products with top 90% of the sales as their original naming\r\nand renamed the remaining products as ‘Others’.\r\nOnce the product names are being recoded, I will join the recoded\r\nproduct names back to the original dataset.\r\n\r\n\r\ndf_prod <- df %>%\r\n  group_by(`Product Name`) %>%\r\n  summarise(count = n(),\r\n            claim_ind = sum(Claim == \"Yes\")) %>%\r\n  mutate(claim_perc = claim_ind/count) %>%\r\n  arrange(desc(count)) %>%\r\n  mutate(claim_cum_perc = cumsum(count)/sum(count),\r\n         product_name_recoded = case_when(claim_cum_perc > 0.9 ~ \"Others\",\r\n                                          TRUE ~ as.character(`Product Name`))) %>%\r\n  select(-c(\"claim_perc\", \"claim_cum_perc\", \"claim_ind\", \"count\")) %>%\r\n  ungroup()\r\n\r\ndf <- df %>%\r\n  left_join(df_prod, by = c(\"Product Name\" = \"Product Name\"))\r\n\r\n\r\n\r\n\r\n\r\ndf %>%\r\n  filter(Duration < 1000) %>%\r\n  ggplot(aes(product_name_recoded, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion of Product Name Recoded\")\r\n\r\n\r\n\r\n\r\nAs shown above, we can see that claim proportion for Bronze Plan and\r\nOthers are higher than other plans.\r\nDuration\r\nBelow is the duration density plot:\r\n\r\n\r\nduration_mean <- df %>%\r\n  summarise(duration_mean = mean(Duration))\r\n\r\nggplot(df, aes(Duration)) +\r\n  geom_density() +\r\n  geom_vline(data = duration_mean, aes(xintercept = duration_mean), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 800, y = 0.025, label = paste0(\"Mean of Duration: \", round(duration_mean$duration_mean, 2))) +\r\n  labs(title = \"Density of Duration\")\r\n\r\n\r\n\r\n\r\nAs shown in the graph, most of the trips were very short trip.\r\nIf we were removed any data points with duration longer than 1000 and\r\nplot duration density between claim and no claim policies, following is\r\nthe chart:\r\n\r\n\r\nduration_mean_claim <- df %>%\r\n  filter(Duration < 1000, Claim == \"Yes\") %>%\r\n  summarise(mean_dur = mean(Duration),\r\n            mean_dur_log = log(mean_dur + 1))\r\n\r\nduration_mean_Noclaim <- df %>%\r\n  filter(Duration < 1000, Claim == \"No\") %>%\r\n  summarise(mean_dur = mean(Duration),\r\n            mean_dur_log = log(mean_dur + 1))\r\n  \r\n  \r\ndf %>%\r\n  filter(Duration < 1000) %>%\r\n  ggplot(aes(x = log(Duration + 1), color = Claim)) +\r\n  geom_density(alpha = 0.2) + \r\n  geom_vline(data = duration_mean_claim, aes(xintercept = mean_dur_log), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 5.65, y = 0.32, label = \"Average Duration - \") +\r\n  annotate(\"text\", x = 5.65, y = 0.3, label = \"Claim Policies\") +\r\n  geom_vline(data = duration_mean_Noclaim, aes(xintercept = mean_dur_log), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 3, y = 0.03, label = \"Average Duration - \") +\r\n  annotate(\"text\", x = 3, y = 0.01, label = \"No Claim Policies\") +\r\n  labs(title = \"Density of Duration between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nIt seems like the average duration of the policies with no claim is\r\nshorter than the average duration of the duration with claim. This is\r\nlogical since the duration is likely to represent the coverage duration\r\nof the travel insurance. Typically the coverage of the travel insurance\r\nstarts from the day the insurance is purchased till the day the trip is\r\nbeing completed. Therefore, with a higher duration, it is more likely\r\nthe customers would make a claim during the coverage period.\r\nMeanwhile, I do note that there are negative values within this\r\nvariable, which the values do not seem to be reasonable. Therefore, I\r\nwill remove them from the dataset.\r\nSales\r\nIn the earlier data summary, there are some policies with sales with\r\nzero or even negative. This is not logical as the sales of insurance\r\npolicies would be positive. As lack of information, I would remove these\r\npolicies from the dataset so that it will not affect the model\r\nperformance.\r\n\r\n\r\nnetSales_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\",\r\n         `Net Sales` > 0) %>%\r\n  summarise(netSales_mean_claim = mean(`Net Sales`))\r\n\r\nnetSales_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\",\r\n         `Net Sales` > 0) %>%\r\n  summarise(netSales_mean_Noclaim = mean(`Net Sales`))\r\n\r\ndf %>%\r\n  filter(`Net Sales` > 0) %>%\r\n  ggplot(aes(`Net Sales`, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = netSales_mean_claim, aes(xintercept = netSales_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 240, y = 0.03, label = \"Average Sales - Claim Policies\") +\r\n  geom_vline(data = netSales_mean_Noclaim, aes(xintercept = netSales_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 200, y = 0.02, label = \"Average Sales - No Claim Policies\") +\r\n  labs(title = \"Density of Sales between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nCommission\r\nSimilarly, I will plot the commission distribution between claim and\r\nno claim policies.\r\n\r\n\r\ncomm_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\") %>%\r\n  summarise(comm_mean_claim = mean(Commission))\r\n\r\ncomm_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\") %>%\r\n  summarise(comm_mean_Noclaim = mean(Commission))\r\n\r\ndf %>%\r\n  ggplot(aes(Commission, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = comm_mean_claim, aes(xintercept = comm_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 80, y = 0.1, label = \"Average Commission - Claim Policies\") +\r\n  geom_vline(data = comm_mean_Noclaim, aes(xintercept = comm_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 100, y = 0.15, label = \"Average Commission - No Claim Policies\") +\r\n  labs(title = \"Density of Commission between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nFrom the graph, the average commission for policies that have made a\r\nclaim is lower than the average commission for policies that does not\r\nmake a claim.\r\nAge\r\n\r\n\r\nage_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\") %>%\r\n  summarise(age_mean_claim = mean(Age))\r\n\r\nage_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\") %>%\r\n  summarise(age_mean_Noclaim = mean(Age))\r\n\r\ndf %>%\r\n  ggplot(aes(Age, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = age_mean_claim, aes(xintercept = age_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 20, y = 0.1, label = \"Average Age - \") +\r\n  annotate(\"text\", x = 20, y = 0.088, label = \"Claim Policies\") +\r\n  geom_vline(data = age_mean_Noclaim, aes(xintercept = age_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 68, y = 0.15, label = \"Average Age - No Claim Policies\") +\r\n  labs(title = \"Density of Age between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nUnlike the previous numerical variables, it seems like the average\r\nage between claim and no claim policies is quite minimal.\r\nAlso, it seems like there are weird ages within the dataset as well.\r\nThere are 984 policies with age 118, which seem to be what we know as\r\n‘magic value’ in feature engineering. One possible explanation for this\r\nis the system will default the age to be 118 if the age is not being\r\ninput into the system.\r\n\r\n\r\ndf %>%\r\n  filter(Age > 100) %>%\r\n  tally()\r\n\r\n\r\n# A tibble: 1 x 1\r\n      n\r\n  <int>\r\n1   984\r\n\r\nAs I do not have any further information to estimate the ages for\r\nthese policies, I will remove them from the dataset before building the\r\nmodel.\r\nData Cleaning\r\nBefore building the model, I will proceed and clean the data.\r\n\r\n\r\ndf_1 <- df %>%\r\n  select(-c(Gender, `Product Name`, Destination)) %>%\r\n  filter(`Net Sales` > 0,\r\n         Duration > 0,\r\n         Age < 100) %>%\r\n  rename_with(~gsub(\" \", \"_\", .x, fixed = TRUE))\r\n\r\n\r\n\r\nModel Building\r\nBelow is the generic parameters set for this analysis:\r\n\r\n\r\nset.seed(1234)\r\n\r\nprop_split <- 0.6\r\n\r\ngrid_num <- 5\r\n\r\n\r\n\r\nIn this section, I will split the data into training and testing\r\ndataset. Once this is done, I will prepare the dataset for cross\r\nvalidation later.\r\n\r\n\r\ndf_split <- initial_split(df_1, prop = prop_split, strata = Claim)\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\ndf_folds <- vfold_cv(df_train, strata = Claim)\r\n\r\n\r\n\r\nNext, I will define the formula and dataset to be used to train the\r\nmodel.\r\n\r\n\r\ngen_recipe <- recipe(Claim ~ ., data = df_train)\r\n\r\n\r\n\r\nOkay, let’s start the analysis by using our classic machine learning\r\nmodel for classification problem - logistic regression!\r\nLogistic Regression\r\nTo record how long it takes to build a logistic regression model, I\r\nuse tic & toc functions from\r\ntictoc package to capture the time spent in building\r\nmodel.\r\n\r\n\r\ntic(\"Time to Build Logistic Regression\")\r\n\r\n\r\n\r\nFirst, I will define the data pre-processing steps to be\r\nperformed.\r\n\r\n\r\nlogit_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_zv(all_predictors()) %>%\r\n  step_normalize(all_predictors())\r\n\r\n\r\n\r\nThen, I will define the model to be built. I have also indicated the\r\nparameters to be tuned by using tune function to mark\r\nthe parameters.\r\n\r\n\r\nlogit_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"glmnet\")\r\n\r\n\r\n\r\nOnce this is done, I will chain all the created objects as a\r\nworkflow.\r\n\r\n\r\nlogit_workflow <- workflow() %>% \r\n  add_recipe(logit_recipe) %>%\r\n  add_model(logit_spec)\r\n\r\n\r\n\r\nThen, I will start performing cross validation to find the best set\r\nof parameters.\r\n\r\n\r\nlogit_grid <- tidyr::crossing(penalty = c(1,2,3,4,5,6,7,8,9,10), mixture = c(0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1))\r\n\r\nlogit_tune <- tune_grid(logit_workflow, \r\n                        resample = df_folds, \r\n                        grid = logit_grid)\r\n\r\n\r\n\r\nThe model is then fitted with the best set of parameters.\r\n\r\n\r\nlogit_fit <- logit_workflow %>%\r\n  finalize_workflow(select_best(logit_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n\r\n\r\nThe predicted values are then “collected” so that I could use them to\r\ncalculate the necessary model performance.\r\n\r\n\r\nlogit_pred <- logit_fit %>%\r\n  collect_predictions()\r\n\r\n\r\n\r\nOnce the predicted values are being “collected”, toc\r\nfunction is used to calculate how long it takes to fit the model and\r\nmake the necessary predictions.\r\n\r\n\r\ntoc()\r\n\r\n\r\nTime to Build Logistic Regression: 199.63 sec elapsed\r\n\r\nAccuracy is typically used to measure classification model.\r\n\r\n\r\naccuracy(logit_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.984\r\n\r\nBased on the accuracy results, it seems like this logistic model is a\r\npretty decent model given it has such a high accuracy. More than 98% of\r\nthe policies are being accurately classified.\r\nHowever, it can be very misleading if our decision is just based on\r\naccuracy measurement. Accuracy looks the number of policies are being\r\naccurately classified whether they have claimed or not. It does not\r\ndifferentiate the true positive and true negative in this case.\r\nHowever, in insurance, we are more interested in the policies that\r\nhave claimed. One way is to compute the confusion matrix for the\r\nrelevant model.\r\n\r\n\r\nconf_mat(logit_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n          Truth\r\nPrediction    No   Yes\r\n       No  23539   372\r\n       Yes     0     0\r\n\r\nOh no! According to the results under confusion matrix, our model\r\nalways predict that the policies will not make a claim. This is bad as\r\nthis would result in the expected claim severely unstated.\r\nNow let’s us plot the ROC curve.\r\n\r\n\r\nautoplot(roc_curve(logit_pred, \r\n                   truth = Claim, \r\n                   estimate = .pred_No))\r\n\r\n\r\n\r\n\r\nThe ROC curve for this model lie at the diagonal line, suggesting\r\nthat this fitted model has no predictive power. It is as good as we take\r\na random guess whether the selected policy will make a claim.\r\nIf we were to visualize the claim count by using a histogram, we can\r\nsee that the proportion of policyholders claimed is very low.\r\n\r\n\r\nggplot(df_1, aes(Claim)) +\r\n  geom_histogram(stat = \"count\")\r\n\r\n\r\n\r\n\r\nNaive Bayes Classifier\r\nOkay, let’s move on and build a naive bayes classifier model.\r\nFirst, I will use tic function to indicate where I\r\nshould start measuring the time taken to build the model.\r\n\r\n\r\ntic(\"Time to Build Naive Bayes Classifier\")\r\n\r\n\r\n\r\nThen, I will start building the model, same as how I did it for\r\nlogistic regression.\r\n\r\n\r\nnaive_recipe <- gen_recipe %>%\r\n  step_zv(all_predictors())\r\n\r\nnaive_spec <- naive_Bayes(smoothness = tune(), Laplace = tune()) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"naivebayes\")\r\n\r\nnaive_workflow <- workflow() %>%\r\n  add_recipe(naive_recipe) %>%\r\n  add_model(naive_spec)\r\n\r\nnaive_tune <- tune_grid(naive_workflow, \r\n                        resample = df_folds, \r\n                        grid = grid_num)\r\n\r\nnaive_fit <- naive_workflow %>%\r\n  finalize_workflow(select_best(naive_tune)) %>%\r\n  last_fit(df_split)\r\n\r\nnaive_pred <- naive_fit %>%\r\n  collect_predictions()\r\n\r\n\r\n\r\n\r\n\r\ntoc()\r\n\r\n\r\nTime to Build Naive Bayes Classifier: 26.5 sec elapsed\r\n\r\nNote that the naive bayes classifier takes lesser time to build the\r\nmodel. In fact, it took less time to build naive bayes classifier than\r\nbuild logistic regression.\r\nThis is consistent with what we have discussed in the earlier section\r\nof this post.\r\n\r\n\r\naccuracy(naive_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.941\r\n\r\nSimilarly, let’s check the confusion matrix results for our naive\r\nbayes classifier.\r\n\r\n\r\nconf_mat(naive_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n          Truth\r\nPrediction    No   Yes\r\n       No  22391   252\r\n       Yes  1148   120\r\n\r\nNote that the model no longer always predict policyholders do not\r\nclaim from their travel insurance.\r\nWhen we plot the ROC curve, we noted that the ROC curve no longer lie\r\non the diagonal line. This suggests that although the accuracy of this\r\nfitted model is lower than logistic regression, this fitted model does\r\nhave some levels of predictive power.\r\n\r\n\r\nautoplot(roc_curve(naive_pred, \r\n                   truth = Claim, \r\n                   estimate = .pred_No))\r\n\r\n\r\n\r\n\r\nOverall, we can see that naive bayes classifier performs better than\r\nlogistic regression in this data.\r\nOne of the possible reason why naive bayes classifier outperforms\r\nlogistic regression is due to the small dataset. (Ng\r\nand Jordan) discussed that how the model performance for\r\nlogistic regression will outperform naive bayes classifier when the\r\ntraining size reaches infinity.\r\nHowever, for this scenario, the dataset seems to be too small for\r\nlogistic regression to converge and outperform naive bayes classifier.\r\nThe performance of logistic regression model might improve further when\r\nwe increase the training data.\r\nConclusion\r\nThat’s all for the day! We have finished “sorting” the policies in\r\nwhether they are expected to claim or does not claim.\r\n\r\n\r\n\r\nPhoto by\r\nemrecan\r\narık on\r\nUnsplash\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page\r\nif you want to find out more on naive bayes classifier.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\n\r\n\r\n\r\nBrownlee, Jason, ed. n.d. “8 Tactics to Combat Imbalanced Classes\r\nin Your Machine Learning Dataset.” https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/.\r\n\r\n\r\nKuhn, Max, and Kjell Johnson, eds. 2013. Applied Predictive\r\nModeling. Springer.\r\n\r\n\r\nNg, Andrew, and Michael Jordan, eds. “On Discriminative Vs.\r\nGenererative Classifiers: A Comparison of Logistic Regression and Naive\r\nBayes.” http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf.\r\n\r\n\r\nOttesen, Christopher. 2017. “Comparison Between Naïve Bayes and\r\nLogistic Regression.” https://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/.\r\n\r\n\r\nscikit-learn, ed. “1.9 Naive Bayes.” https://scikit-learn.org/stable/modules/naive_bayes.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-31-naive-bayes/image/balance.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-23-model-stacking/",
    "title": "Model Stacking",
    "description": "When the sum of all component is greater than individual component",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-05-23",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Ensemble method?\r\nSo, how does stacking work?\r\nWhy is this method popular?\r\nStacks R\r\nPackage\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData\r\nSplitting\r\nDefine\r\nRecipe\r\nFitting Individual\r\nMachine Learning Models\r\nStack the\r\nModels!\r\nCompare the Model Performance\r\nPerformance Metric of\r\nIndividual Models\r\nPerformance Metric of Stack\r\nModel\r\nSummary\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by\r\nCrissy\r\nJarvis on\r\nUnsplash\r\nIn this series of modeling, I will be exploring model stacking.\r\nIt has gained popularity especially in data science competition as\r\nsuch method could further improve the accuracy of the machine learning\r\nmodels.\r\nBefore jumping straight into model stacking, let’s understand what is\r\nensemble method.\r\nWhat is Ensemble method?\r\n\r\n\r\n\r\nExtracted from vas3k blog (vas3k)\r\nEnsemble method is one of the machine learning method.\r\n(Geron 2019)\r\ndescribed that ensemble method is similar to aggregate the answer of a\r\ncomplex question from thousands of random people. This anchored on the\r\nidea of wisdom of the crowd. Often this approach would produce a better\r\nanswer than an answer from an expert.\r\nSuch method can be either applied on the same machine learning\r\nalgorithms or aggregate across different machine learning\r\nalgorithms.\r\nAs the graph shown above, following are the three common ensemble\r\nmethods:\r\nBagging: Different models are trained by using\r\ndifferent dataset randomly drawn with replacement from the training\r\ndataset. The final prediction is the average of predictions from all the\r\nfitted models. Random forest is one of the algorithm that leverages on\r\nbagging to improve the model accuracy.\r\nBoosting: The idea is to a combination of weak\r\nlearners could form a strong learner. XGBoost is one commonly used model\r\nalgorithm that uses boosting method.\r\nStacking: As the name suggested, we “stack” the\r\nmodels on one another and create a new model.\r\nIn this post, I will be exploring model stacking and mainly covering\r\nthe following topics:\r\nHow does model stacking work?\r\nHow to implement model stacking in R?\r\nSo, how does stacking work?\r\nIf we were to visualize the steps for model stacking, this would be\r\nhow it would look like:\r\n\r\n\r\n\r\nFollowing are the steps to perform model stacking:\r\nFirst, fit different individual machine learning models\r\nNext, use target variable from the training dataset and the\r\npredictions from the different machine learning models as input to fit a\r\nmodel\r\nThe conventional way to stack the model is to use linear model.\r\nAlthough in theory we could use other machine learning algorithm to\r\nstack the models, this would increase the complexity of the final model,\r\nmaking it even harder to interpret.\r\nWhy is this method popular?\r\n(Funda Gunes and Tan\r\n2017) discussed in their paper that ensemble method enables\r\nthe users to average out the noise from the diverse models and thereby\r\nenhance the generalizable signal.\r\nTherefore, such method has proven to increase model accuracy in\r\nKaggle competitions. Often, even a slight uplift in improvement could\r\naffect our model performance rankings in the Kaggle competitions.\r\nWhile this method does not guarantee an improvement in the accuracy,\r\nit is worthwhile to fit such models to attempt to improve the accuracy\r\nof the predictions.\r\nHowever, by stacking the different models together, this has\r\nincreased the complexity of the models, making it even harder to\r\nunderstand on how the model reaches the decisions.\r\nThe time required for model fitting would be longer as well since we\r\nwould need to fit the base model before perform model stacking.\r\nStacks R Package\r\nOkay, let’s perform some model stacking!\r\n\r\n\r\n\r\nPhoto by\r\nLa-Rel\r\nEaster on\r\nUnsplash\r\nIn this demonstration, I will be using a package from\r\ntidymodels that allows the users to perform model\r\nstacks with just a few lines of codes. This package is called\r\nstacks.\r\n\r\n\r\n\r\nstacks\r\npackage\r\nAnd yes, this package is it conforms to the tidy data concept. It is\r\nalso designed to work together with the various packages under\r\ntidymodels.\r\nWHAT A BIG RELIEF!\r\n\r\n\r\n\r\nPhoto by Andrea Piacquadio from Pexels\r\nThis would make it easier for one to use perform model stacking.\r\nDemonstration\r\nSetup the environment\r\nBefore I start the demonstration, I will setup the environment by\r\ncalling the necessary packages.\r\n\r\n\r\npackages <- c(\"tidyverse\", \"tidymodels\", \"stacks\")\r\n\r\nfor (p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nI will also define the common parameters that will be used in the\r\nanalysis below.\r\n\r\n\r\n# Define the random seeds for reproducibility\r\nset.seed(1234)\r\n\r\n# Proportion between training and testing dataset\r\nprop_train_test <- 0.6\r\n\r\n# Define the model performance metrics we would like to output later\r\nmodel_metrics <- metric_set(rmse, rsq)\r\n\r\n# The number of grid to be used in the analysis later\r\ngrid_num <- 5\r\n\r\n\r\n\r\nTo stack model, we need to include following as well. According to\r\nthe documentation page, this is to ensure the predictions and necessary\r\ninfo are output in the necessary format required by\r\nstacks package later.\r\n\r\n\r\nctrl_grid <- control_stack_grid()\r\nctrl_res <- control_stack_resamples()\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nFor this demonstration, I will be using a dataset from Actuarial\r\nLoss Prediction Kaggle Competition.\r\nAlso, this sharing will be focusing on the model stacking, instead of\r\nend to end process. Hence, I will import the data I have previously\r\ncleaned.\r\nRefer to my EDA\r\ncode file on my Github page for the steps taken to clean the\r\ndata.\r\n\r\n\r\ndf <- read_csv(\"data/data_eda_actLoss_3.csv\") %>%\r\n  dplyr::select(-c(ClaimNumber,\r\n                   num_week_paid_ult,\r\n                   InitialIncurredClaimCost,\r\n                   UltimateIncurredClaimCost)) %>%\r\n  dplyr::select(-starts_with(c(\"acc\", \"report\"))) %>%\r\n  filter(Gender != \"U\") %>%\r\n  drop_na() %>%\r\n  sample_frac(0.3)\r\n\r\n\r\n\r\nWhile importing the data, I have also:\r\nDrop ‘ClaimNumber’, ‘num_week_paid_ult’ & column headers that\r\nstarts with ‘acc’ & ‘report’ since I am not using them\r\nlater\r\nFilter out gender ‘U’ as according to data dictionary, indicator\r\n‘U’ means missing values\r\nDrop the data with missing values\r\nSample 30% of the data to use for the analysis, otherwise it\r\nwould take forever to run on my laptop 😢\r\nData Splitting\r\nNext, I will split the dataset into training and testing dataset.\r\n\r\n\r\n# Split dataset\r\ndf_split <- initial_split(df,\r\n                            prop = prop_train_test,\r\n                            strata = init_ult_diff)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nThen, I will prepare the dataset for the k-fold validation later.\r\n\r\n\r\n# Cross validation\r\ndf_folds <- vfold_cv(df_train, strata = init_ult_diff)\r\n\r\n\r\n\r\nDefine Recipe\r\nIn this section, I will define the general recipe for the different\r\nmachine learning models.\r\n\r\n\r\ngen_recipe <- recipe(init_ult_diff ~ ., data = df_train) %>%\r\n  update_role(c(DateTimeOfAccident, DateReported, ClaimDescription), new_role = \"id\") %>% # update the roles of original date variables to \"id\"\r\n  prep()\r\n\r\n\r\n\r\nBasically, I try to perform the following in the code chunk\r\nabove:\r\nExtract the date information from date variable by using\r\nstep_date & step_mutate\r\nfunctions\r\nConvert the extracted date features into ordinal variables by\r\nusing factor function\r\nUpdate the roles for ‘DateTimeOfAccident’ & ‘Date Reported’\r\nto ‘id’ so that they will not be used in fitting the model\r\nFitting Individual\r\nMachine Learning Models\r\nBefore fitting a stack model, let’s fit individual models first.\r\nRandom Forest\r\n\r\n\r\nranger_spec <- \r\n  rand_forest() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"ranger\", importance = \"impurity\")\r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(gen_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\nset.seed(51107)\r\nranger_tune <-\r\n  tune_grid(ranger_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\nxgboost_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal())\r\n\r\nxgboost_spec <- \r\n  boost_tree() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"xgboost\") \r\n\r\nxgboost_workflow <- \r\n  workflow() %>% \r\n  \r\n  add_recipe(xgboost_recipe) %>% \r\n  add_model(xgboost_spec) \r\n\r\nset.seed(12071)\r\nxgboost_tune <-\r\n  tune_grid(xgboost_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\nearth_recipe <- gen_recipe %>% \r\n  step_novel(all_nominal(), -all_outcomes()) %>% \r\n  step_dummy(all_nominal(), -all_outcomes()) %>% \r\n  step_zv(all_predictors()) \r\n\r\nearth_spec <- \r\n  mars() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"earth\") \r\n\r\nearth_workflow <- \r\n  workflow() %>% \r\n  add_recipe(earth_recipe) %>% \r\n  add_model(earth_spec) \r\n\r\nearth_grid <- tidyr::crossing(num_terms = 2 * (1:6), prod_degree = 1:2) \r\n\r\nearth_tune <- \r\n  tune_grid(earth_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nStack the Models!\r\nOkay, we have fitted all the individual models. I will move on and\r\nfit a stack model.\r\nTo do so, I will use stack function to create an\r\nempty stack model and add on the individual fitted models earlier\r\non.\r\n\r\n\r\nstack_model <-\r\n  stacks() %>%\r\n  add_candidates(ranger_tune) %>%\r\n  add_candidates(xgboost_tune) %>%\r\n  add_candidates(earth_tune)\r\n\r\n\r\n\r\nNext, I will use blend_predictions function to find the coefficient\r\nof individual models.\r\n\r\n\r\nstack_model_pred <-\r\n  stack_model %>%\r\n  blend_predictions()\r\n\r\n\r\n\r\nTo see the weights of the models, we can call the object we have\r\ncreated in the previous step.\r\n\r\n\r\nstack_model_pred\r\n\r\n\r\n# A tibble: 3 x 3\r\n  member           type        weight\r\n  <chr>            <chr>        <dbl>\r\n1 xgboost_tune_1_1 boost_tree   0.458\r\n2 ranger_tune_1_1  rand_forest  0.409\r\n3 earth_tune_1_1   mars         0.217\r\n\r\nAs the results shown above, random forest, xgboost and mars are used\r\nto create the stacked model.\r\nWe can also use the autoplot function to visualize\r\nthe weight for different models.\r\n\r\n\r\nautoplot(stack_model_pred, type = \"weights\")\r\n\r\n\r\n\r\n\r\nOnce the weights of the different models are determined, I will\r\nprepare the fitted stack model by using fit_members\r\nfunction.\r\n\r\n\r\nstack_model_fit <- stack_model_pred %>%\r\n  fit_members()\r\n\r\n\r\n\r\nCompare the Model\r\nPerformance\r\nPerformance Metric of\r\nIndividual Models\r\nTo compare the model performance from different models, I will do the\r\nfollowing:\r\nFirst, finalize the workflow by selecting the best parameters\r\n& perform last_fit on the model\r\nThen, collect the model predictions by using\r\ncollect_predictions\r\nLastly, calculate the model performance, create a column named\r\n‘model’ and pivot the columns\r\nRanger metrics\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nranger_fit <- ranger_workflow %>%\r\n  finalize_workflow(select_best(ranger_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nranger_pred <- ranger_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nranger_metric <- model_metrics(ranger_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"ranger\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nxgboost_fit <- xgboost_workflow %>%\r\n  finalize_workflow(select_best(xgboost_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nxgboost_pred <- xgboost_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nxgboost_metric <- model_metrics(xgboost_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"xgboost\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nearth_fit <- earth_workflow %>%\r\n  finalize_workflow(select_best(earth_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nearth_pred <- earth_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nearth_metric <- model_metrics(earth_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"earth\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nPerformance Metric of Stack\r\nModel\r\nNext, I will calculate the model performance for the stack model.\r\n\r\n\r\nstack_metric <- predict(stack_model_fit, df_test) %>%\r\n  bind_cols(init_ult_diff = df_test$init_ult_diff) %>%\r\n  model_metrics(truth = init_ult_diff, estimate = .pred) %>%\r\n  mutate(model = \"stack\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nSummary\r\nNow I will combine all the results in a tibble table so that its\r\neasier to compare the performance metrics from different models.\r\n\r\n\r\ntibble() %>%\r\n  bind_rows(stack_metric) %>%\r\n  bind_rows(ranger_metric) %>%\r\n  bind_rows(xgboost_metric) %>%\r\n  bind_rows(earth_metric)\r\n\r\n\r\n# A tibble: 4 x 4\r\n  .estimator model    rmse   rsq\r\n  <chr>      <chr>   <dbl> <dbl>\r\n1 standard   stack   1592. 0.415\r\n2 standard   ranger  1614. 0.399\r\n3 standard   xgboost 1604. 0.406\r\n4 standard   earth   1702. 0.341\r\n\r\nAs shown above, both model performance metrics improved after we\r\nstacked the models.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page if you want\r\nto find out more on model stacking.\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\n\r\n\r\n\r\nFunda Gunes, Russ Wolfinger, and Pei-Yi Tan, eds. 2017. “Stacked\r\nEnsemble Models for Improved Prediction Accuracy.” http://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf.\r\n\r\n\r\nGeron, Aurelien, ed. 2019. Hands-on Machine Learning with\r\nScikit-Learn, Keras, and TensorFlow. O’Reilly Media.\r\n\r\n\r\nvas3k, ed. “Machine Learning for Everyone - in Simple Words. With\r\nReal-World Examples. Yes, Again.” https://vas3k.com/blog/machine_learning/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-23-model-stacking/image/cup_stack.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-22-data-wrangling/",
    "title": "Data Wrangling - The Quest to Tame The 'Beast'",
    "description": "Rawwwwwwwwww!",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "Data Wrangling"
    ],
    "contents": "\r\n\r\n\r\n\r\nArtwork on Data Wrangling by Allison Horst\r\nAs mentioned in my last post, it has been a long journey to learn\r\ndata science pretty much from scratch. I intend to share some of my\r\nlearning to help others to start their data science journey as well.\r\nTherefore, for my second post, I will share about data wrangling,\r\nwhich is often the essential task before building any machine learning\r\nmodels. Hence this post will serve as a building block of my future\r\nmachine learning posts.\r\nAlso, I intend to share some of the modern data science R package and\r\nhow these packages could benefit some of the analysis tasks.\r\nBackground\r\nIn R for Data\r\nScience book, Wickham & Grolemund mentioned that a typical data\r\nscience project would look as following:\r\n\r\n\r\n\r\nExtracted from R for Data Science\r\nAfter importing the data, the very first step usually involves some\r\nlevels of data wrangling. This is probably one of the most important\r\nsteps as without a good dataset, the analysis is unlikely to be\r\nmeaningful.\r\nGolden rule –> Garbage in, garbage out\r\nA good data exploratory will help to answer the following questions\r\n(not limited to):  - Does this dataset answer the business\r\nquestions (eg. does the dataset have the necessary info to predict which\r\ncustomers are likely to purchase in the next promotion?)?  - Are\r\nthere any issues that exist in the dataset (eg. missing value, excessive\r\ncategories, value range vary widely)?  - Any data transformation\r\nrequired? As some of the machine learning models can accept certain data\r\ntypes. \r\nThrough understanding the data, it will also give us some insights on\r\nwhether there is any more “gold” we can squeeze out from data through\r\nfeature engineering.\r\nIt will also provide us some clues whether the variable is a good\r\npredictor even without needing one to fit the model.\r\nThe conventional method of data wrangling is to perform data cleaning\r\nand transformation in Excel.\r\n\r\n\r\n\r\nPhoto by Mika Baumeister on Unsplash\r\nHowever, this method is not very sustainable/ideal due to the\r\nfollowing reasons:  - Excel has memory limitation and the\r\nsoftware would keep on crashing as the data size increases  -\r\nBecome a hurdle when we want to join different tables together or\r\nperform a transformation on the data  - More challenging to\r\nextract insights from the unstructured data (eg. perform text mining)\r\n\r\nIntroduction to Tidyverse\r\nWhen my professor first introduced this R package to me, I was amazed\r\nby what we could perform by using this package. And in fact, the more I\r\nuse it, the more I love it.\r\ntidyverse is a collection of R packages designed by\r\nRStudio.\r\n\r\n\r\n\r\nThe very awesome thing about tidyverse is all of the\r\npackages are following the tidy data concept.\r\nSo what the heck is tidy\r\ndata?\r\nTidy data is a concept on how to store/structure the data/results\r\nintroduced by Hadley Wickham.\r\n\r\n\r\n\r\nArtwork on Tidy Data by Allison Horst\r\nBelow are the definition of tidy data:  - Each variable must\r\nhave its column  - Each observation must have its row  -\r\nEach value must have its cell \r\n\r\n\r\n\r\nExtraction from R for Data Science\r\nbook\r\nWhy is tidy data awesome?\r\nThis is as simple as the time spent transforming the dataset/output\r\nis now lesser.\r\nWhy less time is required?\r\nThe functions are aligned on the input data formats they require.\r\nOften this allows us to ‘join’ the function together (also often\r\nknown as “pipe” in tidyverse context. Don’t worry about\r\nthe meaning of “pipe” for now, will be covering what I mean by “pipe”\r\nlater in this post).\r\nTherefore, as a user, I find that the learning curve of using\r\ntidyverse to perform data wrangling is less steep than other programming\r\nlanguages. This has effectively allowed me to spend more time on the\r\nanalysis itself.\r\n\r\n\r\n\r\nArtwork by Allison Horst\r\nEnough talking, let’s get our hands dirty.\r\nIllustration\r\nI will be using the motor insurance dataset taken from CASdataset\r\nas a demonstration. In this illustration, I am interested to find out\r\nhow the different profiles of the insured affected the total\r\npremium.\r\nPreparation of the\r\n“equipment”\r\nOver here, I call the necessary R packages by using a for-loop\r\nfunction.\r\n\r\n\r\npackage <- c('CASdatasets', 'tidyverse', 'skimr', 'funModeling', 'ggplot2', 'plotly', 'ggstatsplot')\r\n\r\nfor (p in package){\r\n  if(!require (p, character.only = TRUE)){\r\n    install(p)\r\n  }\r\n  library(p, character.only = TRUE)\r\n}\r\n\r\n\r\n\r\nThe code chunk will first check whether the relevant packages are\r\ninstalled in the machine.\r\nIf it is not installed, it will first install the required\r\npackage.\r\nAfter that, the relevant packages will be attached to the\r\nenvironment.\r\nCalling the “beast”\r\nAfter loading the relevant packages, I will indicate to R the dataset\r\nI want through data function as there are many datasets\r\nwithin CASdatasets package. I have renamed the dataset\r\nto df to shorten the name after attaching the dataset into the\r\nenvironment.\r\n\r\n\r\ndata(fremotor1prem0304a)\r\ndf <- fremotor1prem0304a\r\n\r\n\r\n\r\nNow, tame the “beast”\r\nConventionally, summary function is used to check\r\nthe quality of the data.\r\n\r\n\r\nsummary(df)\r\n\r\n\r\n    IDpol                Year         DrivAge      DrivGender\r\n Length:51949       Min.   :2003   Min.   :18.00   F:17794   \r\n Class :character   1st Qu.:2003   1st Qu.:31.00   M:34155   \r\n Mode  :character   Median :2003   Median :38.00             \r\n                    Mean   :2003   Mean   :39.84             \r\n                    3rd Qu.:2004   3rd Qu.:47.00             \r\n                    Max.   :2004   Max.   :97.00             \r\n                                                             \r\n    MaritalStatus     BonusMalus       LicenceNb    \r\n Cohabiting:10680   Min.   : 50.00   Min.   :1.000  \r\n Divorced  :  189   1st Qu.: 50.00   1st Qu.:1.000  \r\n Married   : 3554   Median : 57.00   Median :2.000  \r\n Single    : 2037   Mean   : 62.98   Mean   :1.885  \r\n Widowed   :  541   3rd Qu.: 72.00   3rd Qu.:2.000  \r\n NA's      :34948   Max.   :156.00   Max.   :7.000  \r\n                                                    \r\n        PayFreq                  JobCode          VehAge      \r\n Annual     :17579   Private employee: 9147   Min.   : 0.000  \r\n Half-yearly:28978   Public employee : 5045   1st Qu.: 4.000  \r\n Monthly    : 1486   Retiree         : 1035   Median : 7.000  \r\n Quarterly  : 3906   Other           :  856   Mean   : 7.525  \r\n                     Craftsman       :  637   3rd Qu.:10.000  \r\n                     (Other)         :  281   Max.   :89.000  \r\n                     NA's            :34948                   \r\n        VehClass        VehPower        VehGas     \r\n Cheapest   :17894   P10    :9148   Diesel :20615  \r\n Cheaper    :15176   P12    :8351   Regular:31334  \r\n Cheap      : 9027   P11    :8320                  \r\n Medium low : 5566   P9     :6671                  \r\n Medium     : 2021   P13    :6605                  \r\n Medium high: 1385   P8     :5188                  \r\n (Other)    :  880   (Other):7666                  \r\n                   VehUsage                           Garage     \r\n Private+trip to office:50435   Closed collective parking: 9820  \r\n Professional          : 1089   Closed zbox              :26318  \r\n Professional run      :  425   Opened collective parking: 8620  \r\n                                Street                   : 7191  \r\n                                                                 \r\n                                                                 \r\n                                                                 \r\n      Area                Region      Channel   Marketing \r\n A5     :15108   Center      :27486   A:30220   M1:26318  \r\n A3     :12696   Headquarters: 9788   B: 6111   M2: 8620  \r\n A2     : 7414   Paris area  : 7860   L:15618   M3: 9820  \r\n A7     : 6879   South West  : 6815             M4: 7191  \r\n A4     : 3779                                            \r\n A9     : 3397                                            \r\n (Other): 2676                                            \r\n PremWindscreen     PremDamAll         PremFire         PremAcc1    \r\n Min.   :  0.00   Min.   :   0.00   Min.   : 0.000   Min.   : 0.00  \r\n 1st Qu.: 13.00   1st Qu.:   0.00   1st Qu.: 0.000   1st Qu.: 0.00  \r\n Median : 22.00   Median :   0.00   Median : 4.000   Median : 0.00  \r\n Mean   : 25.78   Mean   :  83.06   Mean   : 4.421   Mean   :12.94  \r\n 3rd Qu.: 35.00   3rd Qu.: 144.00   3rd Qu.: 7.000   3rd Qu.:32.00  \r\n Max.   :264.00   Max.   :1429.00   Max.   :50.000   Max.   :77.00  \r\n                                                                    \r\n    PremAcc2       PremLegal        PremTPLM         PremTPLV     \r\n Min.   :  0.0   Min.   : 0.00   Min.   :  38.9   Min.   : 0.000  \r\n 1st Qu.:  0.0   1st Qu.: 8.00   1st Qu.: 102.6   1st Qu.: 5.000  \r\n Median :  0.0   Median :10.00   Median : 141.5   Median : 7.000  \r\n Mean   : 15.4   Mean   :10.43   Mean   : 167.7   Mean   : 8.571  \r\n 3rd Qu.: 45.0   3rd Qu.:12.00   3rd Qu.: 204.8   3rd Qu.:11.000  \r\n Max.   :198.0   Max.   :50.00   Max.   :1432.7   Max.   :68.000  \r\n                                                                  \r\n    PremServ        PremTheft         PremTot      \r\n Min.   :  0.00   Min.   :  0.00   Min.   :  91.0  \r\n 1st Qu.: 51.00   1st Qu.:  0.00   1st Qu.: 269.7  \r\n Median : 53.00   Median : 38.00   Median : 381.4  \r\n Mean   : 53.77   Mean   : 46.58   Mean   : 428.7  \r\n 3rd Qu.: 57.00   3rd Qu.: 68.00   3rd Qu.: 530.4  \r\n Max.   :237.00   Max.   :642.00   Max.   :3163.3  \r\n                                                   \r\n\r\nHowever, the data quality result shown under summary\r\nfunction is often not sufficient. Instead of just looking at the\r\nquantile, it’s also important to check other information, such as there\r\nis any missing value, what is the distribution of the variable look\r\nlike, what is coefficient variation, and so on.\r\nBelow are three examples of modern R functions to check the data\r\nquality:\r\nskim\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n30\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nfactor\r\n13\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nIDpol\r\n0\r\n1\r\n5\r\n13\r\n0\r\n32117\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nDrivGender\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 34155, F: 17794\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid:\r\n541\r\nPayFreq\r\n0\r\n1.00\r\nFALSE\r\n4\r\nHal: 28978, Ann: 17579, Qua: 3906, Mon:\r\n1486\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth:\r\n856\r\nVehClass\r\n0\r\n1.00\r\nFALSE\r\n9\r\nChe: 17894, Che: 15176, Che: 9027, Med:\r\n5566\r\nVehPower\r\n0\r\n1.00\r\nFALSE\r\n15\r\nP10: 9148, P12: 8351, P11: 8320, P9:\r\n6671\r\nVehGas\r\n0\r\n1.00\r\nFALSE\r\n2\r\nReg: 31334, Die: 20615\r\nVehUsage\r\n0\r\n1.00\r\nFALSE\r\n3\r\nPri: 50435, Pro: 1089, Pro: 425\r\nGarage\r\n0\r\n1.00\r\nFALSE\r\n4\r\nClo: 26318, Clo: 9820, Ope: 8620, Str:\r\n7191\r\nArea\r\n0\r\n1.00\r\nFALSE\r\n10\r\nA5: 15108, A3: 12696, A2: 7414, A7:\r\n6879\r\nRegion\r\n0\r\n1.00\r\nFALSE\r\n4\r\nCen: 27486, Hea: 9788, Par: 7860, Sou:\r\n6815\r\nChannel\r\n0\r\n1.00\r\nFALSE\r\n3\r\nA: 30220, L: 15618, B: 6111\r\nMarketing\r\n0\r\n1.00\r\nFALSE\r\n4\r\nM1: 26318, M3: 9820, M2: 8620, M4:\r\n7191\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nprofiling_num\r\n\r\n\r\nprofiling_num(df)\r\n\r\n\r\n         variable        mean     std_dev variation_coef   p_01\r\n1            Year 2003.381759   0.4858226   0.0002425013 2003.0\r\n2         DrivAge   39.835146  11.8652020   0.2978576243   21.0\r\n3      BonusMalus   62.983330  15.3002405   0.2429252393   50.0\r\n4       LicenceNb    1.884733   0.6664325   0.3535951452    1.0\r\n5          VehAge    7.525477   4.7817572   0.6354091975    0.0\r\n6  PremWindscreen   25.779053  20.4951432   0.7950308962    0.0\r\n7      PremDamAll   83.055285 105.7145996   1.2728220676    0.0\r\n8        PremFire    4.421317   4.4846822   1.0143317368    0.0\r\n9        PremAcc1   12.942001  16.7326001   1.2928912922    0.0\r\n10       PremAcc2   15.400219  23.8030155   1.5456283309    0.0\r\n11      PremLegal   10.426399   3.9025924   0.3742991579    4.0\r\n12       PremTPLM  167.737897  96.9041558   0.5777117614   51.8\r\n13       PremTPLV    8.570521   5.2053077   0.6073501958    0.0\r\n14       PremServ   53.770852   5.1332147   0.0954646327   42.0\r\n15      PremTheft   46.584169  48.7219715   1.0458911785    0.0\r\n16        PremTot  428.687713 224.5838408   0.5238868156  129.1\r\n     p_05   p_25   p_50   p_75   p_95     p_99  skewness  kurtosis\r\n1  2003.0 2003.0 2003.0 2004.0 2004.0 2004.000 0.4867707  1.236946\r\n2    24.0   31.0   38.0   47.0   63.0   73.000 0.8090189  3.339693\r\n3    50.0   50.0   57.0   72.0   93.0  106.000 1.1457567  3.756333\r\n4     1.0    1.0    2.0    2.0    3.0    4.000 0.9380943  5.925007\r\n5     1.0    4.0    7.0   10.0   15.0   21.000 1.5128291 15.246811\r\n6     0.0   13.0   22.0   35.0   65.0   92.000 1.5801010  8.575197\r\n7     0.0    0.0    0.0  144.0  283.0  420.000 1.5981568  7.474360\r\n8     0.0    0.0    4.0    7.0   13.0   20.000 1.7369281  9.079772\r\n9     0.0    0.0    0.0   32.0   39.0   44.000 0.5764758  1.461663\r\n10    0.0    0.0    0.0   45.0   57.0   64.000 0.9548824  2.088081\r\n11    5.0    8.0   10.0   12.0   17.0   23.000 1.3283374  6.830506\r\n12   68.0  102.6  141.5  204.8  352.0  512.100 2.1710206 11.739705\r\n13    3.0    5.0    7.0   11.0   18.0   26.000 1.9178789 10.133990\r\n14   46.0   51.0   53.0   57.0   62.0   68.000 1.8713567 51.639566\r\n15    0.0    0.0   38.0   68.0  136.0  216.000 1.9999025 11.271121\r\n16  169.8  269.7  381.4  530.4  852.3 1187.504 1.7016729  8.533766\r\n     iqr          range_98         range_80\r\n1    1.0      [2003, 2004]     [2003, 2004]\r\n2   16.0          [21, 73]         [27, 56]\r\n3   22.0         [50, 106]         [50, 85]\r\n4    1.0            [1, 4]           [1, 3]\r\n5    6.0           [0, 21]          [2, 14]\r\n6   22.0           [0, 92]          [0, 52]\r\n7  144.0          [0, 420]         [0, 221]\r\n8    7.0           [0, 20]          [0, 10]\r\n9   32.0           [0, 44]          [0, 36]\r\n10  45.0           [0, 64]          [0, 53]\r\n11   4.0           [4, 23]          [6, 15]\r\n12 102.2     [51.8, 512.1]    [78.8, 290.8]\r\n13   6.0           [0, 26]          [4, 15]\r\n14   6.0          [42, 68]         [48, 60]\r\n15  68.0          [0, 216]         [0, 106]\r\n16 260.7 [129.1, 1187.504] [199.48, 713.12]\r\n\r\nstatus\r\n\r\n\r\nstatus(df)\r\n\r\n\r\n                     variable q_zeros      p_zeros  q_na      p_na\r\nIDpol                   IDpol       0 0.0000000000     0 0.0000000\r\nYear                     Year       0 0.0000000000     0 0.0000000\r\nDrivAge               DrivAge       0 0.0000000000     0 0.0000000\r\nDrivGender         DrivGender       0 0.0000000000     0 0.0000000\r\nMaritalStatus   MaritalStatus       0 0.0000000000 34948 0.6727367\r\nBonusMalus         BonusMalus       0 0.0000000000     0 0.0000000\r\nLicenceNb           LicenceNb       0 0.0000000000     0 0.0000000\r\nPayFreq               PayFreq       0 0.0000000000     0 0.0000000\r\nJobCode               JobCode       0 0.0000000000 34948 0.6727367\r\nVehAge                 VehAge    1712 0.0329553986     0 0.0000000\r\nVehClass             VehClass       0 0.0000000000     0 0.0000000\r\nVehPower             VehPower       0 0.0000000000     0 0.0000000\r\nVehGas                 VehGas       0 0.0000000000     0 0.0000000\r\nVehUsage             VehUsage       0 0.0000000000     0 0.0000000\r\nGarage                 Garage       0 0.0000000000     0 0.0000000\r\nArea                     Area       0 0.0000000000     0 0.0000000\r\nRegion                 Region       0 0.0000000000     0 0.0000000\r\nChannel               Channel       0 0.0000000000     0 0.0000000\r\nMarketing           Marketing       0 0.0000000000     0 0.0000000\r\nPremWindscreen PremWindscreen    7153 0.1376927371     0 0.0000000\r\nPremDamAll         PremDamAll   26087 0.5021655855     0 0.0000000\r\nPremFire             PremFire   14450 0.2781574236     0 0.0000000\r\nPremAcc1             PremAcc1   32183 0.6195114439     0 0.0000000\r\nPremAcc2             PremAcc2   36379 0.7002829698     0 0.0000000\r\nPremLegal           PremLegal       2 0.0000384993     0 0.0000000\r\nPremTPLM             PremTPLM       0 0.0000000000     0 0.0000000\r\nPremTPLV             PremTPLV    1353 0.0260447747     0 0.0000000\r\nPremServ             PremServ       2 0.0000384993     0 0.0000000\r\nPremTheft           PremTheft   14654 0.2820843520     0 0.0000000\r\nPremTot               PremTot       0 0.0000000000     0 0.0000000\r\n               q_inf p_inf      type unique\r\nIDpol              0     0 character  32117\r\nYear               0     0   numeric      2\r\nDrivAge            0     0   numeric     72\r\nDrivGender         0     0    factor      2\r\nMaritalStatus      0     0    factor      5\r\nBonusMalus         0     0   numeric     68\r\nLicenceNb          0     0   numeric      7\r\nPayFreq            0     0    factor      4\r\nJobCode            0     0    factor      7\r\nVehAge             0     0   numeric     49\r\nVehClass           0     0    factor      9\r\nVehPower           0     0    factor     15\r\nVehGas             0     0    factor      2\r\nVehUsage           0     0    factor      3\r\nGarage             0     0    factor      4\r\nArea               0     0    factor     10\r\nRegion             0     0    factor      4\r\nChannel            0     0    factor      3\r\nMarketing          0     0    factor      4\r\nPremWindscreen     0     0   numeric    177\r\nPremDamAll         0     0   numeric    612\r\nPremFire           0     0   numeric     49\r\nPremAcc1           0     0   numeric     45\r\nPremAcc2           0     0   numeric     63\r\nPremLegal          0     0   numeric     43\r\nPremTPLM           0     0   numeric   1097\r\nPremTPLV           0     0   numeric     58\r\nPremServ           0     0   numeric     70\r\nPremTheft          0     0   numeric    377\r\nPremTot            0     0   numeric   9140\r\n\r\nThese functions show more info than just quantile. They also show\r\ninfo such as:  - Number of unique categories under categorical\r\nvariables  - The proportion of missing values in the data \r\n- Standard deviation of the numeric variables \r\nBelow are some insights we could draw from the functions above:\r\n\r\nExcessive missing value (i.e. more than half of the values are\r\nmissing) in MaritalStatus & JobCode. More than 65% of the data from\r\nthese two columns have missing values. So, it doesn’t like the variables\r\nwill yield meaningful results if they are used to build machine learning\r\nmodels \r\n\r\n\r\ndf %>% dplyr::select(MaritalStatus, JobCode) %>% skim()\r\n\r\n\r\nTable 2: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n2\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid:\r\n541\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth:\r\n856\r\n\r\nAbout 15 unique categories under VehPower. Might not ideal to keep\r\nso many unique categories within the variables as it might create noise\r\nwhen fitting machine learning models \r\n\r\n\r\ndf %>% group_by(VehPower) %>% tally()\r\n\r\n\r\n# A tibble: 15 x 2\r\n   VehPower     n\r\n   <fct>    <int>\r\n 1 P10       9148\r\n 2 P11       8320\r\n 3 P12       8351\r\n 4 P13       6605\r\n 5 P14       3773\r\n 6 P15       1510\r\n 7 P16        720\r\n 8 P17         58\r\n 9 P2          15\r\n10 P4          46\r\n11 P5         597\r\n12 P6           2\r\n13 P7         945\r\n14 P8        5188\r\n15 P9        6671\r\n\r\nAlso, note that the count of some categories is relatively low.\r\nPerhaps it is better to group these ‘low count categories’ since they\r\nare unlikely to have a significant impact on the predicted results.\r\nMaximum BonusMalus can go up to 156. According to the data\r\ndictionary, <100 means bonus, >100 means malus. Hence, this is\r\nokay. \r\n\r\n\r\ndf %>% dplyr::select(BonusMalus) %>% skim()\r\n\r\n\r\nTable 3: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.3\r\n50\r\n50\r\n57\r\n72\r\n156\r\n▇▂▁▁▁\r\n\r\nInterestingly, the premium for different benefits can range very\r\nwidely. This is probably due to the difference in the respective burning\r\ncost under each benefit. \r\n\r\n\r\ndf %>% dplyr::select(starts_with(\"Prem\")) %>% skim()\r\n\r\n\r\nTable 4: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nI prefer skim function as its output is in a nice\r\n& neat format. It also provides most of the crucial info one is\r\nlooking for in checking the data quality.\r\nIf I am only interested in checking the data quality for the numeric\r\nvariables, this can be found by piping the variables together.\r\nFirst, I have specified the dataset. Next, I use\r\nselect_if function to select all the numeric columns\r\nbefore using skim function to skim through the\r\ndataset.\r\n\r\n\r\ndf %>% \r\n  select_if(is.numeric) %>%\r\n  skim()\r\n\r\n\r\nTable 5: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nThe piping method is made possible because the functions are\r\nfollowing the tidy data concept, which makes it easier to “play” with\r\nthe data.\r\nOften, visualizing data might provide us unexpected insights. The\r\ndata can “tell” us underlying/hidden stories by leveraging on the power\r\nof data visualization.\r\nFor example, I would like to find out the scatterplot between PremTot\r\nagainst the selected numeric variables.\r\n\r\n\r\nnum_list <- c(\"Year\", \"DrivAge\", \"BonusMalus\", \"LicenceNb\", \"VehAge\")\r\n\r\n\r\nfor (i in num_list){\r\n  print(ggplot(df, aes(x = get(i), y = log(PremTot))) +\r\n      geom_point() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nThe graphs show that the data type for Year and LicenceNb is\r\nincorrect. They are supposed to read as factors, instead of numeric\r\nvariables.\r\nTo fix this, I will use mutate and\r\nfactor function to transform the data into the correct\r\ndata type.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(Year = factor(Year),\r\n         LicenceNb = factor(LicenceNb))\r\n\r\n\r\n\r\nAlternatively, sometimes we just want to find out the graphs of a\r\nlist of variables, instead of a small subset of variables. By typing\r\ndown all the variables names can be a hassle and prone to human\r\nerror.\r\nThis is where the different R functions come to the “rescue”.\r\nFor example, I am interested to find out the boxplot of all the\r\nfactor variables against PremTot. The code chunk below has shown how we\r\ncould pipe the different functions together to select all the factor\r\ncolumns and extract out the column names as a list.\r\n\r\n\r\ncat_list <- df_1 %>%\r\n  select_if(is.factor) %>%\r\n  names()\r\n\r\n\r\n\r\nSubsequently, I will use for loop to plot the necessary graphs. I\r\nwill explain the awesome-ness of ggplot function in my\r\nfuture post.\r\n\r\n\r\nfor (i in cat_list){\r\n  print(ggplot(df_1, aes(x = get(i), y = log(PremTot))) +\r\n      geom_boxplot() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nBelow are some of the insights drawn from the graphs above (not\r\nlimited to):  - Average PremTot for Retailer is higher than the\r\nrest  - Average PremTot also varies significantly across\r\ndifferent VehPower  - Somehow the premium for a diesel car is\r\nhigher than a regular car  - PremTot for professional &\r\nprofessional run is higher than private+trip to office \r\nConclusion\r\nOkay, that’s all the sharing for this post!\r\nI have shown the awesome-ness of tidyverse through\r\nthis post. There are many more functions within tidyverse universe, which\r\nare not covered in this post. Do check out their website for many more awesome\r\nfunctions!\r\nFeel free to contact me through email or LinkedIn if\r\nyou have any suggestions on future topics to share.\r\nRefer to this link for the blog\r\ndisclaimer.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-22-data-wrangling/image/data_cowboy.png",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-my-data-science-journey/",
    "title": "The Unconventional Path",
    "description": "Journey of Joining the \"Dark\" Side",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nPhoto taken in Taipei\r\nDuring a stormy night in 2018, it was raining cats and dogs outside. Suddenly there was thunder and lightning. Jeng jeng jeng, I suddenly decided to take up a master degree to study data science.\r\nNah, I always wish it was that simple to make such a big decision in life.\r\nIt took me about 5 years to decide that I would go back to university for a data science degree.\r\nIt took me 2.5 years to have the courage in taking the leap to go for a change in job function for the 1st time. \r\nWas I worried or scared when I made these choice? Oh absolutely.\r\nUnconventional Path - Work After working on actuarial pricing for more than 4 years, I have asked for a change in job function, which I have chickened out twice times after the requests. There were a lot of ‘what-if’ in my head.\r\n“What if I don’t like the new function?”\r\n“What if I struggle to understand the new concepts?”\r\n“What if I screwed up?”\r\n“What if I get along with them?”\r\nHowever, on the third time, I came to the conclusion that if I don’t take the leap of faith, I would never step out from my comfort zone.\r\nMeanwhile, unfortunately sometimes life is not a bed of roses. Even if it is a bed of roses, it is also a bed of roses with full of thorns.\r\nInitially, things were not as smooth as what I wanted. Team mates left one after another one and things were pretty messy. It was through these tough times I discovered my strength, how I could play my strength and make a difference.\r\nFast forward to 1.5 years later, I was being offered to change my job scope again at my work. However, this change in job scope was even more drastic. I was being offered to change my job scope from life insurance to general insurance.\r\nAm I scared? Still as worried as usual.\r\nHowever, I decided that I would regret if I did not take the leap of faith again. My industry friends were shocked, they even checked with me to verify the news. Deep down I knew that worry does not solve anything. So, I worked my a** hard to put in the extra effort to understand the fundamental concepts for general insurance.\r\nWithin less than 3 months after joining the team, I was asked to cover group insurance while my colleague was on maternity leave. At the moment, I was worried as my main expertise was never in short term insurance business and I was expecting to guide one of the junior on this piece of work. Despite of the hiccups at the beginning, things turned out better than what I initially imagined.\r\nThese past success have nevertheless given me more confidence in myself.\r\nUnconventional Path - Data Science Journey I first discovered my interest for data science back in my undergraduate studies. I still remember it was generalized linear model project on Titanic dataset that sparked my interest.\r\n\r\n\r\n\r\nScreenshot of my GLM report\r\nAfter I started working, I never forget my interest in this area. However, as life goes on, I was not sure this data science was for me since actuarial science and data science are still different (in terms of concepts and work) although they are somewhat similar.\r\nUntil few years ago, I was once exposed a bit on data science at work again. That re-kindled my interest for data science. I knew that it is not going to work if I rely others for my learning, eg. waiting to learn from work, waiting for others to teach me and so on. Eventually, I decided to go back into university to study data science.\r\nThroughout these processes, I have encountered many “Why” from myself and others.\r\n“Why would you still want to study, given that you have been studying for your actuarial exams after graduating from university?”\r\n“Why are you still studying this when you hold an actuarial science degree?”\r\n“Why are you studying when your working hours can be quite bad?”\r\n“Why are you spending so much money on getting another degree? Are you sure you can earn back the money?”\r\n“Just why??”\r\nI knew it is going to tough and tiring to work full time + study part time, although I was complaining that I did not its going to be this tiring.\r\nLooking back now, did I regret anything? Absolutely no.\r\nIn fact, I met some of my “gui ren” (ie. people who are great help to one’s life) through my this journey. Prof Kam Tin Seong has spent so much time with me guiding me through my capstone project, explaining the concepts of data science techniques (eg. how to do proper clustering, considerations when building a model), making me working very hard for the weekly data visualization makeover and many more. Another of my gui ren has to be Prof Wong Yuet Nan. The valuable advice and experience he shared with me has built up my knowledge and confidence.\r\nWas I stressed during these process? Super duper stressed. I was so so stressed out until there were days my brain couldn’t think or function.\r\nDeep down I knew what are opportunity cost and price I am paying, but I am willing to going through the pain. Sleeping at 2am almost every day, occasionally running back to work after class ended at 10pm and fell asleep at random places due to lack of enough sleep are really nothing when I compared to these valuable experience and confidence I have gained.\r\nWould I trade these experience for anything? Definitely no.\r\nTakeaway Do I have the certainty of what is going to happen next when I was making the above decisions? Nope, often I am not sure how things will work out.\r\nDo I want to have the certainty in these situations? Oh absolutely yes. However, I knew life does not work this way. I would miss out so much fun if I demand on the certainty.\r\nIt was taking these seemed “unconventional paths” that provided me the opportunities to discover amazing things. Just like the the photo shown below, it was taken with no filter during my Japan trip. Instead of walking along the main road like what other tourists were doing, I followed another path that is less taken and discovered this awesome view.\r\n\r\n\r\n\r\nPhoto taken in Hakone, Japan\r\nNevertheless, this is my story of unconventional path.\r\nLife is short, so go & live a life with no regret! :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-my-data-science-journey/image/journey.jpg",
    "last_modified": "2022-12-27T22:23:35+08:00",
    "input_file": {}
  }
]
