[
  {
    "path": "posts/2022-02-XX-glm/",
    "title": "Generalized Linear Model",
    "description": "Back to Actuaries Most Beloved Model",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2022-02-15",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nGeneralized Linear Models (GLM)\r\nAssumptions\r\nFamily\r\nOffset function\r\nPros and cons of using GLM models\r\n\r\nApplication of GLM models\r\nDemonstration\r\nSetup the environment\r\nDataset\r\nExplotary Data Analysis (EDA)\r\nModel Building\r\nData Splitting\r\nBuilding Model\r\n\r\nCross Validation\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring actuaries’ most beloved model, generalized linear model (GLM).\r\n\r\n\r\n\r\nPhoto by Jason Leung on Unsplash\r\nThis will set the scene for future sharing on other topics, such as generalized additive model, frequency severity modeling and so on.\r\nGeneralized Linear Models (GLM)\r\nGLM is an extension of linear model.\r\nIn linear model, we assume the relationship between independent and dependent variables are linear.\r\nHowever, this assumption is not appropriate in most of the scenarios (eg. insurance premium setting). This is where GLM could help us in our modeling work.\r\nThe idea of using GLM is that with appropriate transformation on the target variable, the relationship between independent variables and transformed dependent variable remain linear.\r\nhttps://statisticaloddsandends.wordpress.com/tag/glmnet/\r\n[@] A GLM model consists of following 3 parts:\r\nLinear predictor \\(\\eta_i=\\beta_0+\\beta^Tx_i\\)\r\nLink function \\(\\eta_i=g(\\mu_i)\\)\r\nRandom component \\(y_i\\sim f(y|\\mu_i)\\)\r\nwhere the general form of GLM formula can be found below:\r\n\\[y_i=\\beta^Tx_i+\\epsilon_i\\]\r\nAssumptions\r\nBelow are some of the assumptions when using GLM models:\r\nThe predictors (a.k.a. independent variables) are independent from one another\r\nThe dependent variable is assumed to be one of the distribution from an exponential family (eg. normal, Poisson, binomial etc)\r\nErrors are independent from one another\r\nThe model performance might be affected if any of the assumptions is being violated.\r\nFamily\r\nBelow are some of the families supported under GLM:\r\nGaussian\r\nBinomial\r\nPoisson\r\nMultinomial\r\nCox\r\nTweedie\r\n[@] summarized down the appropriate families for the different predictions we are trying to make. This is the link to the book.\r\nOffset function\r\n[@] The offset enters the score in an additive way.\r\nThis post provides a clear example on how the offset function works in a Poisson regression context.\r\nThen you might be asking instead of passing feature into offset function, why can’t we just divide the target variable by offset variable?\r\nhttps://towardsdatascience.com/offsetting-the-model-logic-to-implementation-7e333bc25798\r\nThe short answer is the likelihood function would change if we divide the target variable by using the offset variable. [@] provides the mathematical proof on how the likelihood would change if we divide the target variable by the offset variable.\r\nOne important thing to note when using offset is offset should be in the same scale as the target variable. For example, when using Poisson family, the default link function is log. The offset variable should be log as well.\r\nNote that offset function should not be confused with weight function in GLM.\r\nIn short, following are the differences between offset & weight:\r\nOffset is to adjust the target so that the targets of the different data points are in same unit of measurements\r\nWeight is to assign different importance to some observations than the rest\r\nPros and cons of using GLM models\r\nOf course there is not perfect model. Sometimes the advantages of the models bring could be disadvantages of using the relevant models.\r\nFollowing are advantages and disadvantages of using GLM models:\r\nAdvantages of using GLM\r\nDoes not assume the relationship between independent and dependent variables is linear\r\nEasier to interpret than some other models, such as neural network\r\nLess resistant from the business users since GLM can produce rating factors, which has been widely used\r\nDisadvantages of using GLM\r\nSensitive to outliers\r\nUnable to handle categorical variables\r\nModel performance will be affected if any of the model assumptions mentioned above is being violated\r\nApplication of GLM models\r\nEffective statistical learning methods for actuaries I_GLMs and extensions\r\n[@] GLMs were originally introduced to improve the accuracy of motor insurance pricing. Today GLM is being used in many insurance contexts.\r\n[@] also listed some of the typical insurance use case by using GLMs in the book:\r\nClaim count modeling\r\nClaim amount modeling\r\nGraduation of mortality and morbidity rates\r\nElasticity modeling\r\nLoss reserving\r\nCompetitive analysis\r\nNevertheless, let’s start the demonstration!\r\n\r\n\r\n\r\nPhoto by Clemens van Lay on Unsplash\r\nDemonstration\r\nSetup the environment\r\nIn this demonstration, I will be using glmnet package to build the GLM model. The documentation can be found under this link.\r\nHowever, it doesn’t seem like we could use tidymodels to wrap around glmnet. This is because according to this issue ticket, it seems like tidymodels is unable to support offset function for time being.\r\nglmnet requires the users to specify the variable to be used as offset in both training and testing datasets. However, tidymodels only allows the users to specify offset in training dataset.\r\nI will call the relevant packages to setup the environment.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'tidymodels', 'corrplot', 'glmnet')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nDataset\r\nFor the demonstration, I will be using the car insurance dataset from Chapter 1 of Predictive Modeling Applications in Actuarial Science - Volume 1.\r\nThe dataset can be found from the book website.\r\n\r\n\r\n\r\nPhoto by why kei on Unsplash\r\nI will first import the data into environment.\r\nI will also specify that the year of driving licence should be imported as character, instead of continuous variable.\r\n\r\n\r\ndf <- read_csv(\"data/sim-modeling-dataset.csv\",\r\n               col_types = list(yrs.lic = col_character()))\r\n\r\n\r\n\r\nTo keep the post short and sweet, I will not put the EDA results in this post. Nevertheless, based on the EDA outcome, this is a rather clean data to work with since there is no missing value.\r\nAlso, I will be building a model to predict the claim count from the various policies.\r\nExplotary Data Analysis (EDA)\r\nBefore building the model, I will perform some simple EDA to perform simple data cleaning.\r\nI will first select all the numeric variables.\r\n\r\n\r\ndf_num <- df %>%\r\n  select_if(is.numeric)\r\n\r\n\r\n\r\nThen I will pass them into corrplot function.\r\n\r\n\r\ncorrplot(cor(df_num, use=\"pairwise.complete.obs\"), \r\n         method = \"number\", \r\n         type = \"upper\", \r\n         tl.cex = 0.65, \r\n         number.cex = 0.65, \r\n         diag = FALSE)\r\n\r\n\r\n\r\n\r\nNote that there are some variables are perfectly correlated.\r\nBased on the variable names, it seems like these variables are measuring the same things.\r\nHence I will remove the highly correlated variable.\r\n\r\n\r\ndf_1 <- df %>%\r\n  select(-c(year, drv.age, veh.age))\r\n\r\n\r\n\r\nAlso, GLM is unable to handle non-numeric variables.\r\nTo overcome this, I have used to the step_dummy function in recipe package to encode the non-numeric variables by using one hot encoding method. all_nominal_predictiors function will tell the function to identify all the nominal independent variables in the dataset.\r\nThen I will pass the recipe into the prep function. The current dataset will then be passed into bake function to generate out the pre-processed data.\r\n\r\n\r\ndf_recoded <- df_1 %>%\r\n  recipe(clm.count ~ .) %>%\r\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%\r\n  prep() %>%\r\n  bake(df_1)\r\n\r\n\r\n\r\nModel Building\r\nData Splitting\r\nNext, I will split the dataset into training and testing dataset.\r\n\r\n\r\ndf_split <- initial_split(df_recoded, prop = 0.6, strata = clm.count)\r\n\r\n\r\n\r\nAs usual, I will use training and testing function to crave the dataset into training and testing dataset.\r\n\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nBuilding Model\r\nglmnet requires the independent variables to be in matrix format. Hence I will pass the object into data.matrix to convert the tibble table into matrix format.\r\n\r\n\r\n# predictors need to be in matrix format\r\nx <- df_train %>%\r\n  select(-c(clm.count, exposure)) %>%\r\n  data.matrix()\r\n\r\n\r\n\r\nSimilarly, I will “cut out” the target variable from the training dataset.\r\n\r\n\r\ny <- df_train$clm.count\r\n\r\n\r\n\r\nI will also convert the test dataset into matrix format.\r\n\r\n\r\nnewx <- df_test %>%\r\n  select(-c(clm.count, exposure)) %>%\r\n  data.matrix()\r\n\r\n\r\n\r\nNext, I will start building the model.\r\nAs I am trying to predict the claim count, hence I will specify Poisson to be the family in the GLM function.\r\n\r\n\r\nglmnet_model <- glmnet(x, \r\n                       y, \r\n                       family = \"poisson\", \r\n                       offset = log(df_train$exposure))\r\n\r\n\r\n\r\nAs in the data, we are not told whether the claim counts and claim amounts are being adjusted for the exposure. Hence, I will assume the claim counts and claim amounts are not adjusted for the exposure of the policies.\r\nTo adjust for the exposure, I will specify what should be the “offset” in the model.\r\ntidy function from tidymodels package can be used to convert the output into tidy data format so that its easier to read and perform data wrangling.\r\n\r\n\r\nglmnet_model %>% tidy()\r\n\r\n\r\n# A tibble: 1,408 x 5\r\n   term         step estimate lambda dev.ratio\r\n   <chr>       <dbl>    <dbl>  <dbl>     <dbl>\r\n 1 (Intercept)     1    -1.80 0.178   6.06e-14\r\n 2 (Intercept)     2    -1.82 0.162   7.19e- 2\r\n 3 (Intercept)     3    -1.84 0.148   1.01e- 1\r\n 4 (Intercept)     4    -1.85 0.135   1.16e- 1\r\n 5 (Intercept)     5    -1.87 0.123   1.26e- 1\r\n 6 (Intercept)     6    -1.88 0.112   1.33e- 1\r\n 7 (Intercept)     7    -1.89 0.102   1.38e- 1\r\n 8 (Intercept)     8    -1.90 0.0929  1.41e- 1\r\n 9 (Intercept)     9    -1.90 0.0846  1.44e- 1\r\n10 (Intercept)    10    -1.91 0.0771  1.46e- 1\r\n# ... with 1,398 more rows\r\n\r\nCross Validation\r\n\r\n\r\nalpha_list <- c(0, 0.1, 0.5, 1)\r\n\r\n\r\nfor (i in alpha_list){\r\n  cv_glmnet <- cv.glmnet(x, \r\n                       y, \r\n                       family = \"poisson\",\r\n                       intercept = FALSE,\r\n                       type.measure = \"deviance\", \r\n                       alpha = i,\r\n                       nfolds = 20, \r\n                       offset = log(df_train$exposure))\r\n\r\n  print(cv_glmnet)\r\n}\r\n\r\n\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n    Lambda Index Measure       SE Nonzero\r\nmin 0.8218   100  0.3551 0.005184      45\r\n1se 0.9898    98  0.3593 0.005263      45\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n     Lambda Index Measure       SE Nonzero\r\nmin 0.00822   100   0.339 0.004259      32\r\n1se 0.10131    73   0.343 0.003962       6\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n     Lambda Index Measure       SE Nonzero\r\nmin 0.00198    98  0.3387 0.006235      28\r\n1se 0.03541    67  0.3447 0.005642       3\r\n\r\nCall:  cv.glmnet(x = x, y = y, offset = log(df_train$exposure), type.measure = \"deviance\",      nfolds = 20, family = \"poisson\", intercept = FALSE, alpha = i) \r\n\r\nMeasure: Poisson Deviance \r\n\r\n      Lambda Index Measure       SE Nonzero\r\nmin 0.000822   100  0.3396 0.006300      30\r\n1se 0.019430    66  0.3454 0.005917       3\r\n\r\nwe can pull out the coefficient by calling coef function.\r\n\r\n\r\ncoef(cv_glmnet, s = \"lambda.min\")\r\n\r\n\r\n46 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                                   s1\r\n(Intercept)              .           \r\nrow.id                   2.474225e-06\r\ndriver.age              -2.331096e-03\r\nyrs.licensed            -5.507775e-02\r\nncd.level               -9.183146e-02\r\nregion                   .           \r\nvehicle.age             -3.781105e-02\r\nvehicle.value           -1.719751e-02\r\nseats                    .           \r\nccm                      .           \r\nhp                       5.760427e-03\r\nweight                  -1.840155e-04\r\nlength                   .           \r\nwidth                    2.436615e-01\r\nheight                  -9.420336e-01\r\nprior.claims             1.089927e-01\r\nclm.incurred             5.685927e-04\r\nnb.rb_NB                 .           \r\nnb.rb_RB                -7.308259e-02\r\ndriver.gender_Female    -9.442518e-02\r\ndriver.gender_Male       .           \r\nmarital.status_Divorced  .           \r\nmarital.status_Married   .           \r\nmarital.status_Single   -6.850856e-02\r\nmarital.status_Widow     2.185257e-01\r\nyrs.lic_X1               2.874141e-04\r\nyrs.lic_X2               .           \r\nyrs.lic_X3              -1.154329e-02\r\nyrs.lic_X4               6.905822e-02\r\nyrs.lic_X5               1.949079e-02\r\nyrs.lic_X6              -1.288704e-01\r\nyrs.lic_X7              -2.232137e-02\r\nyrs.lic_X8              -4.090729e-01\r\nyrs.lic_X9               .           \r\nyrs.lic_X9.              .           \r\nbody.code_A              .           \r\nbody.code_B             -4.860346e-01\r\nbody.code_C              6.565414e-02\r\nbody.code_D              4.798269e-03\r\nbody.code_E             -9.705319e-03\r\nbody.code_F              .           \r\nbody.code_G              1.023419e-01\r\nbody.code_H              5.353576e-02\r\nfuel.type_Diesel         .           \r\nfuel.type_Gasoline      -1.083279e-01\r\nfuel.type_LPG            .           \r\n\r\n\r\n\r\nggplot(df, aes(x = factor(clm.count), y = log(height))) +\r\n  geom_violin(draw_quantiles = c(0.5))\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(df, aes(prior.claims)) +\r\n  geom_histogram(aes(group = clm.count, fill = clm.count, alpha = 0.15)) #+\r\n\r\n\r\n\r\n  #facet_wrap(~clm.count)\r\n\r\n\r\n\r\nNumber of prior.claim definitely looks weird.\r\n\r\n\r\ncv_glmnet_tidy <- tidy(cv_glmnet)\r\n\r\ncv_glmnet_tidy\r\n\r\n\r\n# A tibble: 100 x 6\r\n   lambda estimate std.error conf.low conf.high nzero\r\n    <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <int>\r\n 1   8.22    0.955   0.00305    0.952     0.959     0\r\n 2   7.49    0.894   0.00289    0.891     0.896     1\r\n 3   6.82    0.837   0.00271    0.835     0.840     1\r\n 4   6.22    0.787   0.00258    0.785     0.790     1\r\n 5   5.66    0.742   0.00250    0.740     0.745     1\r\n 6   5.16    0.702   0.00246    0.700     0.705     1\r\n 7   4.70    0.666   0.00247    0.664     0.669     1\r\n 8   4.28    0.635   0.00252    0.632     0.637     1\r\n 9   3.90    0.606   0.00260    0.604     0.609     1\r\n10   3.56    0.581   0.00270    0.579     0.584     1\r\n# ... with 90 more rows\r\n\r\n\r\n\r\nglance(glmnet_model)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  nulldev npasses  nobs\r\n    <dbl>   <int> <int>\r\n1   9991.     518 24456\r\n\r\nAs I would like to have the predicted values, I will pass “response” to the type augment. https://cran.r-project.org/web/packages/glmnet/glmnet.pdf\r\noffset needs to be specified in the predict function again based on the documentation. https://www.r-bloggers.com/2019/01/a-deep-dive-into-glmnet-offset/\r\n\r\n\r\nglmnet_predict <- predict(glmnet_model, \r\n                          type = \"response\", \r\n                          newx = newx, \r\n                          newoffset = log(df_test$exposure))\r\n\r\n\r\n\r\nNote that without cross validation, the glmnet will generate the predictions under all lambda values [@].\r\nhttps://stackoverflow.com/questions/68689368/why-r-glmnet-predict-gives-a-matrix-instead-of-just-one-column\r\n\r\n\r\ncv_predict_glmnet <- predict(cv_glmnet, newx = newx, newoffset = log(df_test$exposure), type = \"response\", s = \"lambda.min\")\r\n\r\ncv_predict_glmnet_tf <- cv_predict_glmnet %>%\r\n  as_tibble() %>%\r\n  setNames(c(\"prediction\")) %>%\r\n  bind_cols(df_test)\r\n\r\ncv_predict_glmnet_tf\r\n\r\n\r\n# A tibble: 16,304 x 48\r\n   prediction row.id exposure driver.age yrs.licensed ncd.level region\r\n        <dbl>  <dbl>    <dbl>      <dbl>        <dbl>     <dbl>  <dbl>\r\n 1     0.131       3   1              68            2         4     33\r\n 2     0.0101      6   0.0833         68            2         3      3\r\n 3     0.121       8   1              68            2         3      3\r\n 4     0.114      10   1              65            2         3     21\r\n 5     0.0134     12   0.0833         58            5         3     36\r\n 6     0.158      13   1              58            5         3     36\r\n 7     0.0901     14   1              37            3         6     20\r\n 8     0.171      17   1              55            1         1      3\r\n 9     0.171      18   1              33            1         1      3\r\n10     0.191      20   1              46            1         1      3\r\n# ... with 16,294 more rows, and 41 more variables:\r\n#   vehicle.age <dbl>, vehicle.value <dbl>, seats <dbl>, ccm <dbl>,\r\n#   hp <dbl>, weight <dbl>, length <dbl>, width <dbl>, height <dbl>,\r\n#   prior.claims <dbl>, clm.incurred <dbl>, clm.count <dbl>,\r\n#   nb.rb_NB <dbl>, nb.rb_RB <dbl>, driver.gender_Female <dbl>,\r\n#   driver.gender_Male <dbl>, marital.status_Divorced <dbl>,\r\n#   marital.status_Married <dbl>, marital.status_Single <dbl>, ...\r\n\r\n\r\n\r\nglance(cv_glmnet)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  lambda.min lambda.1se  nobs\r\n       <dbl>      <dbl> <int>\r\n1   0.000822     0.0194 24456\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Markus Spiske on Unsplash\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-XX-glm/image/tiles.jpg",
    "last_modified": "2022-01-23T11:32:03+08:00",
    "input_file": "glm.knit.md"
  },
  {
    "path": "posts/2022-01-15-topic-modeling/",
    "title": "Topic Modeling",
    "description": "Discovering the topics within the text data",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "Text Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nCategorizing documents\r\nWhat is topic modeling?\r\nApplication of topic modeling\r\nMethods to perform topic modeling\r\nLatent Dirichlet Allocation (LDA)\r\nWhat is Dirichlet distribution?\r\n\r\nR package\r\nDemonstration\r\nSetup the environment\r\nDocument-feature matrix\r\nWord Cloud\r\n\r\nN-gram\r\nWord Cloud\r\n\r\nLDA\r\n\r\nConclusion\r\n\r\nI have discussed the basics of text analytics and the pre-processing for text data in the previous post.\r\nTherefore, in this post, I will explore topic modeling, i.e. one of the techniques under text analytics.\r\n\r\n\r\n\r\nPhoto by Prateek Katyal on Unsplash\r\nCategorizing documents\r\nCategorizing documents is one of the commonly used techniques in text analytics.\r\n(Blumenau 2021) summarizes the suitable technique to categorize the documents depending on the situation.\r\nIn general, the two considerations of which methods are appropriate to be used to categorize the documents are as follows:\r\nAre the categories of the documents known?\r\nDo we know the rules of categorizing the documents?\r\n\r\n\r\n\r\nHowever, the categories within the documents may not be so straightforward some of the time. For example, how granular should we split the actuarial topics within the documents? Should I split by the broader topics within the documents? How do we define the broader topic?\r\nThis is where the topic modeling technique becomes very handy.\r\nWhat is topic modeling?\r\nTopic modeling is a statistical method for identifying words in a corpus of documents that tend to co-occur together and as a result, share some sort of semantic relationship (Jockers and Thalken 2020).\r\nIn other words, topic modeling can also be thought of as an exercise to perform clustering on the documents based on the keywords in the documents.\r\nApplication of topic modeling\r\nIn a report by Milliman (Pouget et al. 2021), the authors provided a use case on how topic modeling could help in claim management. In the use case, the author mentioned that the topic modeling technique can be used to categorize the emails into different topics. This could greatly reduce the number of variables for the classification tasks.\r\nApart from that, the following are the other possible applications of topic modeling (Valput 2020):\r\nText summarization\r\nQuery expansion\r\nSentiment analysis\r\nRecommender systems\r\nNext, we will look at how to perform topic modeling.\r\nMethods to perform topic modeling\r\n(Sarkar 2016) There are three methods in extracting the topics from the text data:\r\nLatent Semantic Indexing (LSA)\r\nLatent Dirichlet Allocation (LDA)\r\nNon-negative Matrix Factorization (NNMF)\r\nOver here, I will be using the second method, where it is one of the popular methods to perform topic modeling.\r\nLatent Dirichlet Allocation (LDA)\r\nAfter reading through a few different materials, personally, I think is below explanation is easier to understand.\r\n(Bansal 2016) LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.\r\n\r\n\r\n\r\nThis method is quite similar to LSA. The main difference between LSA and LDA is that LDA assumes that the distribution of topics in a document and the distribution of words in topics are Dirichlet distributions (Pascual 2019).\r\nWhat is Dirichlet distribution?\r\nThe Dirichlet distribution Dir(α) is a family of continuous multivariate probability distributions parameterized by a vector α of positive reals (Liu 2019). It is a multivariate generalization of the Beta distribution. Dirichlet distributions are commonly used as prior distributions in Bayesian statistics.\r\nThe author also provided the explanation of using Dirichlet distribution used as a prior distribution in Bayesian statistics, i.e. this distribution is the conjugate before the categorical distribution and multinomial distribution, and making it the prior would simplify the maths.\r\nR package\r\nJust to recap how the different R packages work together with tidytext package (Silge and Robinson 2021):\r\n\r\n\r\n\r\nScreenshot from Chapter 6 of Text Mining with R book\r\nI will be using topicmodels package and tidytext package to perform topic modeling.\r\nDemonstration\r\nIn this demonstration, I will be using the same dataset as the previous post. This is the link to download the dataset.\r\nThe data consists of a publicly available set of question and sentence pairs from an open domain question.\r\n\r\n\r\n\r\nPhoto by Olya Kobruseva from Pexels\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I need for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', \r\n              'ggwordcloud', 'lexicon', 'topicmodels')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nI will also be using the pre-processing steps as previous post. I will be also using the output from lemmatization to perform topic modeling.\r\nHence, I will skip the pre-processing steps in this post so that this post is not too lengthy.\r\nDocument-feature matrix\r\nTo perform topic modeling, first, we will need to convert the token into the document-feature matrix (dfm).\r\nAccording to the quanteda documentation page, dfm refers to documents in rows and “features” as columns, where we can understand the tokens as the “features” in this context.\r\n\r\n\r\ntext_df_lemma_1 <- text_df_lemma %>%\r\n  dfm()\r\n\r\n\r\n\r\nI will also trim away the terms that appear 10 times or less across the document since it is unlikely the terms will carry much meaning to the analysis.\r\n\r\n\r\ntext_df_lemma_1 <- text_df_lemma_1  %>%\r\n  dfm_trim(min_termfreq = 10)\r\n\r\n\r\n\r\nThen, topfeatures function is used to extract the top 20 features from the dfm object.\r\n\r\n\r\ntopfeatures(text_df_lemma_1, 20)\r\n\r\n\r\n                    state        unite      include     american \r\n        8588         2782         1842         1416         1304 \r\n        year        world          war         time        large \r\n        1132         1087         1032          927          910 \r\n        film         make       series       system         city \r\n         832          767          759          755          730 \r\n        form united state         term      country         call \r\n         702          694          686          681          667 \r\n\r\nWord Cloud\r\nNext, I will illustrate the result in word cloud format so that it’s easier to visualize how frequently the words appear in the documents relative to one another.\r\nTo do so, first I will convert the dfm object to a tidy object by using tidy function.\r\n\r\n\r\ntext_df_lemma_1_tidy <- tidy(text_df_lemma_1)\r\n\r\n\r\n\r\nNext, I will perform group_by to sum up the total count of the different words in the documents.\r\n\r\n\r\ntext_df_lemma_1_tidy_count <- text_df_lemma_1_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\n\r\n\r\nOnce that is done, I will use ggplot function to visualize the word cloud.\r\n\r\n\r\ntext_df_lemma_1_tidy_count %>%\r\n  filter(tot_count >= 500) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"square\") +\r\n  scale_size_area(max_size = 500) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nFrom the graph, it seems like some unigram tokens may not carry much meaning. The algorithm has treated each word as a token by itself. However, sometimes we need multiple words to convey the message.\r\nFor example, if we refer to the graph, we will note “unite,” where “unite” could mean a lot of things. If we were to refer to the raw data, we will realize that “unite” is from the “united states.”\r\nN-gram\r\nTo overcome this, instead of performing a uni-gram, I will specify 2:3 in the function so that both bi-gram and tri-gram will be generated.\r\n\r\n\r\ntext_df_lemma_3 <- text_df_lemma %>%\r\n  tokens_ngrams(n = 2:3, concatenator = \" \") %>%\r\n  dfm()\r\n\r\n\r\n\r\ntopfeatures function is being used to extract the top features from the term frequency.\r\n\r\n\r\ntopfeatures(text_df_lemma_3, 20)\r\n\r\n\r\n          unite state             world war         unite kingdom \r\n                 1538                   228                   178 \r\n        north america            super bowl                war ii \r\n                  157                   138                   121 \r\n         world war ii             civil war           los angeles \r\n                  119                   110                    96 \r\n            york city            video game       president unite \r\n                   90                    88                    80 \r\npresident unite state          unite nation    federal government \r\n                   80                    76                    75 \r\n        academy award     television series    united state state \r\n                   73                    72                    68 \r\n    metropolitan area          sell million \r\n                   68                    66 \r\n\r\nNote that the term “unite state” appears much more frequent than the rest of the words. After some trial and error, it seems like the topic modeling result seems to be more satisfying after I drop the word “unite state.”\r\nHence in the dfm_trim function, I have specified I would like to drop the term that has more than 1000 counts.\r\nI will be dropping terms with a count of 30 or less, otherwise, the dfm will be too huge.\r\n\r\n\r\ntext_df_lemma_3 <- text_df_lemma_3 %>%\r\n  dfm_trim(min_termfreq = 30,\r\n           max_termfreq = 1000, \r\n           termfreq_type = \"count\")\r\n\r\n\r\n\r\ndfm_trim function also supports other trimming methods, eg. drop by proportion, rank and, quantile. Refer to this documentation page for the different trimming methods.\r\nWord Cloud\r\nNext, I will convert the dfm object into a tidy object so that I could use ggplot function to visualize the word cloud.\r\n\r\n\r\ntext_df_lemma_3_tidy <- tidy(text_df_lemma_3)\r\n\r\ntext_df_lemma_3_tidy_count <- text_df_lemma_3_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\ntext_df_lemma_3_tidy_count %>%\r\n  filter(tot_count >= 45) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"square\") +\r\n  scale_size_area(max_size = 14) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nLDA\r\nOnce that is done, I will pass the dfm object into LDA function. I have specified that I would like to have 5 topics in the result. Also, I will stick with the default fitting method, which is “VEM.”\r\nBefore that, I will perform a check to ensure there is no presence of all 0 rows within the dfm object. Otherwise, the algorithm would not work.\r\n\r\n\r\nrowTotals <- apply(text_df_lemma_3, 1, sum)\r\n\r\ntext_df_lemma_3_new <- text_df_lemma_3[rowTotals > 0, ]\r\n\r\n\r\n\r\nOnce the check is done, I will pass the updated dfm object into the LDA function.\r\n\r\n\r\ndf_lda_lemma_3 <- LDA(text_df_lemma_3_new, \r\n              k = 5, \r\n              control = list(seed = 0,\r\n                             nstart = 1))\r\n\r\n\r\n\r\nIf we were to call the LDA object, you could see that we have fitted a LDA model with VEM fitting method. There are 5 topics within the fitted model.\r\n\r\n\r\ndf_lda_lemma_3\r\n\r\n\r\nA LDA_VEM topic model with 5 topics.\r\n\r\nTo visualize the result, I will use tidy function to convert the object into tidy data format so that ggplot function can be used.\r\nThis is where we can use the tidy function within tidytext package to help us to convert the dfm object into a tidy object. Note that I have not passed in another argument, so the default matrix would be “beta” as shown in the result below.\r\n\r\n\r\ndf_lda_lemma_3_tidy <- tidy(df_lda_lemma_3)\r\n\r\ndf_lda_lemma_3_tidy\r\n\r\n\r\n# A tibble: 545 x 3\r\n   topic term             beta\r\n   <int> <chr>           <dbl>\r\n 1     1 world war     0.182  \r\n 2     2 world war     0.00268\r\n 3     3 world war     0.00311\r\n 4     4 world war     0.00651\r\n 5     5 world war     0.00471\r\n 6     1 unite kingdom 0.0255 \r\n 7     2 unite kingdom 0.0554 \r\n 8     3 unite kingdom 0.0341 \r\n 9     4 unite kingdom 0.0120 \r\n10     5 unite kingdom 0.0289 \r\n# ... with 535 more rows\r\n\r\nRecall beta is the parameter for per-topic word distribution. The way we could understand the beta is the probability of being generated from a necessary topic.\r\nOnce the results are being converted into tidy data format, I will find the top 10 terms under each topic. I will also sort the term based on their beta values in descending order.\r\n\r\n\r\ndf_lda_lemma_3_tidy_terms <- df_lda_lemma_3_tidy %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n = 10) %>% \r\n  ungroup() %>%\r\n  arrange(topic, -beta)\r\n\r\n\r\n\r\nThen, I will pass the result to ggplot function to visualize the result in the word cloud.\r\n\r\n\r\ndf_lda_lemma_3_tidy_terms %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(beta, term, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~topic, scales = \"free\") +\r\n  scale_y_reordered() +\r\n  theme_minimal() +\r\n  theme(text = element_text(size = 20))\r\n\r\n\r\n\r\n\r\n\r\n\r\nFrom the result, below are some of the interesting insights:\r\nTopic 1 has some words that are related to football\r\nThere are few words related to countries or areas within topic 2, eg. united states, united kingdom and, so on\r\nTopic 4 seems to be related to the super bowl\r\nTopic 3 & 5 seems to be related to war since there are a few words related to wars\r\nAlthough both topic 3 & 5 seem to be related to wars, topic 3 seems to be related to civil war and topic 5 relates to world war\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Kinga Cichewicz on Unsplash\r\n\r\n\r\n\r\nBansal, Shivam. 2016. “Beginners Guide to Topic Modeling in Pythons.” https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/.\r\n\r\n\r\nBlumenau, Jack. 2021. “Day 11: Topic Models: Me314: Introduction to Data Science and Machine Learning.” https://lse-me314.github.io/lecturenotes/ME314_day11.pdf.\r\n\r\n\r\nJockers, Matthew L., and Rosamond Thalken. 2020. Text Analysis with r. Springer.\r\n\r\n\r\nLiu, Sue. 2019. “Dirichlet Distribution: Motivating LDA.” https://towardsdatascience.com/dirichlet-distribution-a82ab942a879.\r\n\r\n\r\nPascual, Federico. 2019. “Topic Modeling: An Introduction.” https://monkeylearn.com/blog/introduction-to-topic-modeling/.\r\n\r\n\r\nPouget, Fanny, Remi Bellina, Mehdi Echchelh, Karol Maciejewski, Christoph Krischanitz, Bartosz Gaweda, Dominik Sznajder, et al. 2021. “The Use of Artificial Intelligence and Data Analytics in Life Insurance.” Milliman.\r\n\r\n\r\nSarkar, Dipanjan. 2016. Text Analytics with Python a Practical Real- World Approach to Gaining Actionable Insights from Your Datal. Apress.\r\n\r\n\r\nSilge, Julia, and David Robinson. 2021. “6 Topic Modeling.” https://www.tidytextmining.com/topicmodeling.html.\r\n\r\n\r\nValput, Damir. 2020. “Topic Modelling: Interpretability and Applications.” https://datascience.aero/topic-modelling/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-topic-modeling/image/books.jpg",
    "last_modified": "2022-01-10T22:46:20+08:00",
    "input_file": "topicModeling.knit.md"
  },
  {
    "path": "posts/2021-12-02-text-analytics/",
    "title": "Text Analytics 101",
    "description": "Finding the \"gold\" within the text",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [
      "Text Analytics"
    ],
    "contents": "\r\n\r\nContents\r\nText Analytics\r\nPotential Opportunities in Text Analytics\r\nText pre-processing\r\nImport of Raw Text and Formating\r\nRemoving unwanted characters (eg. special characters and symbols)\r\nMisspelling\r\n\r\nConverting into lowercases\r\nTokenization\r\nStopwords\r\nPart of Speech Tagging\r\nStemming & lemmatization\r\n\r\nFeature engineering techniques for text analytics\r\nBag of words (BoW)\r\nTerm frequency & inverse document frequency\r\nWord embedding\r\n\r\nHow different R packages work together\r\nDemonstration\r\nSetup the environment\r\nImport data\r\nClean text\r\nConvert to lower letters\r\nRemove stopwords & unwanted characters\r\nReplace words\r\nStemming\r\nLemmatization\r\n\r\n\r\nConclusion\r\n\r\nIn this post, I will be exploring different text analytics.\r\n\r\n\r\n\r\nPhoto by Leah Kelley from Pexels\r\nThis is one of the topics I wanted to explore while I was pursuing my master degree.\r\nWhile I was reading through the different materials on text analytics and natural language processing, I realized the more I read about text analytics, the more I realized it is not as simple as what I initially imagined so.\r\nMind blownTaken from giphy\r\nHence I have spent quite a fair bit of time reading through different books and articles to understand more about text analytics.\r\nHope this would help you in understanding more about text analytics.\r\nText Analytics\r\nAs the name suggested, this technique focuses on cleaning and drawing insights out from the text data. Often, the text data are not ‘structured’ in a way that could be easily understood by the models/algorithms.\r\nOnce the texts are processed, further techniques can be applied to the pre-processed text data to draw insights or even build a machine learning model on the text data.\r\nLet’s look at how text analytics could add value to the insurance business.\r\nPotential Opportunities in Text Analytics\r\nThere are articles on how text data can be used in the insurance context. (Rodriguez 2019) discussed the benefits unstructured data (eg. text data) could provide, including how text data could assist insurers in improving decision making and so on.\r\nThe author also provided an example of how insurers could tap onto text data is how the various text analytics and natural language processing can be used to process the volume of the texts, assisting claim adjusters potentially to make a more accurate decision with a shorter amount of time spent.\r\nApart from using text data in claim analysis, (Duncan 2016) also listed some of the financial service use cases where text analytics is being applied.\r\nWhile the actual usage of text data in insurance day-to-day operation is somewhat lower than expected, the insurers seem to remain optimistic on the potential of using text data (McGrath 2021).\r\nText pre-processing\r\n(Ferrario and Naegelin 2020) The typical steps of text pre-processing include the following:\r\nImport of raw text and formatting\r\nConversion of text to lowercase\r\nTokenization, i.e. split of all strings of text into tokens\r\nStopwords removal\r\nPart-of-speech (POS) tagging of tokenized text\r\nStemming or lemmatization\r\nLet’s look at what does each step does in text analytics.\r\nImport of Raw Text and Formating\r\nAfter importing the text into the environment, it is common that we will perform some levels of “text cleaning” to remove or correct some of the texts before performing the analysis.\r\nFollowing are some considerations while performing “text cleaning”:\r\nRemoving unwanted characters (eg. special characters and symbols)\r\nWithin the text, we might have characters we wanted to remove from the analysis. For example, we might want to exclude the reference link from the text analysis.\r\nMost of the modern packages would have included some pre-defined functions to remove the unwanted characters within the text data. For example, below is the function to tokenize text data in quanteda package:\r\n\r\n\r\n\r\nScreen shot from quanteda documentation page\r\nAs shown in the screenshot above, there are options for the users to indicate whether they would like to remove the necessary text within the data.\r\nHowever, there are scenarios where the pre-defined functions are unable to “clean” the data. For example, we might have the same words but spell differently within the data (eg. color vs colour).\r\nThis is where regular expression (a.k.a. regex) comes in very handy. Regex can be used to extract, remove, replace or even find a string within the text.\r\nPersonally, I find this regex website is very helpful. It helps me to visualize whether the regex is working (i.e. finding the relevant words) as intended.\r\nBelow are some of the common syntax in regex:\r\n\r\n\r\n\r\nExtracted from this website\r\nThis is another great resource of the different regex syntax to extract the necessary info from the text.\r\nMisspelling\r\nOften the texts are unlikely to be clean. Misspelling of words is one of the common problems to tackle when analyzing texts.\r\nTo fix the misspelling, there are two approaches. One is to use existing packages to fix the typos.\r\nAnother method is to create a manual listing of typos. The downside of such an approach is that we are limited to correct words we have observed within the dataset.\r\nConverting into lowercases\r\nAs the usual text analytics algorithm is case-sensitive, the same words with different lower cases and/or upper cases will be treated as different words. For example, “TEXT,” “Text” and “text” will be treated as different words.\r\nHence, the usual approach is to convert all the words into lowercase so that the algorithm would not treat the same words as different words due to the difference in letter cases.\r\nTokenization\r\nTokenization is the process of chopping character streams into tokens (Manning, Raghavan, and Schütze 2009). The authors also further explained that a token is an instance of a sequence of characters in some particular document that is grouped as a useful semantic unit for processing.\r\nIn general, tokenization consists of following two types and their relevant descriptions (Sarkar 2016):\r\n\r\n\r\nTypes\r\n\r\n\r\nDescriptions\r\n\r\n\r\nSentence Tokenization\r\n\r\n\r\nSplit text into sentences\r\n\r\n\r\nWord Tokenization\r\n\r\n\r\nSplit text into words\r\n\r\n\r\nStopwords\r\nStopwords are the most common words in any natural language (Singh 2019). Often these stopwords do not carry much value in helping us in understanding the context of the documents/articles.\r\nIn general, the stopwords can be categorized into following groups (Hvitfeldt and Silge 2021a):\r\n\r\n\r\nGrouping\r\n\r\n\r\nDescriptions\r\n\r\n\r\nRecommendation\r\n\r\n\r\nGlobal stopwords\r\n\r\n\r\nWords that are almost always low in meaning in a given language\r\n\r\n\r\nUse pre-made stopwords lists to remove them\r\n\r\n\r\nSubject-specific stopwords\r\n\r\n\r\nWords that are uninformative for a given a subject area\r\n\r\n\r\nMay improve performance if we have domain expertise to create a good list to remove them. But likely require us to create the list manually as these words are generally not considered as stopwords in a given language\r\n\r\n\r\nDocument-level stopwords\r\n\r\n\r\nWords that do not provide any or much information for a given document\r\n\r\n\r\nDifficult to classify and would not be worth the effort to identify\r\n\r\n\r\nWhile the common approach is to remove stopwords while performing text analytics, removal stopwords would not be appropriate in some of the NLP tasks such as machine translation and so on.\r\nThis is because the meaning of the text could change when we remove stopwords. (Schumacher 2019) provided an example to illustrate a scenario where removing stopwords is a bad idea.\r\nConsidering the following example by (Schumacher 2019):\r\nOriginal statement: Stacy gave her doll to the puppy\r\nAfter removing stopwords: Stacy gave doll puppy\r\nWithout the stopwords, we are unable to interpret whether the doll was given to the puppy or the puppy was given to the doll. Hence, this illustrates that the importance of stopwords in this context.\r\nPart of Speech Tagging\r\nParts of speech (POS) are specific lexical categories to which words are assigned based on their syntactic context and role (Sarkar 2016). In other words, the words in the text are being analyzed and tagged to a role in a sentence (eg. is the word a noun or a verb).\r\nSome of the natural language processing tasks (eg. named entity recognition) would require the tokens to be tagged first.\r\nStemming & lemmatization\r\nOften within the texts, there might be similar words. For example, the text might contain words like ‘cave’ and ‘caves,’ where both of the words are referring to the same thing.\r\nHence, it would be better for us to “clean” the words before performing any analysis. Otherwise, the algorithm will be treating these words as separate words. (Hvitfeldt and Silge 2021b) The author also mentioned through this process, we reduce the sparsity of the test data, which can be very helpful when training models.\r\nFollowing is the comparison of stemming and lemmatization (bitext 2021):\r\n\r\n\r\nCategory\r\n\r\n\r\nDescriptions\r\n\r\n\r\nStemming\r\n\r\n\r\nRule-based\r\nWork by cutting off the end or the beginning of the words\r\n\r\n\r\nLemmatization\r\n\r\n\r\nLinguistics-based\r\nNormalize the words based on language structure and how words are used in their context\r\nTake the morphological analysis of the words into considerations\r\nGenerally, longer runtime than stemming\r\n\r\n\r\nFeature engineering techniques for text analytics\r\nAs the machine learning algorithm is unable to work with the text directly, different feature engineering techniques can be used in preparing the text data before passing it into the algorithm.\r\nFollowing are some of the feature engineering techniques for text analytics:\r\nBag of words (BoW)\r\n(Brownlee 2019) explained that BoW is a representation of text that describes the occurrence of words within a document. Essentially we will split the texts into either individual text or a group of texts.\r\nAlso, this technique is called ‘bag of words’ as under this technique, the sequence of the words does not matter.\r\nTerm frequency & inverse document frequency\r\n(Stecanella 2019) explained that term frequency is calculating the frequency of the words in a document, where the inverse document frequency is measuring how common or a rare a word is in the entire document.\r\nAlternatively, the inverse document frequency can be thought as a “penalty function” imposed on term frequency to measure how frequently the words appear in the document. If the words appear more frequently in the document, the higher the “penalty” is.\r\nWord embedding\r\nThis is a more advanced technique, which I won’t be covering in this post. For more info, please refer to this link or this link.\r\nHow different R packages work together\r\nIn R, many packages work with text analytics and natural language processing.\r\nIn this post, I will be exploring the different R packages mentioned in the Text Mining with R book.\r\nFollowing are how the different text analytics R packages could work together (Silge and Robinson 2021):\r\n\r\n\r\n\r\nScreenshot from Chapter 6 of Text Mining with R book\r\nDemonstration\r\nIn this demonstration, I will be using a dataset from Microsoft. This dataset contains a publicly available set of question and sentence pairs.\r\nThis is the link to download the dataset.\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I need for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', \r\n              'ggwordcloud', 'lexicon')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport data\r\nNext, I will import the dataset into the environment.\r\nNote that the dataset is in tsv format, hence I will be using read_tsv function to import the dataset into the environment.\r\n\r\n\r\ndf <- read_tsv(\"data/WikiQA.tsv\",\r\n               quote = \"\\t\")\r\n\r\n\r\n\r\nClean text\r\nOnce the data is imported into the environment, tokens function is used to tokenize the words.\r\nMeanwhile, I have also indicated that punctuation, numbers, symbols, and separators should be removed when the words are being tokenized.\r\n\r\n\r\ntext_df <- tokens(df$Sentence, \r\n                  remove_punct = TRUE,\r\n                  remove_numbers = TRUE,\r\n                  remove_symbols = TRUE,\r\n                  remove_separators = TRUE,\r\n                  split_hyphens = FALSE)\r\n\r\n\r\n\r\nConvert to lower letters\r\nAs discussed in the post earlier, the common text cleaning also involves converting the token into the lower case as the algorithm is case-sensitive. Same words with the different cases will be treated as different words.\r\nHence, I have used tokens_tolower function to convert all the words to lower case.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_tolower()\r\n\r\n\r\n\r\nRemove stopwords & unwanted characters\r\nNext, I will remove the stopwords from the text data. To do so, I wrap stopwords function with tokens_remove function.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_remove(stopwords(language = \"en\", source = \"smart\"), padding = FALSE)\r\n\r\n\r\n\r\nNote that I have also indicated that padding to be false so that the function will not replace the stopwords with an empty string. In other words, the stopwords will be dropped from the dataset.\r\nAlso, note that I have indicated the source for stopwords should be smart, where the list of stopwords within this source can be found under this link.\r\nThere are also other sources for the English stopwords. Following are the different sources that are currently supported:\r\n\r\n\r\nstopwords::stopwords_getsources()\r\n\r\n\r\n[1] \"snowball\"      \"stopwords-iso\" \"misc\"          \"smart\"        \r\n[5] \"marimo\"        \"ancient\"       \"nltk\"          \"perseus\"      \r\n\r\nNext, I will perform a quick check on the cleaned text data. I have noted a few issues within the text data.\r\nFor example, there are some digits with alphabets as shown below.\r\n\r\n\r\ntext_df[[51]][2]\r\n\r\n\r\n[1] \"21a\"\r\n\r\nIf we were to go back to the original text data, we will realize this string seems to be the robot version number, which may not add much value/insight to the analysis later.\r\n\r\n\r\ndf$Sentence[51]\r\n\r\n\r\n[1] \"The Beretta 21A Bobcat is a small pocket-sized semi-automatic pistol designed by Beretta in Italy.\"\r\n\r\nHence, I will remove this string from the text data. To do so, I have used tokens_replace and the relevant regex to remove the words.\r\nNote that in the regex, I have indicated that I want to find all the matching strings that start with a digit and continue by letters. The asterisk means there are 0 or more letters after the digits.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_replace(pattern = c(\"\\\\d\\\\w*\"), replacement = c(\"\"), valuetype = \"regex\")\r\n\r\n\r\n\r\nBelow is another issue found in the text data. Somehow there is a hyphen before some of the words.\r\n\r\n\r\ndf$Sentence[2735]\r\n\r\n\r\n[1] \"The bar was a staple of the Chicago -based company for some seven decades.\"\r\n\r\nTherefore, during the tokenization, I have indicated that the words should be split by hyphens. According to the documentation, hyphens will be treated as a separate token.\r\nTo remove the hyphens after splitting the words, I will use regex to remove the hyphens.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_replace(pattern = c(\"\\\\-\"), replacement = c(\"\"), valuetype = \"regex\")\r\n\r\n\r\n\r\nThese are common issues one might face when cleaning the text data. Therefore, it is always a good practice to double the output to check whether the output is as expected based on the pre-processing steps indicated earlier.\r\nReplace words\r\nSome words may have different spelling. Without any cleaning, the algorithm will these words with a different spelling as different words.\r\nFor example, within the text data, I noted that “united state” can also be spelled as “united st” or “u.s” although both of the words refer to “united state.”\r\nTherefore, I will list the words to be replaced within tokens_replace function.\r\n\r\n\r\ntext_df <- text_df %>%\r\n  tokens_replace(pattern = c(\"united st\", \"u.s\"), \r\n                 replacement = c(\"united state\", \"united state\"), \r\n                 valuetype = \"fixed\")\r\n\r\n\r\n\r\nNote that as I am finding the exact match of the words, hence the valuetype should be indicated as “fixed.”\r\nStemming\r\nAs mentioned in the earlier section, often we may perform some further cleaning to find the root words, otherwise the algorithm will treat these words as separate words by themselves.\r\nFor example, if I were to extract the text that contains “tree,” we will note that “trees” are just the plural form of “tree” and we may not want to differentiate “tree” and “trees” in the analysis.\r\n\r\n\r\ntemp <- text_df %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  filter(term != \"\") %>%\r\n  filter(str_detect(term, \"\\\\b(tree)\") == TRUE)\r\n\r\nunique(temp$term)\r\n\r\n\r\n[1] \"tree\"      \"trees\"     \"treedome\"  \"treener\"   \"treehouse\"\r\n\r\nTherefore, one of the common approaches is to use stemming to find the root words.\r\nTo do so, I will use tokens_wordstem function to perform stemming on the words.\r\n\r\n\r\ntext_df_stem <- text_df %>%\r\n   tokens_wordstem(language = \"english\")\r\n\r\n\r\n\r\nNext, I will dfm function to create a document-feature matrix so that later I could use tidy function to convert the object to conform to tidy data format.\r\nAlso, I will trim away all the term frequency that is lesser than 8.\r\n\r\n\r\ntext_df_stem_1 <- text_df_stem %>%\r\n  dfm() %>%\r\n  dfm_trim(min_termfreq = 8)\r\n\r\ntext_df_stem_1_tidy <- tidy(text_df_stem_1) %>%\r\n  filter(term != \"\")\r\n\r\n\r\n\r\nOnce the object is converted into tidy data format, we could use our usual dplyr to transform for the necessary analysis.\r\nI will perform a frequency count on each word in the entire dataset, regardless of which documents they appear on.\r\n\r\n\r\ntext_df_stem_1_tidy_count <- text_df_stem_1_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\n\r\n\r\nI will further sort the count descending. From the results, we can see that some of the words appear more frequently than the rest.\r\n\r\n\r\ntext_df_stem_1_tidy_count %>%\r\n  arrange(desc(tot_count))\r\n\r\n\r\n# A tibble: 5,398 x 2\r\n   term     tot_count\r\n   <chr>        <dbl>\r\n 1 state         2810\r\n 2 unit          2076\r\n 3 includ        1416\r\n 4 american      1305\r\n 5 world         1267\r\n 6 year          1154\r\n 7 nation        1047\r\n 8 war           1037\r\n 9 time           928\r\n10 film           862\r\n# ... with 5,388 more rows\r\n\r\nHowever, it would be quite difficult to compare the frequency count of different words by looking at the data above.\r\nHence, I will pass the output into ggwordcloud to visualize the output in wordcloud. As there are too many unique words within the data and the graph will be cluttered with different words if we were to visualize all the words in wordcloud, the graph will be cluttered with words.\r\nHence, to overcome this, I will filer out those words with less than 350 count to lower the number of word counts.\r\n\r\n\r\ntext_df_stem_1_tidy_count %>%\r\n  filter(tot_count >= 350) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"circle\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nWhile this stemming could assist us in finding the root words, but it also fails to identify the root words for some of the words.\r\n\r\n\r\ntext_df_stem_1_tidy_count %>%\r\n  filter(str_detect(term, \"(young)\") == TRUE)\r\n\r\n\r\n# A tibble: 3 x 2\r\n  term     tot_count\r\n  <chr>        <dbl>\r\n1 young          123\r\n2 younger         24\r\n3 youngest        24\r\n\r\nLemmatization\r\nRecall in the earlier section, I have discussed that lemmatization is another method to find the root words.\r\nThis method is linguistics-based and could potentially overcome the issue we face when using stemming.\r\nTo perform lemmatization, I have referred to this Stack Overflow post on how to perform lemmatization by using tokens_replace function.\r\n\r\n\r\ntext_df_lemma <- text_df %>%\r\n  tokens_replace(pattern = lexicon::hash_lemmas$token, \r\n                 replacement = lexicon::hash_lemmas$lemma)\r\n\r\n\r\n\r\nOnce the lemmatization is done, let’s us check whether lemmatization has successfully finding the root words of “younger” and “youngest.”\r\n\r\n\r\ntext_df_lemma %>%\r\n  dfm() %>%\r\n  tidy() %>%\r\n  filter(term != \"\") %>%\r\n  filter(str_detect(term, \"(young)\") == TRUE) %>%\r\n  group_by(term) %>%\r\n  tally()\r\n\r\n\r\n# A tibble: 3 x 2\r\n  term           n\r\n  <chr>      <int>\r\n1 young        163\r\n2 young's        4\r\n3 youngblood     1\r\n\r\nAt least now the original words for “younger” & “youngest” are found through lemmatization. The rest of the words which contain “young” are unlikely to have “young” as the root word.\r\nNext, I will follow similar steps to create the document-feature matrix and convert the data into tidy data format.\r\n\r\n\r\ntext_df_lemma_1 <- text_df_lemma %>%\r\n  dfm() %>%\r\n  dfm_trim(min_termfreq = 8)\r\n\r\ntext_df_lemma_1_tidy <- tidy(text_df_lemma_1) %>%\r\n  filter(term != \"\")\r\n\r\ntext_df_lemma_1_tidy_count <- text_df_lemma_1_tidy %>%\r\n  group_by(term) %>%\r\n  summarise(tot_count = sum(count))\r\n\r\n\r\n\r\nOnce the frequency count is computed, I will pass the data into word cloud function to visualize the words in the word cloud format.\r\n\r\n\r\ntext_df_lemma_1_tidy_count %>%\r\n  filter(tot_count >= 350) %>%\r\n  ggplot(aes(label = term, size = tot_count, color = tot_count)) +\r\n  geom_text_wordcloud_area(shape  = \"square\") +\r\n  scale_size_area(max_size = 18) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Emre Can Acer from Pexels\r\n\r\n\r\n\r\nbitext. 2021. “What Is the Difference Between Stemming and Lemmatization?” https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/.\r\n\r\n\r\nBrownlee, Jason. 2019. “A Gentle Introduction to the Bag-of-Words Model.” https://machinelearningmastery.com/gentle-introduction-bag-words-model/.\r\n\r\n\r\nDuncan, Alan D. 2016. “Is It Time for Text Analytics in Financial Services?” https://blogs.gartner.com/alan-duncan/2016/09/21/time-come-text-analytics-financial-services/.\r\n\r\n\r\nFerrario, Andrea, and Mara Naegelin. 2020. “The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification.” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887.\r\n\r\n\r\nHvitfeldt, Emil, and Julia Silge. 2021a. “Chapter 3 Stop Words.” https://smltar.com/stopwords.html#stopwordssummary.\r\n\r\n\r\n———. 2021b. “Chapter 4 Stemming.” https://smltar.com/stemming.html#how-to-stem-text-in-r.\r\n\r\n\r\nManning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. 2009. An Introduction to Information Retrieval. Cambridge University Press.\r\n\r\n\r\nMcGrath, Liam. 2021. “Mining for Gold: Text Analytics in Insurance: Natural Language Processing Provides Insurers Tools for Using Text Data.” https://www.willistowerswatson.com/en-US/Insights/2021/03/mining-for-gold-text-analytics-in-insurance.\r\n\r\n\r\nRodriguez, Jason. 2019. “Decoding the Hidden Value of Unstructured Text Data.” https://www.willistowerswatson.com/en-US/Insights/2019/02/decoding-the-hidden-value-of-unstructured-text-data.\r\n\r\n\r\nSarkar, Dipanjan. 2016. Text Analytics with Python a Practical Real- World Approach to Gaining Actionable Insights from Your Datal. Apress.\r\n\r\n\r\nSchumacher, Alex. 2019. “When (Not) to Lemmatize or Remove Stop Words in Text Preprocessing.” https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html.\r\n\r\n\r\nSilge, Julia, and David Robinson. 2021. “6 Topic Modeling.” https://www.tidytextmining.com/topicmodeling.html.\r\n\r\n\r\nSingh, Shubham. 2019. “NLP Essentials: Removing Stopwords and Performing Text Normalization Using NLTK and spaCy in Python.” https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/.\r\n\r\n\r\nStecanella, Bruno. 2019. “Understanding TF-ID: A Simple Introduction.” https://monkeylearn.com/blog/what-is-tf-idf/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-02-text-analytics/image/typewriter.jpg",
    "last_modified": "2021-12-03T01:15:28+08:00",
    "input_file": "text-analytics.knit.md"
  },
  {
    "path": "posts/2021-10-30-k-means/",
    "title": "K-Means Clustering",
    "description": "Why don't we call this algorithm as k-average algorithm? \n\n\nBecause it's mean.",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-10-30",
    "categories": [
      "Machine Learning",
      "Unsupervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nK-means clustering\r\nPros and Cons of K-means\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Checking & Wrangling\r\nTreatment on Magic Values\r\nHandling Outlier\r\nStandardization\r\nFilter out Non-numeric variables\r\nCorrelation Checking\r\n\r\nRunning clustering algorithm\r\nElbow curve\r\nChecking on clustering results\r\nCheck cluster size\r\nCheck average values for each cluster\r\n\r\nParallel Coordinate Plot\r\n\r\nConclusion\r\n\r\nIn the previous post, I have discussed the basic concept of clustering and what are considerations when performing any clustering exercise.\r\nTherefore, I will be using K-means to demonstrate how a clustering algorithm works and how to interpret the clustering results in this post.\r\nK-means clustering\r\n\r\n\r\n\r\nIllustration by Allison Horst\r\nK-means clustering is one of the most common basic algorithms one would learn when they embark on their machine learning journey.\r\nIn short, K-means clustering attempts to partition a dataset into K distinct, non-overlapping clusters (James et al. 2021).\r\nAllison Horst has nice illustrations on how K-means clustering work in graphic format, making it super easy to understand.\r\nFollowing are the illustrations on how K-means clustering work:\r\n\r\n\r\n\r\nIllustration by Allison Horst\r\nPros and Cons of K-means\r\n(Google Developers 2020) has listed a few pros and cons of the K-means clustering algorithm.\r\nBelow are the pros and cons extracted from the website:\r\nPros\r\nRelatively simple to implement\r\nScales to large data sets\r\nGuarantees convergence\r\nCan warm-start the positions of centroids\r\nEasily adapts to new examples\r\nGeneralizes to clusters of different shapes and sizes, such as elliptical clusters\r\nCons\r\nChoosing k manually\r\nBeing dependent on initial values\r\nClustering data of varying sizes and density\r\nClustering outliers\r\nScaling with number of dimensions\r\nDemonstration\r\nFor this demonstration, I will be using bank marketing data from UCI Machine Learning Repository.\r\nThis dataset contains data points from past direct marketing campaigns of a Portuguese banking institution.\r\n\r\n\r\n\r\nPhoto by Monstera from Pexels\r\nSetup the environment\r\nFirst, I will set up the environment by calling all the packages I need for the analysis later.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidymodels', 'plotly', \r\n              'corrplot', 'GGally')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport Data\r\nNext, I will import the data into the environment and perform some data wrangling for the analysis later.\r\n\r\n\r\nset.seed(12345)\r\n\r\ndf <- read_delim(\"data/bank-full.csv\", delim = \";\") %>%\r\n  mutate(across(where(is.character), as.factor)) %>%\r\n  mutate(month = ordered(month, levels = c(\"jan\",\r\n                                          \"feb\",\r\n                                          \"mar\",\r\n                                          \"apr\",\r\n                                          \"may\",\r\n                                          \"jun\",\r\n                                          \"jul\",\r\n                                          \"aug\",\r\n                                          \"sep\",\r\n                                          \"oct\",\r\n                                          \"nov\",\r\n                                          \"dec\"))) %>%\r\n  sample_frac(size = 0.5)\r\n\r\n\r\n\r\nNote that I will perform clustering on a sample of the data points. This approach is commonly used when the dataset is too huge for us to perform data exploratory.\r\nOnce we have finished the clustering analysis, we could re-run on the full dataset although the results are unlikely to change significantly when we change from subset of dataset to full dataset. If the results have changed significantly, this could indicate that the subset of data used for clustering is not representative of the full dataset.\r\nData Checking & Wrangling\r\nAs part of the usual analysis, I will start by checking the data quality.\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n22606\r\nNumber of columns\r\n17\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n10\r\nnumeric\r\n7\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\njob\r\n0\r\n1\r\nFALSE\r\n12\r\nblu: 4926, man: 4666, tec: 3788, adm: 2574\r\nmarital\r\n0\r\n1\r\nFALSE\r\n3\r\nmar: 13679, sin: 6335, div: 2592\r\neducation\r\n0\r\n1\r\nFALSE\r\n4\r\nsec: 11605, ter: 6621, pri: 3461, unk: 919\r\ndefault\r\n0\r\n1\r\nFALSE\r\n2\r\nno: 22188, yes: 418\r\nhousing\r\n0\r\n1\r\nFALSE\r\n2\r\nyes: 12516, no: 10090\r\nloan\r\n0\r\n1\r\nFALSE\r\n2\r\nno: 18944, yes: 3662\r\ncontact\r\n0\r\n1\r\nFALSE\r\n3\r\ncel: 14610, unk: 6523, tel: 1473\r\nmonth\r\n0\r\n1\r\nTRUE\r\n12\r\nmay: 6837, jul: 3449, aug: 3115, jun: 2709\r\npoutcome\r\n0\r\n1\r\nFALSE\r\n4\r\nunk: 18501, fai: 2410, oth: 931, suc: 764\r\ny\r\n0\r\n1\r\nFALSE\r\n2\r\nno: 19960, yes: 2646\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nage\r\n0\r\n1\r\n40.90\r\n10.59\r\n18\r\n33\r\n39\r\n48\r\n92\r\n▅▇▅▁▁\r\nbalance\r\n0\r\n1\r\n1350.98\r\n3007.46\r\n-3372\r\n70\r\n451\r\n1437\r\n98417\r\n▇▁▁▁▁\r\nday\r\n0\r\n1\r\n15.81\r\n8.36\r\n1\r\n8\r\n16\r\n21\r\n31\r\n▇▆▇▆▆\r\nduration\r\n0\r\n1\r\n258.57\r\n257.97\r\n0\r\n103\r\n180\r\n318\r\n3366\r\n▇▁▁▁▁\r\ncampaign\r\n0\r\n1\r\n2.79\r\n3.20\r\n1\r\n1\r\n2\r\n3\r\n58\r\n▇▁▁▁▁\r\npdays\r\n0\r\n1\r\n39.82\r\n99.48\r\n-1\r\n-1\r\n-1\r\n-1\r\n850\r\n▇▁▁▁▁\r\nprevious\r\n0\r\n1\r\n0.57\r\n1.86\r\n0\r\n0\r\n0\r\n0\r\n51\r\n▇▁▁▁▁\r\n\r\nTreatment on Magic Values\r\nIn the data dictionary, we are told that if the customer is not being contacted, the pdays will be recorded as -1.\r\nIn Google Machine Learning Course, the author mentioned it is not ideal to mix “magic” values with actual data.\r\nTherefore, I will separate the “magic” values from the actual data by including an indicator to indicate whether the pdays were captured in the dataset and changing the magic value to 0.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(pdays_supplied = case_when(pdays == -1 ~ \"No\",\r\n                                    TRUE ~ \"Yes\"),\r\n         pdays_supplied = as.factor(pdays_supplied),\r\n         pdays_recoded = case_when(pdays == -1 ~ 0,\r\n                                   TRUE ~ pdays)) %>%\r\n  dplyr::select(-pdays)\r\n\r\n\r\n\r\nHandling Outlier\r\nAs K-means clustering is finding the centroid within each cluster, hence the algorithm can sensitive to outliers. Hence, to overcome this, we can either take the following actions:\r\nRemove outliers before running the clustering algorithm\r\nUse other algorithms that are less sensitive to outliers (eg. k-medians clustering)\r\nIn this post, I will remove the outliers from the dataset before performing clustering analysis.\r\n\r\n\r\ndf_1 <- df_1 %>%\r\n filter(previous < 100)\r\n\r\n\r\n\r\nStandardization\r\nIn the data quality checking, we can see that the values are very skewed.\r\nAs K-means clustering is very sensitive to the range of the variable, hence standardization should be performed to avoid variables with larger variance to dominate the effect of clustering.\r\n\r\n\r\ndf_1_scale <- df_1 %>%\r\n  mutate(across(where(is.numeric), scale))\r\n\r\n\r\n\r\nFilter out Non-numeric variables\r\nThere are non-numeric variables within the dataset. However, K-means clustering algorithm is unable to handle non-numeric variables.\r\nThe common approach to get around with this issue is to convert the non-numeric variables into dummy variables through a one-hot encoding method. (IBM Support 2020) mentioned that the clustering results are unlikely to be satisfactory by using binary data. As the naming of the algorithm suggested, the algorithm attempts to find the clusters through finding the “mean” (a.k.a. the centroid), where finding the average is not meaningful for binary data.\r\nAs such, we can either perform clustering only on the numeric variables or use other algorithms (e.g. K-proto) that allow us to perform clustering on the mixed data type. I will explore this alternative clustering algorithm in my future post.\r\nIn this post, I will use the first method, which is to filter out all the non-numeric variables for clustering.\r\n\r\n\r\ndf_1_scale_num <- df_1_scale %>%\r\n  select_if(is.numeric)\r\n\r\n\r\n\r\nCorrelation Checking\r\nI will also check the correlation before I proceed and run K-means clustering. This is because highly correlated variables may affect the clustering results, causing clustering results to overlap with one another.\r\n\r\n\r\ncorrplot(cor(df_1_scale_num, use=\"pairwise.complete.obs\"), \r\n         method = \"number\", \r\n         type = \"upper\", \r\n         tl.cex = 0.65, \r\n         number.cex = 0.65, \r\n         diag = FALSE)\r\n\r\n\r\n\r\n\r\nFrom the correlation results, it does not seem like we have any highly correlated variables. This suggests that we could use all the variables for clustering purposes.\r\nRunning clustering algorithm\r\nLet’s start our analysis in K-means clustering.\r\nAs mentioned earlier, we would need to upfront pre-define how many clusters we need for the K-means clustering exercise. However, we are unable to know what is the optimal number of clusters with the given dataset. Hence, the common approach is to run the algorithms over a pre-defined range number of clusters.\r\nTherefore, to do so, I have referenced the code from Exploratory clustering shared on Tidymodels documentation page.\r\n\r\n\r\nkclusts_1_scale <- \r\n  tibble(k = 3:10) %>%\r\n  mutate(\r\n    kclust = map(k, ~kmeans(df_1_scale_num, algorithm=\"Lloyd\", .x)),\r\n    tidied = map(kclust, tidy),\r\n    glanced = map(kclust, glance),\r\n    augmented = map(kclust, augment, df_1_scale_num)\r\n  )\r\n\r\n\r\n\r\nNext, I would use unnest function to flatten the selected columns into regular columns.\r\n\r\n\r\nclusters_1_scale <- \r\n  kclusts_1_scale %>%\r\n  unnest(cols = c(tidied))\r\n\r\nassignments_1_scale <- \r\n  kclusts_1_scale %>% \r\n  unnest(cols = c(augmented))\r\n\r\nclusterings_1_scale <- \r\n  kclusts_1_scale %>%\r\n  unnest(cols = c(glanced))\r\n\r\n\r\n\r\nThe beauty of such approach above is the output from the codes follows tidy data concept, which allows us to join different functions together without needing many transformations.\r\nElbow curve\r\nOnce the columns are flattened, I will plot the elbow curve to find the optimal number of clusters by using ggplot function.\r\nIn general, the total within-cluster variation (a.k.a. tot.withinss in the algorithm/graph) decreases when the number of clusters increases. This is expected since when the number of clusters increases, there would be more centroids within the dataset, which decreases the distance between each data point and the centroids.\r\nAlternatively, we can understand that when all the data points are individual clusters by themselves, the total within-cluster variation would be zero. Hence, the total within-cluster variation would decrease when we increase the number of clusters.\r\nAs such, below is the elbow curve for the given dataset:\r\n\r\n\r\nggplot(clusterings_1_scale, aes(k, tot.withinss)) +\r\n  geom_line() +\r\n  geom_point()\r\n\r\n\r\n\r\n\r\nBased on the elbow curve above, it seems like 8 clusters is a good cutoff.\r\nNote that it is not ideal to choose the cluster that gives the lowest total within-cluster variation. Merely choosing the number of clusters that give us the lowest total within-cluster variation would not work as the data point is a cluster on its own would give us the lowest total within-cluster variation. This would defeat the purpose of the clustering exercise.\r\nTherefore, it would be beneficial to consider how should we select the clustering results.\r\nDo refer to my previous post where I have discussed the different considerations while performing clustering analysis.\r\nWith that, I will pull out the clustering results by using pull function and pluck function.\r\n\r\n\r\nkclusts_result_1_scale <- kclusts_1_scale %>%\r\n  pull(kclust) %>%\r\n  pluck(6)\r\n\r\n\r\n\r\nChecking on clustering results\r\nCheck cluster size\r\nAs discussed in my previous post, it is also quite crucial to check the number of data points within each cluster to ensure there is a sizable count within each cluster. The usual rule of thumb used in practice is to have at least 5 data points within each cluster.\r\nLet’s pull out the number of data points in each cluster.\r\n\r\n\r\nkclusts_result_1_scale$size\r\n\r\n\r\n[1] 6282  532  744 1534 5078 6501  293 1642\r\n\r\nFrom the results shown above, there are at least 5 data points within each cluster.\r\nCheck average values for each cluster\r\nObserving the average values of each cluster could give us a glimpse of the different characteristics of each cluster, allowing us to formulate different strategies for different groups of customers.\r\nThis is because “not all the customers are the same.”\r\nNote that we have previously performed standardization on the dataset before using it to run the clustering algorithm. Hence, instead of using the average values in the clustering results, I will compute the average values of each cluster by using the original dataset (i.e. the dataset before performing any standardization).\r\nTo do so, I will first insert back which cluster each data point belongs to back to the dataset.\r\n\r\n\r\ndf_1_num <- df_1 %>%\r\n  select_if(is.numeric)\r\n\r\ndf_1_num$cluster <- kclusts_result_1_scale$cluster\r\n\r\n\r\n\r\nThen, I will extract out all the column names.\r\n\r\n\r\nvar_list <- df_1_num %>%\r\n  names()\r\n\r\n\r\n\r\nOnce that is done, I will use loop function to calculate the average values for each cluster.\r\n\r\n\r\ncluster <- c(1,2,3,4,5,6,7,8)\r\nresult <- as.data.frame(cluster)\r\n\r\nfor(i in var_list){\r\n  temp_result <- df_1_num %>%\r\n    group_by(cluster) %>%\r\n    summarise({{i}} := mean(!!sym(i))) %>%\r\n    dplyr::select(-cluster)\r\n\r\n  result <- cbind(result, temp_result)\r\n\r\n}\r\n\r\nresult\r\n\r\n\r\n  cluster      age    balance       day duration  campaign\r\n1       1 35.64470   792.6613  8.171283 277.4908  2.207418\r\n2       2 43.73684 14940.4192 16.116541 255.8008  2.447368\r\n3       3 40.80511   951.4731 22.552419 146.7083 15.901882\r\n4       4 39.42894   903.2040 11.952412 240.2086  2.095176\r\n5       5 54.64120  1294.2125 15.476369 274.3899  2.412958\r\n6       6 35.68220   921.3529 23.470697 243.5325  2.553761\r\n7       7 41.35495  1440.7577 14.498294 246.1604  2.887372\r\n8       8 39.55055  1544.0225 16.370889 267.7808  1.946407\r\n      previous pdays_recoded\r\n1  0.036612544     2.1620503\r\n2  0.421052632    28.4041353\r\n3  0.005376344     0.4744624\r\n4  2.169491525   338.7992177\r\n5  0.086451359     6.1053564\r\n6  0.023073373     1.4911552\r\n7 12.283276451   211.6825939\r\n8  2.953105968   162.6619976\r\n\r\nFollowing are some observations from the results above and possible actions we could take:\r\nAll customers under Cluster 7 seems to be new customers since their average value of previous variable and pdays_recoded variable is 0\r\nTheir average balance is on the lower end\r\nSince we have just recently contacted this group of customers (i.e. the lead is still warm), maybe we could extract more info on this group of customers to attempt to do any up-selling or cross-selling to increase the ‘balance’ amount\r\n\r\nInterestingly enough, Cluster 2 has the highest average last contact duration (i.e. the duration variable)\r\nAlthough they have the highest average last contract duration, the average values of other features are not very different from other clusters\r\nPerhaps we could check with the business team to understand why this group of customers has the highest average last contract duration\r\n\r\nThe average balance for Cluster 6 is much higher than the rest of the group. It is about 12 times more than the average balance of the entire customer base\r\nCustomer under Cluster 2 seems to be quite similar to Cluster 6 as the average values for the features are quite similar (except for balance and duration)\r\nIs there anything we can learn from Cluster 6 so that we could “shift” the customers in Cluster 2 to Cluster 6?\r\n\r\nThe customers under Cluster 3 were contacted quite sometimes ago (about 8 months ago)\r\nWe could run campaigns to “warm-up” and recapture this group of customers\r\n\r\nParallel Coordinate Plot\r\nAlternatively, a good way to illustrate the clustering results is to use a parallel coordinate plot. This approach allows us to use the visualization effectively to compare the average value of the clusters.\r\nTo make the graph less clustered, I will plot out 1% of the clustering results so that it is easier for us to make comparisons across different clusters.\r\n\r\n\r\ndf_1_num$cluster <- as.factor(kclusts_result_1_scale$cluster)\r\n\r\nkclusts_result_1_scale_graph <- df_1_num %>%\r\n  sample_frac(size = 0.01) %>%\r\n  ggparcoord(columns = c(1:7), groupColumn = \"cluster\", scale = \"center\")\r\n\r\nggplotly(kclusts_result_1_scale_graph)\r\n\r\n\r\n\r\n{\"x\":{\"data\":[{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[-0.356420444283908,-0.0282069130059354,-0.390412979351032,0.201945824262481,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0517428210222142,0.0425779375530425,-0.157079646017699,-0.0966071079538178,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.213563301426765,-0.0272752832242029,-0.490412979351032,0.0747561822213536,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0109264944916019,0.00151017574606098,-0.0237463126843658,-0.113362629659835,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,-0.0278836945102323,-0.390412979351032,-0.13925752684186,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,0.00637746603429582,-0.490412979351032,-0.0615728352957828,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.153783637348745,0.00584510615902014,-0.290412979351032,-0.150681746186872,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0199743477918507,-0.223746312684366,-0.0105446555547318,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0502979953043164,0.00272699831811968,0.00958702064896755,0.808952678794088,0.229105211406096,-0.0357142857142857,-0.0589970501474926,null,-0.0911143218349287,0.0439278500939201,-0.390412979351032,-0.0234921041457447,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,0.00681476164612942,-0.0237463126843658,-0.156013048547877,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.112967310818133,-0.00835749479922764,-0.323746312684366,-0.165914038646887,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,-0.0219516844714461,-0.257079646017699,0.197376136524476,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,-0.0169132785090155,-0.390412979351032,-0.151443360809873,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0313346577569081,-0.0263056277370936,-0.290412979351032,-0.149920131563871,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.0282069130059354,-0.390412979351032,0.118929830355398,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.213563301426765,-0.0248416380800855,-0.0904129793510324,-0.115647473528837,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.254379627957378,-0.0198222449703433,-0.257079646017699,-0.117932317397839,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,-0.0186434481036614,-0.223746312684366,-0.155251433924876,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,-0.0331692675576123,-0.190412979351032,-0.00140528007872265,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0502979953043164,-0.0231494941908164,-0.290412979351032,0.0199199293652987,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.274787791222684,-0.0259443835360137,0.00958702064896755,0.396157553127675,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.0109264944916019,-0.0282069130059354,-0.357079646017699,0.206515512000485,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.0313346577569081,-0.0318003421640463,-0.123746312684366,-0.0105446555547318,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,0.000635584522393776,-0.323746312684366,-0.152966590055874,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,-0.0256021521876222,-0.0904129793510324,-0.0128294994237341,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,0.004381116502012,-0.190412979351032,-0.164390809400886,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0502979953043164,-0.0105249600057072,-0.357079646017699,-0.163629194777885,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.0214573503015472,-0.223746312684366,-0.0166375725387379,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,-0.0353367327640919,-0.0904129793510324,-0.152966590055874,0.0809570632579482,-0.0357142857142857,-0.0589970501474926,null,-0.254379627957378,-0.0258112935671948,-0.423746312684366,-0.0242537187687455,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0721509842875203,-0.0309827894984443,-0.157079646017699,-0.00369012394772492,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0359071183447444,-0.257079646017699,-0.0638576791647851,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.153783637348745,-0.0280738230371165,-0.390412979351032,-0.0531950744427744,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0251078180177233,-0.423746312684366,-0.00902142630873026,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,-0.0212291960692862,-0.323746312684366,-0.0920374202158133,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.00900393179063383,-0.290412979351032,-0.147635287694869,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.0101827286573157,-0.323746312684366,-0.0288234065067501,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.152338811630847,-0.0256021521876222,-0.0237463126843658,-0.0950838787078163,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.152338811630847,-0.0263816791478473,-0.0570796460176991,-0.175815028745897,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,-0.315604117753296,-0.0239480340037299,-0.257079646017699,-0.185716018844907,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,-0.0282069130059354,-0.290412979351032,-0.0158759579157371,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,-0.0140803634584413,-0.357079646017699,-0.184954404221906,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.315604117753296,-0.0145176590702749,-0.390412979351032,-0.0798515862478011,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,-0.0282069130059354,-0.0570796460176991,0.333705154041612,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,0.0486050118552708,-0.457079646017699,-0.0364395527367577,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.0271992318134493,-0.290412979351032,0.14101665442242,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0282069130059354,-0.157079646017699,-0.113362629659835,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0109264944916019,0.00801257136549972,-0.457079646017699,-0.073758669263795,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,-0.00923208602289484,-0.290412979351032,-0.0973687225768186,0.155031137332022,-0.0357142857142857,-0.0589970501474926,null,-0.315604117753296,-0.0108291656487219,-0.123746312684366,-0.057764762180779,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.0517428210222142,-0.0301081982747771,-0.357079646017699,-0.025776948014747,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,-0.0255261007768685,-0.0570796460176991,-0.0212072602767424,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0517428210222142,0.0061302989493464,-0.123746312684366,-0.0760435131327973,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.0517428210222142,-0.028054810184428,-0.157079646017699,-0.126310078250848,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.024499406731694,-0.323746312684366,-0.142303985333864,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.00331908883679703,-0.390412979351032,0.248404316265527,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.254379627957378,-0.0228833142531785,-0.0570796460176991,-0.0905141909698118,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0173315612681606,-0.423746312684366,-0.0196840310307409,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,-0.0277125788360365,-0.457079646017699,0.0123037831352911,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,-0.0282069130059354,-0.423746312684366,-0.105746483429827,-0.0671910848901999,0.0357142857142857,0.0700352079170235,null,-0.376828607549214,-0.0253930108080496,-0.457079646017699,0.508114902708787,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0925591475528265,-0.0267809490543041,-0.390412979351032,-0.0189224164077401,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,0.00597819612783906,-0.0904129793510324,-0.0493870013277706,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.0502979953043164,-0.0240811239725488,-0.223746312684366,0.175289312457454,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,0.0109264944916019,-0.0154873145573842,0.00958702064896755,0.0869420161893657,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.213563301426765,-0.0169893299197691,-0.157079646017699,0.138731810553417,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,-0.0305454938866107,-0.0904129793510324,0.643682305602922,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,0.0339841281378778,-0.390412979351032,0.524108809791803,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.0313346577569081,0.00540781054718654,-0.257079646017699,-0.0516718451967729,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926],\"text\":[\".ID: 1<br />cluster: 1<br />variable: age<br />value: -0.3564204443\",\".ID: 1<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 1<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 1<br />cluster: 1<br />variable: duration<br />value:  0.2019458243\",\".ID: 1<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 1<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 1<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 3<br />cluster: 1<br />variable: age<br />value:  0.0517428210\",\".ID: 3<br />cluster: 1<br />variable: balance<br />value:  0.0425779376\",\".ID: 3<br />cluster: 1<br />variable: day<br />value: -0.1570796460\",\".ID: 3<br />cluster: 1<br />variable: duration<br />value: -0.0966071080\",\".ID: 3<br />cluster: 1<br />variable: campaign<br />value:  0.0439200262\",\".ID: 3<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 3<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 4<br />cluster: 1<br />variable: age<br />value: -0.2135633014\",\".ID: 4<br />cluster: 1<br />variable: balance<br />value: -0.0272752832\",\".ID: 4<br />cluster: 1<br />variable: day<br />value: -0.4904129794\",\".ID: 4<br />cluster: 1<br />variable: duration<br />value:  0.0747561822\",\".ID: 4<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 4<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 4<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 7<br />cluster: 1<br />variable: age<br />value:  0.0109264945\",\".ID: 7<br />cluster: 1<br />variable: balance<br />value:  0.0015101757\",\".ID: 7<br />cluster: 1<br />variable: day<br />value: -0.0237463127\",\".ID: 7<br />cluster: 1<br />variable: duration<br />value: -0.1133626297\",\".ID: 7<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 7<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 7<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 8<br />cluster: 1<br />variable: age<br />value: -0.0094816688\",\".ID: 8<br />cluster: 1<br />variable: balance<br />value: -0.0278836945\",\".ID: 8<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 8<br />cluster: 1<br />variable: duration<br />value: -0.1392575268\",\".ID: 8<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 8<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 8<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 9<br />cluster: 1<br />variable: age<br />value:  0.1333754741\",\".ID: 9<br />cluster: 1<br />variable: balance<br />value:  0.0063774660\",\".ID: 9<br />cluster: 1<br />variable: day<br />value: -0.4904129794\",\".ID: 9<br />cluster: 1<br />variable: duration<br />value: -0.0615728353\",\".ID: 9<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 9<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 9<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 11<br />cluster: 1<br />variable: age<br />value:  0.1537836373\",\".ID: 11<br />cluster: 1<br />variable: balance<br />value:  0.0058451062\",\".ID: 11<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 11<br />cluster: 1<br />variable: duration<br />value: -0.1506817462\",\".ID: 11<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 11<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 11<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 12<br />cluster: 1<br />variable: age<br />value: -0.2339714647\",\".ID: 12<br />cluster: 1<br />variable: balance<br />value: -0.0199743478\",\".ID: 12<br />cluster: 1<br />variable: day<br />value: -0.2237463127\",\".ID: 12<br />cluster: 1<br />variable: duration<br />value: -0.0105446556\",\".ID: 12<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 12<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 12<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 16<br />cluster: 1<br />variable: age<br />value: -0.0502979953\",\".ID: 16<br />cluster: 1<br />variable: balance<br />value:  0.0027269983\",\".ID: 16<br />cluster: 1<br />variable: day<br />value:  0.0095870206\",\".ID: 16<br />cluster: 1<br />variable: duration<br />value:  0.8089526788\",\".ID: 16<br />cluster: 1<br />variable: campaign<br />value:  0.2291052114\",\".ID: 16<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 16<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 18<br />cluster: 1<br />variable: age<br />value: -0.0911143218\",\".ID: 18<br />cluster: 1<br />variable: balance<br />value:  0.0439278501\",\".ID: 18<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 18<br />cluster: 1<br />variable: duration<br />value: -0.0234921041\",\".ID: 18<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 18<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 18<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 19<br />cluster: 1<br />variable: age<br />value: -0.0094816688\",\".ID: 19<br />cluster: 1<br />variable: balance<br />value:  0.0068147616\",\".ID: 19<br />cluster: 1<br />variable: day<br />value: -0.0237463127\",\".ID: 19<br />cluster: 1<br />variable: duration<br />value: -0.1560130485\",\".ID: 19<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 19<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 19<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 21<br />cluster: 1<br />variable: age<br />value:  0.1129673108\",\".ID: 21<br />cluster: 1<br />variable: balance<br />value: -0.0083574948\",\".ID: 21<br />cluster: 1<br />variable: day<br />value: -0.3237463127\",\".ID: 21<br />cluster: 1<br />variable: duration<br />value: -0.1659140386\",\".ID: 21<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 21<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 21<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 22<br />cluster: 1<br />variable: age<br />value: -0.1115224851\",\".ID: 22<br />cluster: 1<br />variable: balance<br />value: -0.0219516845\",\".ID: 22<br />cluster: 1<br />variable: day<br />value: -0.2570796460\",\".ID: 22<br />cluster: 1<br />variable: duration<br />value:  0.1973761365\",\".ID: 22<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 22<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 22<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 23<br />cluster: 1<br />variable: age<br />value: -0.0707061586\",\".ID: 23<br />cluster: 1<br />variable: balance<br />value: -0.0169132785\",\".ID: 23<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 23<br />cluster: 1<br />variable: duration<br />value: -0.1514433608\",\".ID: 23<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 23<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 23<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 30<br />cluster: 1<br />variable: age<br />value:  0.0313346578\",\".ID: 30<br />cluster: 1<br />variable: balance<br />value: -0.0263056277\",\".ID: 30<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 30<br />cluster: 1<br />variable: duration<br />value: -0.1499201316\",\".ID: 30<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 30<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 30<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 40<br />cluster: 1<br />variable: age<br />value: -0.1727469749\",\".ID: 40<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 40<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 40<br />cluster: 1<br />variable: duration<br />value:  0.1189298304\",\".ID: 40<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 40<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 40<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 47<br />cluster: 1<br />variable: age<br />value: -0.2135633014\",\".ID: 47<br />cluster: 1<br />variable: balance<br />value: -0.0248416381\",\".ID: 47<br />cluster: 1<br />variable: day<br />value: -0.0904129794\",\".ID: 47<br />cluster: 1<br />variable: duration<br />value: -0.1156474735\",\".ID: 47<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 47<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 47<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 55<br />cluster: 1<br />variable: age<br />value: -0.2543796280\",\".ID: 55<br />cluster: 1<br />variable: balance<br />value: -0.0198222450\",\".ID: 55<br />cluster: 1<br />variable: day<br />value: -0.2570796460\",\".ID: 55<br />cluster: 1<br />variable: duration<br />value: -0.1179323174\",\".ID: 55<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 55<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 55<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 67<br />cluster: 1<br />variable: age<br />value: -0.1115224851\",\".ID: 67<br />cluster: 1<br />variable: balance<br />value: -0.0186434481\",\".ID: 67<br />cluster: 1<br />variable: day<br />value: -0.2237463127\",\".ID: 67<br />cluster: 1<br />variable: duration<br />value: -0.1552514339\",\".ID: 67<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 67<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 67<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 69<br />cluster: 1<br />variable: age<br />value: -0.0707061586\",\".ID: 69<br />cluster: 1<br />variable: balance<br />value: -0.0331692676\",\".ID: 69<br />cluster: 1<br />variable: day<br />value: -0.1904129794\",\".ID: 69<br />cluster: 1<br />variable: duration<br />value: -0.0014052801\",\".ID: 69<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 69<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 69<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 79<br />cluster: 1<br />variable: age<br />value: -0.0502979953\",\".ID: 79<br />cluster: 1<br />variable: balance<br />value: -0.0231494942\",\".ID: 79<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 79<br />cluster: 1<br />variable: duration<br />value:  0.0199199294\",\".ID: 79<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 79<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 79<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 81<br />cluster: 1<br />variable: age<br />value: -0.2747877912\",\".ID: 81<br />cluster: 1<br />variable: balance<br />value: -0.0259443835\",\".ID: 81<br />cluster: 1<br />variable: day<br />value:  0.0095870206\",\".ID: 81<br />cluster: 1<br />variable: duration<br />value:  0.3961575531\",\".ID: 81<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 81<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 81<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 85<br />cluster: 1<br />variable: age<br />value:  0.0109264945\",\".ID: 85<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 85<br />cluster: 1<br />variable: day<br />value: -0.3570796460\",\".ID: 85<br />cluster: 1<br />variable: duration<br />value:  0.2065155120\",\".ID: 85<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 85<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 85<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 89<br />cluster: 1<br />variable: age<br />value:  0.0313346578\",\".ID: 89<br />cluster: 1<br />variable: balance<br />value: -0.0318003422\",\".ID: 89<br />cluster: 1<br />variable: day<br />value: -0.1237463127\",\".ID: 89<br />cluster: 1<br />variable: duration<br />value: -0.0105446556\",\".ID: 89<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 89<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 89<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 90<br />cluster: 1<br />variable: age<br />value: -0.0707061586\",\".ID: 90<br />cluster: 1<br />variable: balance<br />value:  0.0006355845\",\".ID: 90<br />cluster: 1<br />variable: day<br />value: -0.3237463127\",\".ID: 90<br />cluster: 1<br />variable: duration<br />value: -0.1529665901\",\".ID: 90<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 90<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 90<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 92<br />cluster: 1<br />variable: age<br />value: -0.0707061586\",\".ID: 92<br />cluster: 1<br />variable: balance<br />value: -0.0256021522\",\".ID: 92<br />cluster: 1<br />variable: day<br />value: -0.0904129794\",\".ID: 92<br />cluster: 1<br />variable: duration<br />value: -0.0128294994\",\".ID: 92<br />cluster: 1<br />variable: campaign<br />value:  0.1179941003\",\".ID: 92<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 92<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 93<br />cluster: 1<br />variable: age<br />value: -0.2339714647\",\".ID: 93<br />cluster: 1<br />variable: balance<br />value:  0.0043811165\",\".ID: 93<br />cluster: 1<br />variable: day<br />value: -0.1904129794\",\".ID: 93<br />cluster: 1<br />variable: duration<br />value: -0.1643908094\",\".ID: 93<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 93<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 93<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 94<br />cluster: 1<br />variable: age<br />value: -0.0502979953\",\".ID: 94<br />cluster: 1<br />variable: balance<br />value: -0.0105249600\",\".ID: 94<br />cluster: 1<br />variable: day<br />value: -0.3570796460\",\".ID: 94<br />cluster: 1<br />variable: duration<br />value: -0.1636291948\",\".ID: 94<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 94<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 94<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 95<br />cluster: 1<br />variable: age<br />value: -0.1931551382\",\".ID: 95<br />cluster: 1<br />variable: balance<br />value: -0.0214573503\",\".ID: 95<br />cluster: 1<br />variable: day<br />value: -0.2237463127\",\".ID: 95<br />cluster: 1<br />variable: duration<br />value: -0.0166375725\",\".ID: 95<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 95<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 95<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 102<br />cluster: 1<br />variable: age<br />value: -0.0094816688\",\".ID: 102<br />cluster: 1<br />variable: balance<br />value: -0.0353367328\",\".ID: 102<br />cluster: 1<br />variable: day<br />value: -0.0904129794\",\".ID: 102<br />cluster: 1<br />variable: duration<br />value: -0.1529665901\",\".ID: 102<br />cluster: 1<br />variable: campaign<br />value:  0.0809570633\",\".ID: 102<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 102<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 104<br />cluster: 1<br />variable: age<br />value: -0.2543796280\",\".ID: 104<br />cluster: 1<br />variable: balance<br />value: -0.0258112936\",\".ID: 104<br />cluster: 1<br />variable: day<br />value: -0.4237463127\",\".ID: 104<br />cluster: 1<br />variable: duration<br />value: -0.0242537188\",\".ID: 104<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 104<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 104<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 105<br />cluster: 1<br />variable: age<br />value:  0.0721509843\",\".ID: 105<br />cluster: 1<br />variable: balance<br />value: -0.0309827895\",\".ID: 105<br />cluster: 1<br />variable: day<br />value: -0.1570796460\",\".ID: 105<br />cluster: 1<br />variable: duration<br />value: -0.0036901239\",\".ID: 105<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 105<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 105<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 106<br />cluster: 1<br />variable: age<br />value: -0.0298898320\",\".ID: 106<br />cluster: 1<br />variable: balance<br />value: -0.0359071183\",\".ID: 106<br />cluster: 1<br />variable: day<br />value: -0.2570796460\",\".ID: 106<br />cluster: 1<br />variable: duration<br />value: -0.0638576792\",\".ID: 106<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 106<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 106<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 109<br />cluster: 1<br />variable: age<br />value:  0.1537836373\",\".ID: 109<br />cluster: 1<br />variable: balance<br />value: -0.0280738230\",\".ID: 109<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 109<br />cluster: 1<br />variable: duration<br />value: -0.0531950744\",\".ID: 109<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 109<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 109<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 110<br />cluster: 1<br />variable: age<br />value: -0.2339714647\",\".ID: 110<br />cluster: 1<br />variable: balance<br />value: -0.0251078180\",\".ID: 110<br />cluster: 1<br />variable: day<br />value: -0.4237463127\",\".ID: 110<br />cluster: 1<br />variable: duration<br />value: -0.0090214263\",\".ID: 110<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 110<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 110<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 112<br />cluster: 1<br />variable: age<br />value: -0.1319306484\",\".ID: 112<br />cluster: 1<br />variable: balance<br />value: -0.0212291961\",\".ID: 112<br />cluster: 1<br />variable: day<br />value: -0.3237463127\",\".ID: 112<br />cluster: 1<br />variable: duration<br />value: -0.0920374202\",\".ID: 112<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 112<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 112<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 114<br />cluster: 1<br />variable: age<br />value: -0.1727469749\",\".ID: 114<br />cluster: 1<br />variable: balance<br />value: -0.0090039318\",\".ID: 114<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 114<br />cluster: 1<br />variable: duration<br />value: -0.1476352877\",\".ID: 114<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 114<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 114<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 115<br />cluster: 1<br />variable: age<br />value: -0.1727469749\",\".ID: 115<br />cluster: 1<br />variable: balance<br />value: -0.0101827287\",\".ID: 115<br />cluster: 1<br />variable: day<br />value: -0.3237463127\",\".ID: 115<br />cluster: 1<br />variable: duration<br />value: -0.0288234065\",\".ID: 115<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 115<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 115<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 120<br />cluster: 1<br />variable: age<br />value: -0.1523388116\",\".ID: 120<br />cluster: 1<br />variable: balance<br />value: -0.0256021522\",\".ID: 120<br />cluster: 1<br />variable: day<br />value: -0.0237463127\",\".ID: 120<br />cluster: 1<br />variable: duration<br />value: -0.0950838787\",\".ID: 120<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 120<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 120<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 122<br />cluster: 1<br />variable: age<br />value: -0.1523388116\",\".ID: 122<br />cluster: 1<br />variable: balance<br />value: -0.0263816791\",\".ID: 122<br />cluster: 1<br />variable: day<br />value: -0.0570796460\",\".ID: 122<br />cluster: 1<br />variable: duration<br />value: -0.1758150287\",\".ID: 122<br />cluster: 1<br />variable: campaign<br />value:  0.1179941003\",\".ID: 122<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 122<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 124<br />cluster: 1<br />variable: age<br />value: -0.3156041178\",\".ID: 124<br />cluster: 1<br />variable: balance<br />value: -0.0239480340\",\".ID: 124<br />cluster: 1<br />variable: day<br />value: -0.2570796460\",\".ID: 124<br />cluster: 1<br />variable: duration<br />value: -0.1857160188\",\".ID: 124<br />cluster: 1<br />variable: campaign<br />value:  0.0439200262\",\".ID: 124<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 124<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 129<br />cluster: 1<br />variable: age<br />value:  0.1333754741\",\".ID: 129<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 129<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 129<br />cluster: 1<br />variable: duration<br />value: -0.0158759579\",\".ID: 129<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 129<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 129<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 139<br />cluster: 1<br />variable: age<br />value: -0.0094816688\",\".ID: 139<br />cluster: 1<br />variable: balance<br />value: -0.0140803635\",\".ID: 139<br />cluster: 1<br />variable: day<br />value: -0.3570796460\",\".ID: 139<br />cluster: 1<br />variable: duration<br />value: -0.1849544042\",\".ID: 139<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 139<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 139<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 140<br />cluster: 1<br />variable: age<br />value: -0.3156041178\",\".ID: 140<br />cluster: 1<br />variable: balance<br />value: -0.0145176591\",\".ID: 140<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 140<br />cluster: 1<br />variable: duration<br />value: -0.0798515862\",\".ID: 140<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 140<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 140<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 141<br />cluster: 1<br />variable: age<br />value: -0.0094816688\",\".ID: 141<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 141<br />cluster: 1<br />variable: day<br />value: -0.0570796460\",\".ID: 141<br />cluster: 1<br />variable: duration<br />value:  0.3337051540\",\".ID: 141<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 141<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 141<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 147<br />cluster: 1<br />variable: age<br />value: -0.1931551382\",\".ID: 147<br />cluster: 1<br />variable: balance<br />value:  0.0486050119\",\".ID: 147<br />cluster: 1<br />variable: day<br />value: -0.4570796460\",\".ID: 147<br />cluster: 1<br />variable: duration<br />value: -0.0364395527\",\".ID: 147<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 147<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 147<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 149<br />cluster: 1<br />variable: age<br />value: -0.1931551382\",\".ID: 149<br />cluster: 1<br />variable: balance<br />value: -0.0271992318\",\".ID: 149<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 149<br />cluster: 1<br />variable: duration<br />value:  0.1410166544\",\".ID: 149<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 149<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 149<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 151<br />cluster: 1<br />variable: age<br />value: -0.0298898320\",\".ID: 151<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 151<br />cluster: 1<br />variable: day<br />value: -0.1570796460\",\".ID: 151<br />cluster: 1<br />variable: duration<br />value: -0.1133626297\",\".ID: 151<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 151<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 151<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 159<br />cluster: 1<br />variable: age<br />value:  0.0109264945\",\".ID: 159<br />cluster: 1<br />variable: balance<br />value:  0.0080125714\",\".ID: 159<br />cluster: 1<br />variable: day<br />value: -0.4570796460\",\".ID: 159<br />cluster: 1<br />variable: duration<br />value: -0.0737586693\",\".ID: 159<br />cluster: 1<br />variable: campaign<br />value:  0.0439200262\",\".ID: 159<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 159<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 161<br />cluster: 1<br />variable: age<br />value: -0.0094816688\",\".ID: 161<br />cluster: 1<br />variable: balance<br />value: -0.0092320860\",\".ID: 161<br />cluster: 1<br />variable: day<br />value: -0.2904129794\",\".ID: 161<br />cluster: 1<br />variable: duration<br />value: -0.0973687226\",\".ID: 161<br />cluster: 1<br />variable: campaign<br />value:  0.1550311373\",\".ID: 161<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 161<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 174<br />cluster: 1<br />variable: age<br />value: -0.3156041178\",\".ID: 174<br />cluster: 1<br />variable: balance<br />value: -0.0108291656\",\".ID: 174<br />cluster: 1<br />variable: day<br />value: -0.1237463127\",\".ID: 174<br />cluster: 1<br />variable: duration<br />value: -0.0577647622\",\".ID: 174<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 174<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 174<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 176<br />cluster: 1<br />variable: age<br />value:  0.0517428210\",\".ID: 176<br />cluster: 1<br />variable: balance<br />value: -0.0301081983\",\".ID: 176<br />cluster: 1<br />variable: day<br />value: -0.3570796460\",\".ID: 176<br />cluster: 1<br />variable: duration<br />value: -0.0257769480\",\".ID: 176<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 176<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 176<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 178<br />cluster: 1<br />variable: age<br />value: -0.0707061586\",\".ID: 178<br />cluster: 1<br />variable: balance<br />value: -0.0255261008\",\".ID: 178<br />cluster: 1<br />variable: day<br />value: -0.0570796460\",\".ID: 178<br />cluster: 1<br />variable: duration<br />value: -0.0212072603\",\".ID: 178<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 178<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 178<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 179<br />cluster: 1<br />variable: age<br />value:  0.0517428210\",\".ID: 179<br />cluster: 1<br />variable: balance<br />value:  0.0061302989\",\".ID: 179<br />cluster: 1<br />variable: day<br />value: -0.1237463127\",\".ID: 179<br />cluster: 1<br />variable: duration<br />value: -0.0760435131\",\".ID: 179<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 179<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 179<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 180<br />cluster: 1<br />variable: age<br />value:  0.0517428210\",\".ID: 180<br />cluster: 1<br />variable: balance<br />value: -0.0280548102\",\".ID: 180<br />cluster: 1<br />variable: day<br />value: -0.1570796460\",\".ID: 180<br />cluster: 1<br />variable: duration<br />value: -0.1263100783\",\".ID: 180<br />cluster: 1<br />variable: campaign<br />value:  0.0439200262\",\".ID: 180<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 180<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 186<br />cluster: 1<br />variable: age<br />value: -0.0298898320\",\".ID: 186<br />cluster: 1<br />variable: balance<br />value: -0.0244994067\",\".ID: 186<br />cluster: 1<br />variable: day<br />value: -0.3237463127\",\".ID: 186<br />cluster: 1<br />variable: duration<br />value: -0.1423039853\",\".ID: 186<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 186<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 186<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 187<br />cluster: 1<br />variable: age<br />value: -0.0298898320\",\".ID: 187<br />cluster: 1<br />variable: balance<br />value: -0.0033190888\",\".ID: 187<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 187<br />cluster: 1<br />variable: duration<br />value:  0.2484043163\",\".ID: 187<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 187<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 187<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 190<br />cluster: 1<br />variable: age<br />value: -0.2543796280\",\".ID: 190<br />cluster: 1<br />variable: balance<br />value: -0.0228833143\",\".ID: 190<br />cluster: 1<br />variable: day<br />value: -0.0570796460\",\".ID: 190<br />cluster: 1<br />variable: duration<br />value: -0.0905141910\",\".ID: 190<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 190<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 190<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 195<br />cluster: 1<br />variable: age<br />value: -0.0298898320\",\".ID: 195<br />cluster: 1<br />variable: balance<br />value: -0.0173315613\",\".ID: 195<br />cluster: 1<br />variable: day<br />value: -0.4237463127\",\".ID: 195<br />cluster: 1<br />variable: duration<br />value: -0.0196840310\",\".ID: 195<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 195<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 195<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 202<br />cluster: 1<br />variable: age<br />value: -0.1115224851\",\".ID: 202<br />cluster: 1<br />variable: balance<br />value: -0.0277125788\",\".ID: 202<br />cluster: 1<br />variable: day<br />value: -0.4570796460\",\".ID: 202<br />cluster: 1<br />variable: duration<br />value:  0.0123037831\",\".ID: 202<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 202<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 202<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 203<br />cluster: 1<br />variable: age<br />value: -0.1319306484\",\".ID: 203<br />cluster: 1<br />variable: balance<br />value: -0.0282069130\",\".ID: 203<br />cluster: 1<br />variable: day<br />value: -0.4237463127\",\".ID: 203<br />cluster: 1<br />variable: duration<br />value: -0.1057464834\",\".ID: 203<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 203<br />cluster: 1<br />variable: previous<br />value:  0.0357142857\",\".ID: 203<br />cluster: 1<br />variable: pdays_recoded<br />value:  0.0700352079\",null,\".ID: 205<br />cluster: 1<br />variable: age<br />value: -0.3768286075\",\".ID: 205<br />cluster: 1<br />variable: balance<br />value: -0.0253930108\",\".ID: 205<br />cluster: 1<br />variable: day<br />value: -0.4570796460\",\".ID: 205<br />cluster: 1<br />variable: duration<br />value:  0.5081149027\",\".ID: 205<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 205<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 205<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 206<br />cluster: 1<br />variable: age<br />value:  0.0925591476\",\".ID: 206<br />cluster: 1<br />variable: balance<br />value: -0.0267809491\",\".ID: 206<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 206<br />cluster: 1<br />variable: duration<br />value: -0.0189224164\",\".ID: 206<br />cluster: 1<br />variable: campaign<br />value: -0.0671910849\",\".ID: 206<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 206<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 207<br />cluster: 1<br />variable: age<br />value: -0.1319306484\",\".ID: 207<br />cluster: 1<br />variable: balance<br />value:  0.0059781961\",\".ID: 207<br />cluster: 1<br />variable: day<br />value: -0.0904129794\",\".ID: 207<br />cluster: 1<br />variable: duration<br />value: -0.0493870013\",\".ID: 207<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 207<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 207<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 209<br />cluster: 1<br />variable: age<br />value: -0.0502979953\",\".ID: 209<br />cluster: 1<br />variable: balance<br />value: -0.0240811240\",\".ID: 209<br />cluster: 1<br />variable: day<br />value: -0.2237463127\",\".ID: 209<br />cluster: 1<br />variable: duration<br />value:  0.1752893125\",\".ID: 209<br />cluster: 1<br />variable: campaign<br />value:  0.1179941003\",\".ID: 209<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 209<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 216<br />cluster: 1<br />variable: age<br />value:  0.0109264945\",\".ID: 216<br />cluster: 1<br />variable: balance<br />value: -0.0154873146\",\".ID: 216<br />cluster: 1<br />variable: day<br />value:  0.0095870206\",\".ID: 216<br />cluster: 1<br />variable: duration<br />value:  0.0869420162\",\".ID: 216<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 216<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 216<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 217<br />cluster: 1<br />variable: age<br />value: -0.2135633014\",\".ID: 217<br />cluster: 1<br />variable: balance<br />value: -0.0169893299\",\".ID: 217<br />cluster: 1<br />variable: day<br />value: -0.1570796460\",\".ID: 217<br />cluster: 1<br />variable: duration<br />value:  0.1387318106\",\".ID: 217<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 217<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 217<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 220<br />cluster: 1<br />variable: age<br />value: -0.1115224851\",\".ID: 220<br />cluster: 1<br />variable: balance<br />value: -0.0305454939\",\".ID: 220<br />cluster: 1<br />variable: day<br />value: -0.0904129794\",\".ID: 220<br />cluster: 1<br />variable: duration<br />value:  0.6436823056\",\".ID: 220<br />cluster: 1<br />variable: campaign<br />value:  0.0439200262\",\".ID: 220<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 220<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 225<br />cluster: 1<br />variable: age<br />value: -0.1727469749\",\".ID: 225<br />cluster: 1<br />variable: balance<br />value:  0.0339841281\",\".ID: 225<br />cluster: 1<br />variable: day<br />value: -0.3904129794\",\".ID: 225<br />cluster: 1<br />variable: duration<br />value:  0.5241088098\",\".ID: 225<br />cluster: 1<br />variable: campaign<br />value:  0.0068829892\",\".ID: 225<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 225<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 226<br />cluster: 1<br />variable: age<br />value:  0.0313346578\",\".ID: 226<br />cluster: 1<br />variable: balance<br />value:  0.0054078105\",\".ID: 226<br />cluster: 1<br />variable: day<br />value: -0.2570796460\",\".ID: 226<br />cluster: 1<br />variable: duration<br />value: -0.0516718452\",\".ID: 226<br />cluster: 1<br />variable: campaign<br />value: -0.0301540479\",\".ID: 226<br />cluster: 1<br />variable: previous<br />value: -0.0357142857\",\".ID: 226<br />cluster: 1<br />variable: pdays_recoded<br />value: -0.0589970501\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"1\",\"legendgroup\":\"1\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[0.153783637348745,0.240786926829794,0.0762536873156342,-0.133926224480855,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.152338811630847,0.155647372491061,0.142920353982301,-0.0432940843437645,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0502979953043164,0.159659084408317,0.442920353982301,0.179859000195459,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.194599963879357,0.126538695025094,0.0762536873156342,-0.00369012394772492,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,0.265712776704309,0.342920353982301,-0.0120678848007333,0.00688298918387414,0.464285714285714,0.114838075300536,null,-0.131930648365541,0.312997741340403,-0.323746312684366,-0.0021668947017234,0.229105211406096,-0.0357142857142857,-0.0589970501474926,null,0.317048943471194,0.949795216433566,-0.257079646017699,0.0488612850393276,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.112967310818133,0.168252893823481,-0.223746312684366,0.242311399281521,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,0.132185512273554,0.109587020648968,-0.0752818985097965,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.3374571067365,0.217382105170352,-0.457079646017699,-0.129356536742851,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,0.265997969494635,0.176253687315634,-0.130879765988852,-0.0671910848901999,0.107142857142857,0.105877501823834],\"text\":[\".ID: 29<br />cluster: 2<br />variable: age<br />value:  0.1537836373\",\".ID: 29<br />cluster: 2<br />variable: balance<br />value:  0.2407869268\",\".ID: 29<br />cluster: 2<br />variable: day<br />value:  0.0762536873\",\".ID: 29<br />cluster: 2<br />variable: duration<br />value: -0.1339262245\",\".ID: 29<br />cluster: 2<br />variable: campaign<br />value: -0.0301540479\",\".ID: 29<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 29<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 54<br />cluster: 2<br />variable: age<br />value: -0.1523388116\",\".ID: 54<br />cluster: 2<br />variable: balance<br />value:  0.1556473725\",\".ID: 54<br />cluster: 2<br />variable: day<br />value:  0.1429203540\",\".ID: 54<br />cluster: 2<br />variable: duration<br />value: -0.0432940843\",\".ID: 54<br />cluster: 2<br />variable: campaign<br />value: -0.0671910849\",\".ID: 54<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 54<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 113<br />cluster: 2<br />variable: age<br />value: -0.0502979953\",\".ID: 113<br />cluster: 2<br />variable: balance<br />value:  0.1596590844\",\".ID: 113<br />cluster: 2<br />variable: day<br />value:  0.4429203540\",\".ID: 113<br />cluster: 2<br />variable: duration<br />value:  0.1798590002\",\".ID: 113<br />cluster: 2<br />variable: campaign<br />value: -0.0301540479\",\".ID: 113<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 113<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 130<br />cluster: 2<br />variable: age<br />value:  0.1945999639\",\".ID: 130<br />cluster: 2<br />variable: balance<br />value:  0.1265386950\",\".ID: 130<br />cluster: 2<br />variable: day<br />value:  0.0762536873\",\".ID: 130<br />cluster: 2<br />variable: duration<br />value: -0.0036901239\",\".ID: 130<br />cluster: 2<br />variable: campaign<br />value:  0.1179941003\",\".ID: 130<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 130<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 132<br />cluster: 2<br />variable: age<br />value: -0.2339714647\",\".ID: 132<br />cluster: 2<br />variable: balance<br />value:  0.2657127767\",\".ID: 132<br />cluster: 2<br />variable: day<br />value:  0.3429203540\",\".ID: 132<br />cluster: 2<br />variable: duration<br />value: -0.0120678848\",\".ID: 132<br />cluster: 2<br />variable: campaign<br />value:  0.0068829892\",\".ID: 132<br />cluster: 2<br />variable: previous<br />value:  0.4642857143\",\".ID: 132<br />cluster: 2<br />variable: pdays_recoded<br />value:  0.1148380753\",null,\".ID: 135<br />cluster: 2<br />variable: age<br />value: -0.1319306484\",\".ID: 135<br />cluster: 2<br />variable: balance<br />value:  0.3129977413\",\".ID: 135<br />cluster: 2<br />variable: day<br />value: -0.3237463127\",\".ID: 135<br />cluster: 2<br />variable: duration<br />value: -0.0021668947\",\".ID: 135<br />cluster: 2<br />variable: campaign<br />value:  0.2291052114\",\".ID: 135<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 135<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 144<br />cluster: 2<br />variable: age<br />value:  0.3170489435\",\".ID: 144<br />cluster: 2<br />variable: balance<br />value:  0.9497952164\",\".ID: 144<br />cluster: 2<br />variable: day<br />value: -0.2570796460\",\".ID: 144<br />cluster: 2<br />variable: duration<br />value:  0.0488612850\",\".ID: 144<br />cluster: 2<br />variable: campaign<br />value: -0.0671910849\",\".ID: 144<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 144<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 172<br />cluster: 2<br />variable: age<br />value:  0.1129673108\",\".ID: 172<br />cluster: 2<br />variable: balance<br />value:  0.1682528938\",\".ID: 172<br />cluster: 2<br />variable: day<br />value: -0.2237463127\",\".ID: 172<br />cluster: 2<br />variable: duration<br />value:  0.2423113993\",\".ID: 172<br />cluster: 2<br />variable: campaign<br />value:  0.0439200262\",\".ID: 172<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 172<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 198<br />cluster: 2<br />variable: age<br />value: -0.1931551382\",\".ID: 198<br />cluster: 2<br />variable: balance<br />value:  0.1321855123\",\".ID: 198<br />cluster: 2<br />variable: day<br />value:  0.1095870206\",\".ID: 198<br />cluster: 2<br />variable: duration<br />value: -0.0752818985\",\".ID: 198<br />cluster: 2<br />variable: campaign<br />value: -0.0301540479\",\".ID: 198<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 198<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 199<br />cluster: 2<br />variable: age<br />value:  0.3374571067\",\".ID: 199<br />cluster: 2<br />variable: balance<br />value:  0.2173821052\",\".ID: 199<br />cluster: 2<br />variable: day<br />value: -0.4570796460\",\".ID: 199<br />cluster: 2<br />variable: duration<br />value: -0.1293565367\",\".ID: 199<br />cluster: 2<br />variable: campaign<br />value:  0.0068829892\",\".ID: 199<br />cluster: 2<br />variable: previous<br />value: -0.0357142857\",\".ID: 199<br />cluster: 2<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 219<br />cluster: 2<br />variable: age<br />value: -0.1115224851\",\".ID: 219<br />cluster: 2<br />variable: balance<br />value:  0.2659979695\",\".ID: 219<br />cluster: 2<br />variable: day<br />value:  0.1762536873\",\".ID: 219<br />cluster: 2<br />variable: duration<br />value: -0.1308797660\",\".ID: 219<br />cluster: 2<br />variable: campaign<br />value: -0.0671910849\",\".ID: 219<br />cluster: 2<br />variable: previous<br />value:  0.1071428571\",\".ID: 219<br />cluster: 2<br />variable: pdays_recoded<br />value:  0.1058775018\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(205,150,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"2\",\"legendgroup\":\"2\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[-0.00948166877370415,-0.0168372270982618,-0.0904129793510324,-0.146112058448867,0.30317928548017,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0266098333801083,-0.0237463126843658,-0.0295850211297508,0.340216322517207,-0.0357142857142857,-0.0589970501474926,null,0.0721509842875203,-0.0187765380724804,0.442920353982301,-0.157536277793879,0.451327433628319,-0.0357142857142857,-0.0589970501474926,null,0.0925591475528265,-0.0250317666069697,0.476253687315634,-0.124786849004846,0.30317928548017,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0282069130059354,0.509587020648968,-0.163629194777885,0.9328089151098,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.0282069130059354,0.409587020648968,-0.184954404221906,0.451327433628319,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,-0.0265147691166662,0.109587020648968,-0.0752818985097965,0.266142248443133,-0.0357142857142857,-0.0589970501474926,null,0.174191800614051,0.00947656102250785,0.209587020648968,-0.143065599956864,0.451327433628319,-0.0357142857142857,-0.0589970501474926],\"text\":[\".ID: 6<br />cluster: 3<br />variable: age<br />value: -0.0094816688\",\".ID: 6<br />cluster: 3<br />variable: balance<br />value: -0.0168372271\",\".ID: 6<br />cluster: 3<br />variable: day<br />value: -0.0904129794\",\".ID: 6<br />cluster: 3<br />variable: duration<br />value: -0.1461120584\",\".ID: 6<br />cluster: 3<br />variable: campaign<br />value:  0.3031792855\",\".ID: 6<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 6<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 17<br />cluster: 3<br />variable: age<br />value: -0.2339714647\",\".ID: 17<br />cluster: 3<br />variable: balance<br />value: -0.0266098334\",\".ID: 17<br />cluster: 3<br />variable: day<br />value: -0.0237463127\",\".ID: 17<br />cluster: 3<br />variable: duration<br />value: -0.0295850211\",\".ID: 17<br />cluster: 3<br />variable: campaign<br />value:  0.3402163225\",\".ID: 17<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 17<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 59<br />cluster: 3<br />variable: age<br />value:  0.0721509843\",\".ID: 59<br />cluster: 3<br />variable: balance<br />value: -0.0187765381\",\".ID: 59<br />cluster: 3<br />variable: day<br />value:  0.4429203540\",\".ID: 59<br />cluster: 3<br />variable: duration<br />value: -0.1575362778\",\".ID: 59<br />cluster: 3<br />variable: campaign<br />value:  0.4513274336\",\".ID: 59<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 59<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 66<br />cluster: 3<br />variable: age<br />value:  0.0925591476\",\".ID: 66<br />cluster: 3<br />variable: balance<br />value: -0.0250317666\",\".ID: 66<br />cluster: 3<br />variable: day<br />value:  0.4762536873\",\".ID: 66<br />cluster: 3<br />variable: duration<br />value: -0.1247868490\",\".ID: 66<br />cluster: 3<br />variable: campaign<br />value:  0.3031792855\",\".ID: 66<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 66<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 108<br />cluster: 3<br />variable: age<br />value: -0.2339714647\",\".ID: 108<br />cluster: 3<br />variable: balance<br />value: -0.0282069130\",\".ID: 108<br />cluster: 3<br />variable: day<br />value:  0.5095870206\",\".ID: 108<br />cluster: 3<br />variable: duration<br />value: -0.1636291948\",\".ID: 108<br />cluster: 3<br />variable: campaign<br />value:  0.9328089151\",\".ID: 108<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 108<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 154<br />cluster: 3<br />variable: age<br />value: -0.1931551382\",\".ID: 154<br />cluster: 3<br />variable: balance<br />value: -0.0282069130\",\".ID: 154<br />cluster: 3<br />variable: day<br />value:  0.4095870206\",\".ID: 154<br />cluster: 3<br />variable: duration<br />value: -0.1849544042\",\".ID: 154<br />cluster: 3<br />variable: campaign<br />value:  0.4513274336\",\".ID: 154<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 154<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 197<br />cluster: 3<br />variable: age<br />value: -0.0707061586\",\".ID: 197<br />cluster: 3<br />variable: balance<br />value: -0.0265147691\",\".ID: 197<br />cluster: 3<br />variable: day<br />value:  0.1095870206\",\".ID: 197<br />cluster: 3<br />variable: duration<br />value: -0.0752818985\",\".ID: 197<br />cluster: 3<br />variable: campaign<br />value:  0.2661422484\",\".ID: 197<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 197<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 204<br />cluster: 3<br />variable: age<br />value:  0.1741918006\",\".ID: 204<br />cluster: 3<br />variable: balance<br />value:  0.0094765610\",\".ID: 204<br />cluster: 3<br />variable: day<br />value:  0.2095870206\",\".ID: 204<br />cluster: 3<br />variable: duration<br />value: -0.1430656000\",\".ID: 204<br />cluster: 3<br />variable: campaign<br />value:  0.4513274336\",\".ID: 204<br />cluster: 3<br />variable: previous<br />value: -0.0357142857\",\".ID: 204<br />cluster: 3<br />variable: pdays_recoded<br />value: -0.0589970501\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(124,174,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"3\",\"legendgroup\":\"3\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[0.317048943471194,0.00960965099132677,-0.423746312684366,0.057239045892336,-0.0301540478531629,0.107142857142857,0.444587179243188,null,0.0925591475528265,-0.0265147691166662,0.476253687315634,-0.143065599956864,-0.0301540478531629,0.107142857142857,0.568243093221683,null,0.0313346577569081,-0.00611397818199439,-0.0904129793510324,0.110552069502389,0.0809570632579482,0.0357142857142857,0.59333269895645,null,0.3374571067365,-0.0263246405897821,-0.157079646017699,-0.0973687225768186,-0.0301540478531629,0.107142857142857,0.604085387128493,null,-0.152338811630847,0.0397830482078451,-0.457079646017699,-0.104223254183825,-0.0671910848901999,0.107142857142857,0.492974276017382,null,-0.172746974896153,-0.0212291960692862,-0.157079646017699,-0.0356779381137569,-0.0301540478531629,0.178571428571429,0.435626605766486,null,0.398681596532418,-0.0125403223906795,-0.223746312684366,-0.101938410314823,-0.0671910848901999,0.464285714285714,0.941002949852507,null,-0.254379627957378,-0.0123882195691721,0.442920353982301,0.13111566432341,0.00688298918387414,0.0357142857142857,0.553906175658959,null,-0.0911143218349287,-0.0178829339961247,0.0429203539823009,-0.121740390512843,-0.0671910848901999,0.107142857142857,0.423081802899102,null,-0.0707061585696225,-0.011152384144425,-0.0904129793510324,-0.108031327298829,-0.0301540478531629,0.107142857142857,0.442795064547848,null,-0.356420444283908,-0.0266288462327967,-0.323746312684366,-0.0471021574587683,-0.0671910848901999,0.0357142857142857,0.582580010784407],\"text\":[\".ID: 15<br />cluster: 4<br />variable: age<br />value:  0.3170489435\",\".ID: 15<br />cluster: 4<br />variable: balance<br />value:  0.0096096510\",\".ID: 15<br />cluster: 4<br />variable: day<br />value: -0.4237463127\",\".ID: 15<br />cluster: 4<br />variable: duration<br />value:  0.0572390459\",\".ID: 15<br />cluster: 4<br />variable: campaign<br />value: -0.0301540479\",\".ID: 15<br />cluster: 4<br />variable: previous<br />value:  0.1071428571\",\".ID: 15<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.4445871792\",null,\".ID: 32<br />cluster: 4<br />variable: age<br />value:  0.0925591476\",\".ID: 32<br />cluster: 4<br />variable: balance<br />value: -0.0265147691\",\".ID: 32<br />cluster: 4<br />variable: day<br />value:  0.4762536873\",\".ID: 32<br />cluster: 4<br />variable: duration<br />value: -0.1430656000\",\".ID: 32<br />cluster: 4<br />variable: campaign<br />value: -0.0301540479\",\".ID: 32<br />cluster: 4<br />variable: previous<br />value:  0.1071428571\",\".ID: 32<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.5682430932\",null,\".ID: 37<br />cluster: 4<br />variable: age<br />value:  0.0313346578\",\".ID: 37<br />cluster: 4<br />variable: balance<br />value: -0.0061139782\",\".ID: 37<br />cluster: 4<br />variable: day<br />value: -0.0904129794\",\".ID: 37<br />cluster: 4<br />variable: duration<br />value:  0.1105520695\",\".ID: 37<br />cluster: 4<br />variable: campaign<br />value:  0.0809570633\",\".ID: 37<br />cluster: 4<br />variable: previous<br />value:  0.0357142857\",\".ID: 37<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.5933326990\",null,\".ID: 62<br />cluster: 4<br />variable: age<br />value:  0.3374571067\",\".ID: 62<br />cluster: 4<br />variable: balance<br />value: -0.0263246406\",\".ID: 62<br />cluster: 4<br />variable: day<br />value: -0.1570796460\",\".ID: 62<br />cluster: 4<br />variable: duration<br />value: -0.0973687226\",\".ID: 62<br />cluster: 4<br />variable: campaign<br />value: -0.0301540479\",\".ID: 62<br />cluster: 4<br />variable: previous<br />value:  0.1071428571\",\".ID: 62<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.6040853871\",null,\".ID: 64<br />cluster: 4<br />variable: age<br />value: -0.1523388116\",\".ID: 64<br />cluster: 4<br />variable: balance<br />value:  0.0397830482\",\".ID: 64<br />cluster: 4<br />variable: day<br />value: -0.4570796460\",\".ID: 64<br />cluster: 4<br />variable: duration<br />value: -0.1042232542\",\".ID: 64<br />cluster: 4<br />variable: campaign<br />value: -0.0671910849\",\".ID: 64<br />cluster: 4<br />variable: previous<br />value:  0.1071428571\",\".ID: 64<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.4929742760\",null,\".ID: 74<br />cluster: 4<br />variable: age<br />value: -0.1727469749\",\".ID: 74<br />cluster: 4<br />variable: balance<br />value: -0.0212291961\",\".ID: 74<br />cluster: 4<br />variable: day<br />value: -0.1570796460\",\".ID: 74<br />cluster: 4<br />variable: duration<br />value: -0.0356779381\",\".ID: 74<br />cluster: 4<br />variable: campaign<br />value: -0.0301540479\",\".ID: 74<br />cluster: 4<br />variable: previous<br />value:  0.1785714286\",\".ID: 74<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.4356266058\",null,\".ID: 111<br />cluster: 4<br />variable: age<br />value:  0.3986815965\",\".ID: 111<br />cluster: 4<br />variable: balance<br />value: -0.0125403224\",\".ID: 111<br />cluster: 4<br />variable: day<br />value: -0.2237463127\",\".ID: 111<br />cluster: 4<br />variable: duration<br />value: -0.1019384103\",\".ID: 111<br />cluster: 4<br />variable: campaign<br />value: -0.0671910849\",\".ID: 111<br />cluster: 4<br />variable: previous<br />value:  0.4642857143\",\".ID: 111<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.9410029499\",null,\".ID: 123<br />cluster: 4<br />variable: age<br />value: -0.2543796280\",\".ID: 123<br />cluster: 4<br />variable: balance<br />value: -0.0123882196\",\".ID: 123<br />cluster: 4<br />variable: day<br />value:  0.4429203540\",\".ID: 123<br />cluster: 4<br />variable: duration<br />value:  0.1311156643\",\".ID: 123<br />cluster: 4<br />variable: campaign<br />value:  0.0068829892\",\".ID: 123<br />cluster: 4<br />variable: previous<br />value:  0.0357142857\",\".ID: 123<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.5539061757\",null,\".ID: 125<br />cluster: 4<br />variable: age<br />value: -0.0911143218\",\".ID: 125<br />cluster: 4<br />variable: balance<br />value: -0.0178829340\",\".ID: 125<br />cluster: 4<br />variable: day<br />value:  0.0429203540\",\".ID: 125<br />cluster: 4<br />variable: duration<br />value: -0.1217403905\",\".ID: 125<br />cluster: 4<br />variable: campaign<br />value: -0.0671910849\",\".ID: 125<br />cluster: 4<br />variable: previous<br />value:  0.1071428571\",\".ID: 125<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.4230818029\",null,\".ID: 138<br />cluster: 4<br />variable: age<br />value: -0.0707061586\",\".ID: 138<br />cluster: 4<br />variable: balance<br />value: -0.0111523841\",\".ID: 138<br />cluster: 4<br />variable: day<br />value: -0.0904129794\",\".ID: 138<br />cluster: 4<br />variable: duration<br />value: -0.1080313273\",\".ID: 138<br />cluster: 4<br />variable: campaign<br />value: -0.0301540479\",\".ID: 138<br />cluster: 4<br />variable: previous<br />value:  0.1071428571\",\".ID: 138<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.4427950645\",null,\".ID: 171<br />cluster: 4<br />variable: age<br />value: -0.3564204443\",\".ID: 171<br />cluster: 4<br />variable: balance<br />value: -0.0266288462\",\".ID: 171<br />cluster: 4<br />variable: day<br />value: -0.3237463127\",\".ID: 171<br />cluster: 4<br />variable: duration<br />value: -0.0471021575\",\".ID: 171<br />cluster: 4<br />variable: campaign<br />value: -0.0671910849\",\".ID: 171<br />cluster: 4<br />variable: previous<br />value:  0.0357142857\",\".ID: 171<br />cluster: 4<br />variable: pdays_recoded<br />value:  0.5825800108\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,190,103,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"4\",\"legendgroup\":\"4\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[0.235416290409969,-0.0282069130059354,-0.0570796460176991,-0.124786849004846,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.153783637348745,-0.00541050263252295,-0.0904129793510324,-0.140780756087862,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.153783637348745,-0.0502047835664343,0.0429203539823009,-0.103461639560825,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.112967310818133,-0.0153732374412536,-0.123746312684366,-0.0539566890657752,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.215008127144663,-0.0199553349391622,-0.357079646017699,0.192044834163471,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.378273433267112,-0.0258493192725716,0.0429203539823009,-0.167437267892889,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.296640780205888,-0.0112664612605555,0.109587020648968,0.00544925152828421,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.296640780205888,-0.0282069130059354,-0.357079646017699,0.0557158166463345,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.3374571067365,-0.0245754581424477,-0.0570796460176991,-0.101938410314823,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,0.174191800614051,-0.0277696173941018,0.242920353982301,-0.0105446555547318,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0517428210222142,-0.0459268917115404,0.00958702064896755,0.233172023805512,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0925591475528265,-0.023814944034911,0.0762536873156342,-0.0836596593628049,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.215008127144663,-0.0177878697326827,-0.123746312684366,-0.0935606494618148,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.398681596532418,0.0399921895874177,-0.157079646017699,-0.0973687225768186,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.623171392450786,0.0259797171560541,0.0429203539823009,-0.057764762180779,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,-0.0246895352585782,-0.0237463126843658,-0.110316171167832,0.0809570632579482,-0.0357142857142857,-0.0589970501474926,null,0.357865270001806,0.0572938855338775,0.442920353982301,-0.101176795691822,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.296640780205888,-0.0282069130059354,-0.490412979351032,-0.117932317397839,-0.0671910848901999,0.0357142857142857,0.104085387128493,null,0.398681596532418,-0.0342339873081637,0.376253687315634,0.180620614818459,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.255824453675275,-0.0280167844790512,0.142920353982301,-0.141542370710863,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.153783637348745,-0.020126450613358,-0.223746312684366,0.329135466303608,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.317048943471194,0.0327482927131307,0.0762536873156342,-0.100415181068822,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.317048943471194,-0.0241001368252372,0.109587020648968,-0.140019141464861,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.276232616940581,-0.0282069130059354,-0.357079646017699,0.367977812076647,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,0.00124399580842313,-0.0570796460176991,-0.126310078250848,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,0.0197815271796301,-0.123746312684366,-0.11869393202084,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.398681596532418,-0.0152211346197463,-0.257079646017699,0.187475146425466,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.215008127144663,-0.0224460186413449,-0.423746312684366,-0.132402995234854,0.155031137332022,-0.0357142857142857,-0.0589970501474926,null,0.276232616940581,-0.000543212344288099,0.0762536873156342,-0.130118151365851,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,0.357865270001806,-0.0255641264822454,-0.157079646017699,-0.120217161266841,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.317048943471194,0.00597819612783906,0.442920353982301,-0.0250153333917462,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.378273433267112,-0.0289864399661605,0.142920353982301,-0.0684273669027897,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.215008127144663,-0.0280357973317396,0.0762536873156342,0.0838955576973627,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.541538739389561,-0.0197842192649665,0.0762536873156342,0.137208581307416,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.398681596532418,-0.0282069130059354,-0.357079646017699,0.260590150233539,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.255824453675275,-0.0272752832242029,0.409587020648968,-0.0828980447398041,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,0.00616832465472323,0.0429203539823009,-0.103461639560825,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.235416290409969,-0.0276555402779713,0.176253687315634,-0.0120678848007333,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.276232616940581,-0.0180920753756973,-0.0570796460176991,-0.0311082503757523,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.296640780205888,-0.0253359722499844,-0.0237463126843658,-0.131641380611853,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.0721509842875203,-0.0282069130059354,0.00958702064896755,-0.13925752684186,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0925591475528265,-0.00305290889915919,0.0429203539823009,0.220224575214499,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.459906086328337,-0.0289484142607837,-0.490412979351032,0.0960813916653748,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,0.0413611149809838,-0.0570796460176991,0.0785642553363574,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,0.235416290409969,0.0121383603988863,0.109587020648968,0.0473380557933261,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,0.296640780205888,-0.0176737926165522,0.309587020648968,-0.0204456456537417,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926],\"text\":[\".ID: 5<br />cluster: 5<br />variable: age<br />value:  0.2354162904\",\".ID: 5<br />cluster: 5<br />variable: balance<br />value: -0.0282069130\",\".ID: 5<br />cluster: 5<br />variable: day<br />value: -0.0570796460\",\".ID: 5<br />cluster: 5<br />variable: duration<br />value: -0.1247868490\",\".ID: 5<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 5<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 5<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 13<br />cluster: 5<br />variable: age<br />value:  0.1537836373\",\".ID: 13<br />cluster: 5<br />variable: balance<br />value: -0.0054105026\",\".ID: 13<br />cluster: 5<br />variable: day<br />value: -0.0904129794\",\".ID: 13<br />cluster: 5<br />variable: duration<br />value: -0.1407807561\",\".ID: 13<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 13<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 13<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 14<br />cluster: 5<br />variable: age<br />value:  0.1537836373\",\".ID: 14<br />cluster: 5<br />variable: balance<br />value: -0.0502047836\",\".ID: 14<br />cluster: 5<br />variable: day<br />value:  0.0429203540\",\".ID: 14<br />cluster: 5<br />variable: duration<br />value: -0.1034616396\",\".ID: 14<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 14<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 14<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 25<br />cluster: 5<br />variable: age<br />value:  0.1129673108\",\".ID: 25<br />cluster: 5<br />variable: balance<br />value: -0.0153732374\",\".ID: 25<br />cluster: 5<br />variable: day<br />value: -0.1237463127\",\".ID: 25<br />cluster: 5<br />variable: duration<br />value: -0.0539566891\",\".ID: 25<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 25<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 25<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 27<br />cluster: 5<br />variable: age<br />value:  0.2150081271\",\".ID: 27<br />cluster: 5<br />variable: balance<br />value: -0.0199553349\",\".ID: 27<br />cluster: 5<br />variable: day<br />value: -0.3570796460\",\".ID: 27<br />cluster: 5<br />variable: duration<br />value:  0.1920448342\",\".ID: 27<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 27<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 27<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 44<br />cluster: 5<br />variable: age<br />value:  0.3782734333\",\".ID: 44<br />cluster: 5<br />variable: balance<br />value: -0.0258493193\",\".ID: 44<br />cluster: 5<br />variable: day<br />value:  0.0429203540\",\".ID: 44<br />cluster: 5<br />variable: duration<br />value: -0.1674372679\",\".ID: 44<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 44<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 44<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 51<br />cluster: 5<br />variable: age<br />value:  0.2966407802\",\".ID: 51<br />cluster: 5<br />variable: balance<br />value: -0.0112664613\",\".ID: 51<br />cluster: 5<br />variable: day<br />value:  0.1095870206\",\".ID: 51<br />cluster: 5<br />variable: duration<br />value:  0.0054492515\",\".ID: 51<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 51<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 51<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 52<br />cluster: 5<br />variable: age<br />value:  0.2966407802\",\".ID: 52<br />cluster: 5<br />variable: balance<br />value: -0.0282069130\",\".ID: 52<br />cluster: 5<br />variable: day<br />value: -0.3570796460\",\".ID: 52<br />cluster: 5<br />variable: duration<br />value:  0.0557158166\",\".ID: 52<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 52<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 52<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 53<br />cluster: 5<br />variable: age<br />value:  0.3374571067\",\".ID: 53<br />cluster: 5<br />variable: balance<br />value: -0.0245754581\",\".ID: 53<br />cluster: 5<br />variable: day<br />value: -0.0570796460\",\".ID: 53<br />cluster: 5<br />variable: duration<br />value: -0.1019384103\",\".ID: 53<br />cluster: 5<br />variable: campaign<br />value:  0.0439200262\",\".ID: 53<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 53<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 56<br />cluster: 5<br />variable: age<br />value:  0.1741918006\",\".ID: 56<br />cluster: 5<br />variable: balance<br />value: -0.0277696174\",\".ID: 56<br />cluster: 5<br />variable: day<br />value:  0.2429203540\",\".ID: 56<br />cluster: 5<br />variable: duration<br />value: -0.0105446556\",\".ID: 56<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 56<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 56<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 58<br />cluster: 5<br />variable: age<br />value:  0.0517428210\",\".ID: 58<br />cluster: 5<br />variable: balance<br />value: -0.0459268917\",\".ID: 58<br />cluster: 5<br />variable: day<br />value:  0.0095870206\",\".ID: 58<br />cluster: 5<br />variable: duration<br />value:  0.2331720238\",\".ID: 58<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 58<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 58<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 70<br />cluster: 5<br />variable: age<br />value:  0.0925591476\",\".ID: 70<br />cluster: 5<br />variable: balance<br />value: -0.0238149440\",\".ID: 70<br />cluster: 5<br />variable: day<br />value:  0.0762536873\",\".ID: 70<br />cluster: 5<br />variable: duration<br />value: -0.0836596594\",\".ID: 70<br />cluster: 5<br />variable: campaign<br />value:  0.0068829892\",\".ID: 70<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 70<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 71<br />cluster: 5<br />variable: age<br />value:  0.2150081271\",\".ID: 71<br />cluster: 5<br />variable: balance<br />value: -0.0177878697\",\".ID: 71<br />cluster: 5<br />variable: day<br />value: -0.1237463127\",\".ID: 71<br />cluster: 5<br />variable: duration<br />value: -0.0935606495\",\".ID: 71<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 71<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 71<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 73<br />cluster: 5<br />variable: age<br />value:  0.3986815965\",\".ID: 73<br />cluster: 5<br />variable: balance<br />value:  0.0399921896\",\".ID: 73<br />cluster: 5<br />variable: day<br />value: -0.1570796460\",\".ID: 73<br />cluster: 5<br />variable: duration<br />value: -0.0973687226\",\".ID: 73<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 73<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 73<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 75<br />cluster: 5<br />variable: age<br />value:  0.6231713925\",\".ID: 75<br />cluster: 5<br />variable: balance<br />value:  0.0259797172\",\".ID: 75<br />cluster: 5<br />variable: day<br />value:  0.0429203540\",\".ID: 75<br />cluster: 5<br />variable: duration<br />value: -0.0577647622\",\".ID: 75<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 75<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 75<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 77<br />cluster: 5<br />variable: age<br />value:  0.1333754741\",\".ID: 77<br />cluster: 5<br />variable: balance<br />value: -0.0246895353\",\".ID: 77<br />cluster: 5<br />variable: day<br />value: -0.0237463127\",\".ID: 77<br />cluster: 5<br />variable: duration<br />value: -0.1103161712\",\".ID: 77<br />cluster: 5<br />variable: campaign<br />value:  0.0809570633\",\".ID: 77<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 77<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 78<br />cluster: 5<br />variable: age<br />value:  0.3578652700\",\".ID: 78<br />cluster: 5<br />variable: balance<br />value:  0.0572938855\",\".ID: 78<br />cluster: 5<br />variable: day<br />value:  0.4429203540\",\".ID: 78<br />cluster: 5<br />variable: duration<br />value: -0.1011767957\",\".ID: 78<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 78<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 78<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 82<br />cluster: 5<br />variable: age<br />value:  0.2966407802\",\".ID: 82<br />cluster: 5<br />variable: balance<br />value: -0.0282069130\",\".ID: 82<br />cluster: 5<br />variable: day<br />value: -0.4904129794\",\".ID: 82<br />cluster: 5<br />variable: duration<br />value: -0.1179323174\",\".ID: 82<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 82<br />cluster: 5<br />variable: previous<br />value:  0.0357142857\",\".ID: 82<br />cluster: 5<br />variable: pdays_recoded<br />value:  0.1040853871\",null,\".ID: 83<br />cluster: 5<br />variable: age<br />value:  0.3986815965\",\".ID: 83<br />cluster: 5<br />variable: balance<br />value: -0.0342339873\",\".ID: 83<br />cluster: 5<br />variable: day<br />value:  0.3762536873\",\".ID: 83<br />cluster: 5<br />variable: duration<br />value:  0.1806206148\",\".ID: 83<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 83<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 83<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 84<br />cluster: 5<br />variable: age<br />value:  0.2558244537\",\".ID: 84<br />cluster: 5<br />variable: balance<br />value: -0.0280167845\",\".ID: 84<br />cluster: 5<br />variable: day<br />value:  0.1429203540\",\".ID: 84<br />cluster: 5<br />variable: duration<br />value: -0.1415423707\",\".ID: 84<br />cluster: 5<br />variable: campaign<br />value:  0.0068829892\",\".ID: 84<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 84<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 86<br />cluster: 5<br />variable: age<br />value:  0.1537836373\",\".ID: 86<br />cluster: 5<br />variable: balance<br />value: -0.0201264506\",\".ID: 86<br />cluster: 5<br />variable: day<br />value: -0.2237463127\",\".ID: 86<br />cluster: 5<br />variable: duration<br />value:  0.3291354663\",\".ID: 86<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 86<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 86<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 88<br />cluster: 5<br />variable: age<br />value:  0.3170489435\",\".ID: 88<br />cluster: 5<br />variable: balance<br />value:  0.0327482927\",\".ID: 88<br />cluster: 5<br />variable: day<br />value:  0.0762536873\",\".ID: 88<br />cluster: 5<br />variable: duration<br />value: -0.1004151811\",\".ID: 88<br />cluster: 5<br />variable: campaign<br />value:  0.0068829892\",\".ID: 88<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 88<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 96<br />cluster: 5<br />variable: age<br />value:  0.3170489435\",\".ID: 96<br />cluster: 5<br />variable: balance<br />value: -0.0241001368\",\".ID: 96<br />cluster: 5<br />variable: day<br />value:  0.1095870206\",\".ID: 96<br />cluster: 5<br />variable: duration<br />value: -0.1400191415\",\".ID: 96<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 96<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 96<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 97<br />cluster: 5<br />variable: age<br />value:  0.2762326169\",\".ID: 97<br />cluster: 5<br />variable: balance<br />value: -0.0282069130\",\".ID: 97<br />cluster: 5<br />variable: day<br />value: -0.3570796460\",\".ID: 97<br />cluster: 5<br />variable: duration<br />value:  0.3679778121\",\".ID: 97<br />cluster: 5<br />variable: campaign<br />value:  0.0068829892\",\".ID: 97<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 97<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 98<br />cluster: 5<br />variable: age<br />value:  0.1333754741\",\".ID: 98<br />cluster: 5<br />variable: balance<br />value:  0.0012439958\",\".ID: 98<br />cluster: 5<br />variable: day<br />value: -0.0570796460\",\".ID: 98<br />cluster: 5<br />variable: duration<br />value: -0.1263100783\",\".ID: 98<br />cluster: 5<br />variable: campaign<br />value:  0.1179941003\",\".ID: 98<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 98<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 99<br />cluster: 5<br />variable: age<br />value:  0.1333754741\",\".ID: 99<br />cluster: 5<br />variable: balance<br />value:  0.0197815272\",\".ID: 99<br />cluster: 5<br />variable: day<br />value: -0.1237463127\",\".ID: 99<br />cluster: 5<br />variable: duration<br />value: -0.1186939320\",\".ID: 99<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 99<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 99<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 116<br />cluster: 5<br />variable: age<br />value:  0.3986815965\",\".ID: 116<br />cluster: 5<br />variable: balance<br />value: -0.0152211346\",\".ID: 116<br />cluster: 5<br />variable: day<br />value: -0.2570796460\",\".ID: 116<br />cluster: 5<br />variable: duration<br />value:  0.1874751464\",\".ID: 116<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 116<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 116<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 126<br />cluster: 5<br />variable: age<br />value:  0.2150081271\",\".ID: 126<br />cluster: 5<br />variable: balance<br />value: -0.0224460186\",\".ID: 126<br />cluster: 5<br />variable: day<br />value: -0.4237463127\",\".ID: 126<br />cluster: 5<br />variable: duration<br />value: -0.1324029952\",\".ID: 126<br />cluster: 5<br />variable: campaign<br />value:  0.1550311373\",\".ID: 126<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 126<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 128<br />cluster: 5<br />variable: age<br />value:  0.2762326169\",\".ID: 128<br />cluster: 5<br />variable: balance<br />value: -0.0005432123\",\".ID: 128<br />cluster: 5<br />variable: day<br />value:  0.0762536873\",\".ID: 128<br />cluster: 5<br />variable: duration<br />value: -0.1301181514\",\".ID: 128<br />cluster: 5<br />variable: campaign<br />value:  0.0439200262\",\".ID: 128<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 128<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 143<br />cluster: 5<br />variable: age<br />value:  0.3578652700\",\".ID: 143<br />cluster: 5<br />variable: balance<br />value: -0.0255641265\",\".ID: 143<br />cluster: 5<br />variable: day<br />value: -0.1570796460\",\".ID: 143<br />cluster: 5<br />variable: duration<br />value: -0.1202171613\",\".ID: 143<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 143<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 143<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 145<br />cluster: 5<br />variable: age<br />value:  0.3170489435\",\".ID: 145<br />cluster: 5<br />variable: balance<br />value:  0.0059781961\",\".ID: 145<br />cluster: 5<br />variable: day<br />value:  0.4429203540\",\".ID: 145<br />cluster: 5<br />variable: duration<br />value: -0.0250153334\",\".ID: 145<br />cluster: 5<br />variable: campaign<br />value:  0.0068829892\",\".ID: 145<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 145<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 146<br />cluster: 5<br />variable: age<br />value:  0.3782734333\",\".ID: 146<br />cluster: 5<br />variable: balance<br />value: -0.0289864400\",\".ID: 146<br />cluster: 5<br />variable: day<br />value:  0.1429203540\",\".ID: 146<br />cluster: 5<br />variable: duration<br />value: -0.0684273669\",\".ID: 146<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 146<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 146<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 148<br />cluster: 5<br />variable: age<br />value:  0.2150081271\",\".ID: 148<br />cluster: 5<br />variable: balance<br />value: -0.0280357973\",\".ID: 148<br />cluster: 5<br />variable: day<br />value:  0.0762536873\",\".ID: 148<br />cluster: 5<br />variable: duration<br />value:  0.0838955577\",\".ID: 148<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 148<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 148<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 155<br />cluster: 5<br />variable: age<br />value:  0.5415387394\",\".ID: 155<br />cluster: 5<br />variable: balance<br />value: -0.0197842193\",\".ID: 155<br />cluster: 5<br />variable: day<br />value:  0.0762536873\",\".ID: 155<br />cluster: 5<br />variable: duration<br />value:  0.1372085813\",\".ID: 155<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 155<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 155<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 158<br />cluster: 5<br />variable: age<br />value:  0.3986815965\",\".ID: 158<br />cluster: 5<br />variable: balance<br />value: -0.0282069130\",\".ID: 158<br />cluster: 5<br />variable: day<br />value: -0.3570796460\",\".ID: 158<br />cluster: 5<br />variable: duration<br />value:  0.2605901502\",\".ID: 158<br />cluster: 5<br />variable: campaign<br />value:  0.0068829892\",\".ID: 158<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 158<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 166<br />cluster: 5<br />variable: age<br />value:  0.2558244537\",\".ID: 166<br />cluster: 5<br />variable: balance<br />value: -0.0272752832\",\".ID: 166<br />cluster: 5<br />variable: day<br />value:  0.4095870206\",\".ID: 166<br />cluster: 5<br />variable: duration<br />value: -0.0828980447\",\".ID: 166<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 166<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 166<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 175<br />cluster: 5<br />variable: age<br />value:  0.1333754741\",\".ID: 175<br />cluster: 5<br />variable: balance<br />value:  0.0061683247\",\".ID: 175<br />cluster: 5<br />variable: day<br />value:  0.0429203540\",\".ID: 175<br />cluster: 5<br />variable: duration<br />value: -0.1034616396\",\".ID: 175<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 175<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 175<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 181<br />cluster: 5<br />variable: age<br />value:  0.2354162904\",\".ID: 181<br />cluster: 5<br />variable: balance<br />value: -0.0276555403\",\".ID: 181<br />cluster: 5<br />variable: day<br />value:  0.1762536873\",\".ID: 181<br />cluster: 5<br />variable: duration<br />value: -0.0120678848\",\".ID: 181<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 181<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 181<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 183<br />cluster: 5<br />variable: age<br />value:  0.2762326169\",\".ID: 183<br />cluster: 5<br />variable: balance<br />value: -0.0180920754\",\".ID: 183<br />cluster: 5<br />variable: day<br />value: -0.0570796460\",\".ID: 183<br />cluster: 5<br />variable: duration<br />value: -0.0311082504\",\".ID: 183<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 183<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 183<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 191<br />cluster: 5<br />variable: age<br />value:  0.2966407802\",\".ID: 191<br />cluster: 5<br />variable: balance<br />value: -0.0253359722\",\".ID: 191<br />cluster: 5<br />variable: day<br />value: -0.0237463127\",\".ID: 191<br />cluster: 5<br />variable: duration<br />value: -0.1316413806\",\".ID: 191<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 191<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 191<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 200<br />cluster: 5<br />variable: age<br />value:  0.0721509843\",\".ID: 200<br />cluster: 5<br />variable: balance<br />value: -0.0282069130\",\".ID: 200<br />cluster: 5<br />variable: day<br />value:  0.0095870206\",\".ID: 200<br />cluster: 5<br />variable: duration<br />value: -0.1392575268\",\".ID: 200<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 200<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 200<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 201<br />cluster: 5<br />variable: age<br />value:  0.0925591476\",\".ID: 201<br />cluster: 5<br />variable: balance<br />value: -0.0030529089\",\".ID: 201<br />cluster: 5<br />variable: day<br />value:  0.0429203540\",\".ID: 201<br />cluster: 5<br />variable: duration<br />value:  0.2202245752\",\".ID: 201<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 201<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 201<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 210<br />cluster: 5<br />variable: age<br />value:  0.4599060863\",\".ID: 210<br />cluster: 5<br />variable: balance<br />value: -0.0289484143\",\".ID: 210<br />cluster: 5<br />variable: day<br />value: -0.4904129794\",\".ID: 210<br />cluster: 5<br />variable: duration<br />value:  0.0960813917\",\".ID: 210<br />cluster: 5<br />variable: campaign<br />value: -0.0301540479\",\".ID: 210<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 210<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 212<br />cluster: 5<br />variable: age<br />value:  0.1333754741\",\".ID: 212<br />cluster: 5<br />variable: balance<br />value:  0.0413611150\",\".ID: 212<br />cluster: 5<br />variable: day<br />value: -0.0570796460\",\".ID: 212<br />cluster: 5<br />variable: duration<br />value:  0.0785642553\",\".ID: 212<br />cluster: 5<br />variable: campaign<br />value:  0.0439200262\",\".ID: 212<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 212<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 213<br />cluster: 5<br />variable: age<br />value:  0.2354162904\",\".ID: 213<br />cluster: 5<br />variable: balance<br />value:  0.0121383604\",\".ID: 213<br />cluster: 5<br />variable: day<br />value:  0.1095870206\",\".ID: 213<br />cluster: 5<br />variable: duration<br />value:  0.0473380558\",\".ID: 213<br />cluster: 5<br />variable: campaign<br />value:  0.1179941003\",\".ID: 213<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 213<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 223<br />cluster: 5<br />variable: age<br />value:  0.2966407802\",\".ID: 223<br />cluster: 5<br />variable: balance<br />value: -0.0176737926\",\".ID: 223<br />cluster: 5<br />variable: day<br />value:  0.3095870206\",\".ID: 223<br />cluster: 5<br />variable: duration<br />value: -0.0204456457\",\".ID: 223<br />cluster: 5<br />variable: campaign<br />value: -0.0671910849\",\".ID: 223<br />cluster: 5<br />variable: previous<br />value: -0.0357142857\",\".ID: 223<br />cluster: 5<br />variable: pdays_recoded<br />value: -0.0589970501\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,196,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"5\",\"legendgroup\":\"5\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[-0.193155138161459,-0.0279597459209859,0.409587020648968,-0.114885858905836,0.0809570632579482,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0273133089295798,0.00958702064896755,-0.134687839103856,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.015525340262761,0.142920353982301,-0.0828980447398041,0.0809570632579482,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.0114185640820628,0.476253687315634,-0.153728204678875,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.29519595448799,-0.0281498744478701,0.242920353982301,-0.117932317397839,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.274787791222684,-0.0305645067392991,0.0762536873156342,-0.0333930942447546,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,0.00400085944824365,0.509587020648968,-0.156774663170878,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0270851546973187,0.442920353982301,0.307810256859587,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.254379627957378,-0.028054810184428,0.376253687315634,0.789150698596068,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,0.0756412883782003,0.476253687315634,0.128830820454408,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0517428210222142,-0.0143085176907023,0.442920353982301,0.0755177968443543,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.0502979953043164,-0.0157344816423336,0.209587020648968,0.188236761048467,0.155031137332022,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.0228072628424248,0.409587020648968,-0.150681746186872,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.133375474083439,-0.00470702708305151,0.409587020648968,-0.0745202838867958,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,-0.0175977412057985,0.176253687315634,-0.165914038646887,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.152338811630847,-0.0281688873005585,0.00958702064896755,-0.0981303371998194,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,0.0138305042881554,0.309587020648968,-0.089752576346811,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0225600957574754,0.0429203539823009,-0.0158759579157371,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.112967310818133,-0.0241761882359909,0.442920353982301,0.193568063409472,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.0312299565833937,0.176253687315634,-0.161344350908883,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0109264944916019,-0.0236628412134036,0.442920353982301,0.0945581624193733,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.0277696173941018,0.309587020648968,-0.106508098052828,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.315604117753296,-0.0226551600209175,0.0762536873156342,0.583514750385862,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.152338811630847,-0.0253739979553612,0.209587020648968,-0.0821364301168034,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.0313346577569081,-0.0332072932629891,0.209587020648968,0.0351522218253139,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.274787791222684,-0.0243663167628751,0.176253687315634,0.317711246958596,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0707061585696225,-0.00470702708305151,0.442920353982301,0.0298209194643086,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.152338811630847,0.00280304972887336,0.142920353982301,0.0800874845823589,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,-0.315604117753296,-0.0221988515563955,0.109587020648968,0.0755177968443543,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.00948166877370415,-0.0282069130059354,0.509587020648968,-0.0752818985097965,0.0809570632579482,-0.0357142857142857,-0.0589970501474926,null,-0.0911143218349287,-0.0243092782048098,0.0762536873156342,-0.0646192937877859,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,-0.0387970719533838,0.176253687315634,-0.00902142630873026,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0911143218349287,-0.00472603993573992,0.309587020648968,-0.133926224480855,0.117994100294985,-0.0357142857142857,-0.0589970501474926,null,0.112967310818133,-0.0220087230295113,0.342920353982301,-0.0600496060497813,0.155031137332022,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0237008669187805,0.00958702064896755,-0.0349163234907562,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.213563301426765,-0.0226171343155407,0.00958702064896755,-0.160582736285882,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,0.00516064346223711,0.176253687315634,0.249165930888528,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.274787791222684,-0.0282069130059354,0.409587020648968,-0.0973687225768186,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.00137977786257847,0.376253687315634,-0.106508098052828,0.0439200262209112,-0.0357142857142857,-0.0589970501474926,null,0.0721509842875203,-0.0140993763111297,0.476253687315634,-0.140019141464861,0.0809570632579482,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0282069130059354,0.00958702064896755,-0.18876247733691,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.0252028822811654,0.409587020648968,-0.0790899716248003,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,-0.0197842192649665,0.476253687315634,-0.191047321205912,-0.0671910848901999,0.0357142857142857,-0.0554128207568116,null,-0.172746974896153,-0.0257352421564411,0.342920353982301,0.110552069502389,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,-0.0317242907532926,0.442920353982301,-0.111839400413833,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.0911143218349287,-0.00552457974865345,0.476253687315634,0.288769891284568,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.0282069130059354,0.176253687315634,0.132638893569411,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.153783637348745,-0.0282069130059354,0.476253687315634,0.3207577054506,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,-0.111522485100235,-0.0269520647284998,0.509587020648968,-0.0364395527367577,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.028054810184428,0.442920353982301,0.584276365008863,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,0.0925591475528265,-0.027370347487645,0.476253687315634,-0.00825981168572948,-0.0301540478531629,0.0357142857142857,-0.0554128207568116,null,0.194599963879357,-0.021856620208004,0.376253687315634,-0.13849591221886,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,0.0109264944916019,-0.0201074377606696,0.109587020648968,-0.141542370710863,-0.0671910848901999,-0.0357142857142857,-0.0589970501474926,null,0.235416290409969,-0.0190427180101182,0.409587020648968,-0.135449453726857,0.155031137332022,-0.0357142857142857,-0.0589970501474926,null,-0.233971464692072,-0.0200694120552927,0.276253687315634,0.00925732464328802,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.172746974896153,-0.0282069130059354,0.109587020648968,-0.0554799183117767,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.193155138161459,-0.00345217880561596,0.476253687315634,-0.130879765988852,0.00688298918387414,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,0.0103131265407982,0.0429203539823009,-0.122502005135844,0.192068174369059,-0.0357142857142857,-0.0589970501474926,null,-0.0298898320390103,-0.0236248155080268,0.0762536873156342,-0.109554556544831,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926,null,-0.131930648365541,0.00592115756977381,0.442920353982301,-0.0455789282127668,-0.0301540478531629,-0.0357142857142857,-0.0589970501474926],\"text\":[\".ID: 10<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 10<br />cluster: 6<br />variable: balance<br />value: -0.0279597459\",\".ID: 10<br />cluster: 6<br />variable: day<br />value:  0.4095870206\",\".ID: 10<br />cluster: 6<br />variable: duration<br />value: -0.1148858589\",\".ID: 10<br />cluster: 6<br />variable: campaign<br />value:  0.0809570633\",\".ID: 10<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 10<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 20<br />cluster: 6<br />variable: age<br />value: -0.0298898320\",\".ID: 20<br />cluster: 6<br />variable: balance<br />value: -0.0273133089\",\".ID: 20<br />cluster: 6<br />variable: day<br />value:  0.0095870206\",\".ID: 20<br />cluster: 6<br />variable: duration<br />value: -0.1346878391\",\".ID: 20<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 20<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 20<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 24<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 24<br />cluster: 6<br />variable: balance<br />value: -0.0155253403\",\".ID: 24<br />cluster: 6<br />variable: day<br />value:  0.1429203540\",\".ID: 24<br />cluster: 6<br />variable: duration<br />value: -0.0828980447\",\".ID: 24<br />cluster: 6<br />variable: campaign<br />value:  0.0809570633\",\".ID: 24<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 24<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 28<br />cluster: 6<br />variable: age<br />value: -0.1727469749\",\".ID: 28<br />cluster: 6<br />variable: balance<br />value: -0.0114185641\",\".ID: 28<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 28<br />cluster: 6<br />variable: duration<br />value: -0.1537282047\",\".ID: 28<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 28<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 28<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 31<br />cluster: 6<br />variable: age<br />value: -0.2951959545\",\".ID: 31<br />cluster: 6<br />variable: balance<br />value: -0.0281498744\",\".ID: 31<br />cluster: 6<br />variable: day<br />value:  0.2429203540\",\".ID: 31<br />cluster: 6<br />variable: duration<br />value: -0.1179323174\",\".ID: 31<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 31<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 31<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 33<br />cluster: 6<br />variable: age<br />value: -0.2747877912\",\".ID: 33<br />cluster: 6<br />variable: balance<br />value: -0.0305645067\",\".ID: 33<br />cluster: 6<br />variable: day<br />value:  0.0762536873\",\".ID: 33<br />cluster: 6<br />variable: duration<br />value: -0.0333930942\",\".ID: 33<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 33<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 33<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 34<br />cluster: 6<br />variable: age<br />value: -0.1115224851\",\".ID: 34<br />cluster: 6<br />variable: balance<br />value:  0.0040008594\",\".ID: 34<br />cluster: 6<br />variable: day<br />value:  0.5095870206\",\".ID: 34<br />cluster: 6<br />variable: duration<br />value: -0.1567746632\",\".ID: 34<br />cluster: 6<br />variable: campaign<br />value:  0.1179941003\",\".ID: 34<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 34<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 35<br />cluster: 6<br />variable: age<br />value: -0.2339714647\",\".ID: 35<br />cluster: 6<br />variable: balance<br />value: -0.0270851547\",\".ID: 35<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 35<br />cluster: 6<br />variable: duration<br />value:  0.3078102569\",\".ID: 35<br />cluster: 6<br />variable: campaign<br />value:  0.0439200262\",\".ID: 35<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 35<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 36<br />cluster: 6<br />variable: age<br />value: -0.2543796280\",\".ID: 36<br />cluster: 6<br />variable: balance<br />value: -0.0280548102\",\".ID: 36<br />cluster: 6<br />variable: day<br />value:  0.3762536873\",\".ID: 36<br />cluster: 6<br />variable: duration<br />value:  0.7891506986\",\".ID: 36<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 36<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 36<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 39<br />cluster: 6<br />variable: age<br />value: -0.1319306484\",\".ID: 39<br />cluster: 6<br />variable: balance<br />value:  0.0756412884\",\".ID: 39<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 39<br />cluster: 6<br />variable: duration<br />value:  0.1288308205\",\".ID: 39<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 39<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 39<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 41<br />cluster: 6<br />variable: age<br />value:  0.0517428210\",\".ID: 41<br />cluster: 6<br />variable: balance<br />value: -0.0143085177\",\".ID: 41<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 41<br />cluster: 6<br />variable: duration<br />value:  0.0755177968\",\".ID: 41<br />cluster: 6<br />variable: campaign<br />value:  0.0439200262\",\".ID: 41<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 41<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 42<br />cluster: 6<br />variable: age<br />value: -0.0502979953\",\".ID: 42<br />cluster: 6<br />variable: balance<br />value: -0.0157344816\",\".ID: 42<br />cluster: 6<br />variable: day<br />value:  0.2095870206\",\".ID: 42<br />cluster: 6<br />variable: duration<br />value:  0.1882367610\",\".ID: 42<br />cluster: 6<br />variable: campaign<br />value:  0.1550311373\",\".ID: 42<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 42<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 43<br />cluster: 6<br />variable: age<br />value: -0.1727469749\",\".ID: 43<br />cluster: 6<br />variable: balance<br />value: -0.0228072628\",\".ID: 43<br />cluster: 6<br />variable: day<br />value:  0.4095870206\",\".ID: 43<br />cluster: 6<br />variable: duration<br />value: -0.1506817462\",\".ID: 43<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 43<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 43<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 45<br />cluster: 6<br />variable: age<br />value:  0.1333754741\",\".ID: 45<br />cluster: 6<br />variable: balance<br />value: -0.0047070271\",\".ID: 45<br />cluster: 6<br />variable: day<br />value:  0.4095870206\",\".ID: 45<br />cluster: 6<br />variable: duration<br />value: -0.0745202839\",\".ID: 45<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 45<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 45<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 46<br />cluster: 6<br />variable: age<br />value: -0.1319306484\",\".ID: 46<br />cluster: 6<br />variable: balance<br />value: -0.0175977412\",\".ID: 46<br />cluster: 6<br />variable: day<br />value:  0.1762536873\",\".ID: 46<br />cluster: 6<br />variable: duration<br />value: -0.1659140386\",\".ID: 46<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 46<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 46<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 48<br />cluster: 6<br />variable: age<br />value: -0.1523388116\",\".ID: 48<br />cluster: 6<br />variable: balance<br />value: -0.0281688873\",\".ID: 48<br />cluster: 6<br />variable: day<br />value:  0.0095870206\",\".ID: 48<br />cluster: 6<br />variable: duration<br />value: -0.0981303372\",\".ID: 48<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 48<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 48<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 49<br />cluster: 6<br />variable: age<br />value: -0.0707061586\",\".ID: 49<br />cluster: 6<br />variable: balance<br />value:  0.0138305043\",\".ID: 49<br />cluster: 6<br />variable: day<br />value:  0.3095870206\",\".ID: 49<br />cluster: 6<br />variable: duration<br />value: -0.0897525763\",\".ID: 49<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 49<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 49<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 50<br />cluster: 6<br />variable: age<br />value: -0.2339714647\",\".ID: 50<br />cluster: 6<br />variable: balance<br />value: -0.0225600958\",\".ID: 50<br />cluster: 6<br />variable: day<br />value:  0.0429203540\",\".ID: 50<br />cluster: 6<br />variable: duration<br />value: -0.0158759579\",\".ID: 50<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 50<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 50<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 57<br />cluster: 6<br />variable: age<br />value:  0.1129673108\",\".ID: 57<br />cluster: 6<br />variable: balance<br />value: -0.0241761882\",\".ID: 57<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 57<br />cluster: 6<br />variable: duration<br />value:  0.1935680634\",\".ID: 57<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 57<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 57<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 61<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 61<br />cluster: 6<br />variable: balance<br />value: -0.0312299566\",\".ID: 61<br />cluster: 6<br />variable: day<br />value:  0.1762536873\",\".ID: 61<br />cluster: 6<br />variable: duration<br />value: -0.1613443509\",\".ID: 61<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 61<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 61<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 63<br />cluster: 6<br />variable: age<br />value:  0.0109264945\",\".ID: 63<br />cluster: 6<br />variable: balance<br />value: -0.0236628412\",\".ID: 63<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 63<br />cluster: 6<br />variable: duration<br />value:  0.0945581624\",\".ID: 63<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 63<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 63<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 65<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 65<br />cluster: 6<br />variable: balance<br />value: -0.0277696174\",\".ID: 65<br />cluster: 6<br />variable: day<br />value:  0.3095870206\",\".ID: 65<br />cluster: 6<br />variable: duration<br />value: -0.1065080981\",\".ID: 65<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 65<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 65<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 68<br />cluster: 6<br />variable: age<br />value: -0.3156041178\",\".ID: 68<br />cluster: 6<br />variable: balance<br />value: -0.0226551600\",\".ID: 68<br />cluster: 6<br />variable: day<br />value:  0.0762536873\",\".ID: 68<br />cluster: 6<br />variable: duration<br />value:  0.5835147504\",\".ID: 68<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 68<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 68<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 72<br />cluster: 6<br />variable: age<br />value: -0.1523388116\",\".ID: 72<br />cluster: 6<br />variable: balance<br />value: -0.0253739980\",\".ID: 72<br />cluster: 6<br />variable: day<br />value:  0.2095870206\",\".ID: 72<br />cluster: 6<br />variable: duration<br />value: -0.0821364301\",\".ID: 72<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 72<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 72<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 80<br />cluster: 6<br />variable: age<br />value:  0.0313346578\",\".ID: 80<br />cluster: 6<br />variable: balance<br />value: -0.0332072933\",\".ID: 80<br />cluster: 6<br />variable: day<br />value:  0.2095870206\",\".ID: 80<br />cluster: 6<br />variable: duration<br />value:  0.0351522218\",\".ID: 80<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 80<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 80<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 87<br />cluster: 6<br />variable: age<br />value: -0.2747877912\",\".ID: 87<br />cluster: 6<br />variable: balance<br />value: -0.0243663168\",\".ID: 87<br />cluster: 6<br />variable: day<br />value:  0.1762536873\",\".ID: 87<br />cluster: 6<br />variable: duration<br />value:  0.3177112470\",\".ID: 87<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 87<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 87<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 101<br />cluster: 6<br />variable: age<br />value: -0.0707061586\",\".ID: 101<br />cluster: 6<br />variable: balance<br />value: -0.0047070271\",\".ID: 101<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 101<br />cluster: 6<br />variable: duration<br />value:  0.0298209195\",\".ID: 101<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 101<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 101<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 103<br />cluster: 6<br />variable: age<br />value: -0.1523388116\",\".ID: 103<br />cluster: 6<br />variable: balance<br />value:  0.0028030497\",\".ID: 103<br />cluster: 6<br />variable: day<br />value:  0.1429203540\",\".ID: 103<br />cluster: 6<br />variable: duration<br />value:  0.0800874846\",\".ID: 103<br />cluster: 6<br />variable: campaign<br />value:  0.1179941003\",\".ID: 103<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 103<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 117<br />cluster: 6<br />variable: age<br />value: -0.3156041178\",\".ID: 117<br />cluster: 6<br />variable: balance<br />value: -0.0221988516\",\".ID: 117<br />cluster: 6<br />variable: day<br />value:  0.1095870206\",\".ID: 117<br />cluster: 6<br />variable: duration<br />value:  0.0755177968\",\".ID: 117<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 117<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 117<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 118<br />cluster: 6<br />variable: age<br />value: -0.0094816688\",\".ID: 118<br />cluster: 6<br />variable: balance<br />value: -0.0282069130\",\".ID: 118<br />cluster: 6<br />variable: day<br />value:  0.5095870206\",\".ID: 118<br />cluster: 6<br />variable: duration<br />value: -0.0752818985\",\".ID: 118<br />cluster: 6<br />variable: campaign<br />value:  0.0809570633\",\".ID: 118<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 118<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 119<br />cluster: 6<br />variable: age<br />value: -0.0911143218\",\".ID: 119<br />cluster: 6<br />variable: balance<br />value: -0.0243092782\",\".ID: 119<br />cluster: 6<br />variable: day<br />value:  0.0762536873\",\".ID: 119<br />cluster: 6<br />variable: duration<br />value: -0.0646192938\",\".ID: 119<br />cluster: 6<br />variable: campaign<br />value:  0.0439200262\",\".ID: 119<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 119<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 121<br />cluster: 6<br />variable: age<br />value: -0.1115224851\",\".ID: 121<br />cluster: 6<br />variable: balance<br />value: -0.0387970720\",\".ID: 121<br />cluster: 6<br />variable: day<br />value:  0.1762536873\",\".ID: 121<br />cluster: 6<br />variable: duration<br />value: -0.0090214263\",\".ID: 121<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 121<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 121<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 127<br />cluster: 6<br />variable: age<br />value: -0.0911143218\",\".ID: 127<br />cluster: 6<br />variable: balance<br />value: -0.0047260399\",\".ID: 127<br />cluster: 6<br />variable: day<br />value:  0.3095870206\",\".ID: 127<br />cluster: 6<br />variable: duration<br />value: -0.1339262245\",\".ID: 127<br />cluster: 6<br />variable: campaign<br />value:  0.1179941003\",\".ID: 127<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 127<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 133<br />cluster: 6<br />variable: age<br />value:  0.1129673108\",\".ID: 133<br />cluster: 6<br />variable: balance<br />value: -0.0220087230\",\".ID: 133<br />cluster: 6<br />variable: day<br />value:  0.3429203540\",\".ID: 133<br />cluster: 6<br />variable: duration<br />value: -0.0600496060\",\".ID: 133<br />cluster: 6<br />variable: campaign<br />value:  0.1550311373\",\".ID: 133<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 133<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 134<br />cluster: 6<br />variable: age<br />value: -0.0298898320\",\".ID: 134<br />cluster: 6<br />variable: balance<br />value: -0.0237008669\",\".ID: 134<br />cluster: 6<br />variable: day<br />value:  0.0095870206\",\".ID: 134<br />cluster: 6<br />variable: duration<br />value: -0.0349163235\",\".ID: 134<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 134<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 134<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 136<br />cluster: 6<br />variable: age<br />value: -0.2135633014\",\".ID: 136<br />cluster: 6<br />variable: balance<br />value: -0.0226171343\",\".ID: 136<br />cluster: 6<br />variable: day<br />value:  0.0095870206\",\".ID: 136<br />cluster: 6<br />variable: duration<br />value: -0.1605827363\",\".ID: 136<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 136<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 136<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 137<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 137<br />cluster: 6<br />variable: balance<br />value:  0.0051606435\",\".ID: 137<br />cluster: 6<br />variable: day<br />value:  0.1762536873\",\".ID: 137<br />cluster: 6<br />variable: duration<br />value:  0.2491659309\",\".ID: 137<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 137<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 137<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 142<br />cluster: 6<br />variable: age<br />value: -0.2747877912\",\".ID: 142<br />cluster: 6<br />variable: balance<br />value: -0.0282069130\",\".ID: 142<br />cluster: 6<br />variable: day<br />value:  0.4095870206\",\".ID: 142<br />cluster: 6<br />variable: duration<br />value: -0.0973687226\",\".ID: 142<br />cluster: 6<br />variable: campaign<br />value:  0.0439200262\",\".ID: 142<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 142<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 150<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 150<br />cluster: 6<br />variable: balance<br />value: -0.0013797779\",\".ID: 150<br />cluster: 6<br />variable: day<br />value:  0.3762536873\",\".ID: 150<br />cluster: 6<br />variable: duration<br />value: -0.1065080981\",\".ID: 150<br />cluster: 6<br />variable: campaign<br />value:  0.0439200262\",\".ID: 150<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 150<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 153<br />cluster: 6<br />variable: age<br />value:  0.0721509843\",\".ID: 153<br />cluster: 6<br />variable: balance<br />value: -0.0140993763\",\".ID: 153<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 153<br />cluster: 6<br />variable: duration<br />value: -0.1400191415\",\".ID: 153<br />cluster: 6<br />variable: campaign<br />value:  0.0809570633\",\".ID: 153<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 153<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 156<br />cluster: 6<br />variable: age<br />value: -0.0298898320\",\".ID: 156<br />cluster: 6<br />variable: balance<br />value: -0.0282069130\",\".ID: 156<br />cluster: 6<br />variable: day<br />value:  0.0095870206\",\".ID: 156<br />cluster: 6<br />variable: duration<br />value: -0.1887624773\",\".ID: 156<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 156<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 156<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 157<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 157<br />cluster: 6<br />variable: balance<br />value: -0.0252028823\",\".ID: 157<br />cluster: 6<br />variable: day<br />value:  0.4095870206\",\".ID: 157<br />cluster: 6<br />variable: duration<br />value: -0.0790899716\",\".ID: 157<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 157<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 157<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 160<br />cluster: 6<br />variable: age<br />value: -0.1319306484\",\".ID: 160<br />cluster: 6<br />variable: balance<br />value: -0.0197842193\",\".ID: 160<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 160<br />cluster: 6<br />variable: duration<br />value: -0.1910473212\",\".ID: 160<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 160<br />cluster: 6<br />variable: previous<br />value:  0.0357142857\",\".ID: 160<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0554128208\",null,\".ID: 162<br />cluster: 6<br />variable: age<br />value: -0.1727469749\",\".ID: 162<br />cluster: 6<br />variable: balance<br />value: -0.0257352422\",\".ID: 162<br />cluster: 6<br />variable: day<br />value:  0.3429203540\",\".ID: 162<br />cluster: 6<br />variable: duration<br />value:  0.1105520695\",\".ID: 162<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 162<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 162<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 163<br />cluster: 6<br />variable: age<br />value: -0.1319306484\",\".ID: 163<br />cluster: 6<br />variable: balance<br />value: -0.0317242908\",\".ID: 163<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 163<br />cluster: 6<br />variable: duration<br />value: -0.1118394004\",\".ID: 163<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 163<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 163<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 164<br />cluster: 6<br />variable: age<br />value: -0.0911143218\",\".ID: 164<br />cluster: 6<br />variable: balance<br />value: -0.0055245797\",\".ID: 164<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 164<br />cluster: 6<br />variable: duration<br />value:  0.2887698913\",\".ID: 164<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 164<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 164<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 165<br />cluster: 6<br />variable: age<br />value: -0.1727469749\",\".ID: 165<br />cluster: 6<br />variable: balance<br />value: -0.0282069130\",\".ID: 165<br />cluster: 6<br />variable: day<br />value:  0.1762536873\",\".ID: 165<br />cluster: 6<br />variable: duration<br />value:  0.1326388936\",\".ID: 165<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 165<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 165<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 167<br />cluster: 6<br />variable: age<br />value:  0.1537836373\",\".ID: 167<br />cluster: 6<br />variable: balance<br />value: -0.0282069130\",\".ID: 167<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 167<br />cluster: 6<br />variable: duration<br />value:  0.3207577055\",\".ID: 167<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 167<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 167<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 169<br />cluster: 6<br />variable: age<br />value: -0.1115224851\",\".ID: 169<br />cluster: 6<br />variable: balance<br />value: -0.0269520647\",\".ID: 169<br />cluster: 6<br />variable: day<br />value:  0.5095870206\",\".ID: 169<br />cluster: 6<br />variable: duration<br />value: -0.0364395527\",\".ID: 169<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 169<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 169<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 170<br />cluster: 6<br />variable: age<br />value: -0.0298898320\",\".ID: 170<br />cluster: 6<br />variable: balance<br />value: -0.0280548102\",\".ID: 170<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 170<br />cluster: 6<br />variable: duration<br />value:  0.5842763650\",\".ID: 170<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 170<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 170<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 173<br />cluster: 6<br />variable: age<br />value:  0.0925591476\",\".ID: 173<br />cluster: 6<br />variable: balance<br />value: -0.0273703475\",\".ID: 173<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 173<br />cluster: 6<br />variable: duration<br />value: -0.0082598117\",\".ID: 173<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 173<br />cluster: 6<br />variable: previous<br />value:  0.0357142857\",\".ID: 173<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0554128208\",null,\".ID: 182<br />cluster: 6<br />variable: age<br />value:  0.1945999639\",\".ID: 182<br />cluster: 6<br />variable: balance<br />value: -0.0218566202\",\".ID: 182<br />cluster: 6<br />variable: day<br />value:  0.3762536873\",\".ID: 182<br />cluster: 6<br />variable: duration<br />value: -0.1384959122\",\".ID: 182<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 182<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 182<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 184<br />cluster: 6<br />variable: age<br />value:  0.0109264945\",\".ID: 184<br />cluster: 6<br />variable: balance<br />value: -0.0201074378\",\".ID: 184<br />cluster: 6<br />variable: day<br />value:  0.1095870206\",\".ID: 184<br />cluster: 6<br />variable: duration<br />value: -0.1415423707\",\".ID: 184<br />cluster: 6<br />variable: campaign<br />value: -0.0671910849\",\".ID: 184<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 184<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 185<br />cluster: 6<br />variable: age<br />value:  0.2354162904\",\".ID: 185<br />cluster: 6<br />variable: balance<br />value: -0.0190427180\",\".ID: 185<br />cluster: 6<br />variable: day<br />value:  0.4095870206\",\".ID: 185<br />cluster: 6<br />variable: duration<br />value: -0.1354494537\",\".ID: 185<br />cluster: 6<br />variable: campaign<br />value:  0.1550311373\",\".ID: 185<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 185<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 189<br />cluster: 6<br />variable: age<br />value: -0.2339714647\",\".ID: 189<br />cluster: 6<br />variable: balance<br />value: -0.0200694121\",\".ID: 189<br />cluster: 6<br />variable: day<br />value:  0.2762536873\",\".ID: 189<br />cluster: 6<br />variable: duration<br />value:  0.0092573246\",\".ID: 189<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 189<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 189<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 194<br />cluster: 6<br />variable: age<br />value: -0.1727469749\",\".ID: 194<br />cluster: 6<br />variable: balance<br />value: -0.0282069130\",\".ID: 194<br />cluster: 6<br />variable: day<br />value:  0.1095870206\",\".ID: 194<br />cluster: 6<br />variable: duration<br />value: -0.0554799183\",\".ID: 194<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 194<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 194<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 196<br />cluster: 6<br />variable: age<br />value: -0.1931551382\",\".ID: 196<br />cluster: 6<br />variable: balance<br />value: -0.0034521788\",\".ID: 196<br />cluster: 6<br />variable: day<br />value:  0.4762536873\",\".ID: 196<br />cluster: 6<br />variable: duration<br />value: -0.1308797660\",\".ID: 196<br />cluster: 6<br />variable: campaign<br />value:  0.0068829892\",\".ID: 196<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 196<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 211<br />cluster: 6<br />variable: age<br />value: -0.1319306484\",\".ID: 211<br />cluster: 6<br />variable: balance<br />value:  0.0103131265\",\".ID: 211<br />cluster: 6<br />variable: day<br />value:  0.0429203540\",\".ID: 211<br />cluster: 6<br />variable: duration<br />value: -0.1225020051\",\".ID: 211<br />cluster: 6<br />variable: campaign<br />value:  0.1920681744\",\".ID: 211<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 211<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 218<br />cluster: 6<br />variable: age<br />value: -0.0298898320\",\".ID: 218<br />cluster: 6<br />variable: balance<br />value: -0.0236248155\",\".ID: 218<br />cluster: 6<br />variable: day<br />value:  0.0762536873\",\".ID: 218<br />cluster: 6<br />variable: duration<br />value: -0.1095545565\",\".ID: 218<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 218<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 218<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\",null,\".ID: 224<br />cluster: 6<br />variable: age<br />value: -0.1319306484\",\".ID: 224<br />cluster: 6<br />variable: balance<br />value:  0.0059211576\",\".ID: 224<br />cluster: 6<br />variable: day<br />value:  0.4429203540\",\".ID: 224<br />cluster: 6<br />variable: duration<br />value: -0.0455789282\",\".ID: 224<br />cluster: 6<br />variable: campaign<br />value: -0.0301540479\",\".ID: 224<br />cluster: 6<br />variable: previous<br />value: -0.0357142857\",\".ID: 224<br />cluster: 6<br />variable: pdays_recoded<br />value: -0.0589970501\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,169,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"6\",\"legendgroup\":\"6\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[-0.0911143218349287,0.000464468848198021,0.442920353982301,-0.0562415329347775,-0.0301540478531629,0.535714285714286,0.2725441684905,null,-0.0502979953043164,-0.0165520343079355,-0.323746312684366,0.0183967001192972,-0.0301540478531629,0.964285714285714,0.396200082468995,null,-0.0707061585696225,-0.0340628716339679,-0.0904129793510324,-0.0707122107717919,0.0439200262209112,0.75,0.496558505408063],\"text\":[\".ID: 152<br />cluster: 7<br />variable: age<br />value: -0.0911143218\",\".ID: 152<br />cluster: 7<br />variable: balance<br />value:  0.0004644688\",\".ID: 152<br />cluster: 7<br />variable: day<br />value:  0.4429203540\",\".ID: 152<br />cluster: 7<br />variable: duration<br />value: -0.0562415329\",\".ID: 152<br />cluster: 7<br />variable: campaign<br />value: -0.0301540479\",\".ID: 152<br />cluster: 7<br />variable: previous<br />value:  0.5357142857\",\".ID: 152<br />cluster: 7<br />variable: pdays_recoded<br />value:  0.2725441685\",null,\".ID: 192<br />cluster: 7<br />variable: age<br />value: -0.0502979953\",\".ID: 192<br />cluster: 7<br />variable: balance<br />value: -0.0165520343\",\".ID: 192<br />cluster: 7<br />variable: day<br />value: -0.3237463127\",\".ID: 192<br />cluster: 7<br />variable: duration<br />value:  0.0183967001\",\".ID: 192<br />cluster: 7<br />variable: campaign<br />value: -0.0301540479\",\".ID: 192<br />cluster: 7<br />variable: previous<br />value:  0.9642857143\",\".ID: 192<br />cluster: 7<br />variable: pdays_recoded<br />value:  0.3962000825\",null,\".ID: 222<br />cluster: 7<br />variable: age<br />value: -0.0707061586\",\".ID: 222<br />cluster: 7<br />variable: balance<br />value: -0.0340628716\",\".ID: 222<br />cluster: 7<br />variable: day<br />value: -0.0904129794\",\".ID: 222<br />cluster: 7<br />variable: duration<br />value: -0.0707122108\",\".ID: 222<br />cluster: 7<br />variable: campaign<br />value:  0.0439200262\",\".ID: 222<br />cluster: 7<br />variable: previous<br />value:  0.7500000000\",\".ID: 222<br />cluster: 7<br />variable: pdays_recoded<br />value:  0.4965585054\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(199,124,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"7\",\"legendgroup\":\"7\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7,null,1,2,3,4,5,6,7],\"y\":[-0.0707061585696225,-0.0145176590702749,-0.0570796460176991,0.0793258699593581,0.117994100294985,0.178571428571429,0.00551907888476544,null,0.133375474083439,-0.00738783931211835,0.0762536873156342,-0.0684273669027897,-0.0671910848901999,0.107142857142857,0.138135566339963,null,0.0517428210222142,-0.0329030876199745,-0.457079646017699,0.373309114437652,-0.0301540478531629,0.25,0.0772036666983855,null,0.0721509842875203,0.0912888661407678,0.142920353982301,-0.0950838787078163,0.00688298918387414,0.107142857142857,0.16501728677007,null,-0.0707061585696225,-0.037390120854441,-0.323746312684366,-0.0295850211297508,-0.0301540478531629,0.178571428571429,0.242078218669712,null,0.194599963879357,0.0167774964548601,0.409587020648968,0.422814064932702,-0.0301540478531629,0.0357142857142857,0.208028039458242,null,0.215008127144663,-0.0238339568875994,0.0429203539823009,-0.0661425230337874,-0.0671910848901999,0.0357142857142857,0.290465315443905,null,-0.233971464692072,0.0191160773355355,-0.423746312684366,0.0717097237293505,-0.0671910848901999,0.321428571428571,0.0951248136517905,null,0.0517428210222142,0.0386803027519169,0.409587020648968,-0.041770855097763,-0.0301540478531629,0.107142857142857,0.292257430139246,null,-0.0502979953043164,0.0750709027975478,0.109587020648968,0.112075298748391,-0.0671910848901999,0.0357142857142857,0.268959939099819,null,-0.152338811630847,-0.0248036123747087,0.0762536873156342,0.130354049700409,-0.0301540478531629,0.25,0.265375709709138,null,0.255824453675275,-0.0215524145649893,0.0429203539823009,-0.112601015036834,0.00688298918387414,0.107142857142857,0.177562089637454,null,-0.0298898320390103,-0.0031669860152897,0.142920353982301,0.214893272853494,-0.0671910848901999,0.25,0.245662448060393,null,0.235416290409969,-0.00174102206365839,0.409587020648968,0.655106524947934,-0.0301540478531629,0.178571428571429,0.30838646239731,null,-0.0502979953043164,-0.028187900153247,0.176253687315634,-0.155251433924876,0.0809570632579482,0.0357142857142857,0.297633774225267,null,-0.172746974896153,0.00825973845044915,-0.257079646017699,0.0435299826783223,-0.0671910848901999,0.0357142857142857,0.249246677451074,null,0.276232616940581,0.00114893154498105,-0.357079646017699,-0.133926224480855,0.155031137332022,0.178571428571429,0.292257430139246],\"text\":[\".ID: 2<br />cluster: 8<br />variable: age<br />value: -0.0707061586\",\".ID: 2<br />cluster: 8<br />variable: balance<br />value: -0.0145176591\",\".ID: 2<br />cluster: 8<br />variable: day<br />value: -0.0570796460\",\".ID: 2<br />cluster: 8<br />variable: duration<br />value:  0.0793258700\",\".ID: 2<br />cluster: 8<br />variable: campaign<br />value:  0.1179941003\",\".ID: 2<br />cluster: 8<br />variable: previous<br />value:  0.1785714286\",\".ID: 2<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.0055190789\",null,\".ID: 26<br />cluster: 8<br />variable: age<br />value:  0.1333754741\",\".ID: 26<br />cluster: 8<br />variable: balance<br />value: -0.0073878393\",\".ID: 26<br />cluster: 8<br />variable: day<br />value:  0.0762536873\",\".ID: 26<br />cluster: 8<br />variable: duration<br />value: -0.0684273669\",\".ID: 26<br />cluster: 8<br />variable: campaign<br />value: -0.0671910849\",\".ID: 26<br />cluster: 8<br />variable: previous<br />value:  0.1071428571\",\".ID: 26<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.1381355663\",null,\".ID: 38<br />cluster: 8<br />variable: age<br />value:  0.0517428210\",\".ID: 38<br />cluster: 8<br />variable: balance<br />value: -0.0329030876\",\".ID: 38<br />cluster: 8<br />variable: day<br />value: -0.4570796460\",\".ID: 38<br />cluster: 8<br />variable: duration<br />value:  0.3733091144\",\".ID: 38<br />cluster: 8<br />variable: campaign<br />value: -0.0301540479\",\".ID: 38<br />cluster: 8<br />variable: previous<br />value:  0.2500000000\",\".ID: 38<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.0772036667\",null,\".ID: 60<br />cluster: 8<br />variable: age<br />value:  0.0721509843\",\".ID: 60<br />cluster: 8<br />variable: balance<br />value:  0.0912888661\",\".ID: 60<br />cluster: 8<br />variable: day<br />value:  0.1429203540\",\".ID: 60<br />cluster: 8<br />variable: duration<br />value: -0.0950838787\",\".ID: 60<br />cluster: 8<br />variable: campaign<br />value:  0.0068829892\",\".ID: 60<br />cluster: 8<br />variable: previous<br />value:  0.1071428571\",\".ID: 60<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.1650172868\",null,\".ID: 76<br />cluster: 8<br />variable: age<br />value: -0.0707061586\",\".ID: 76<br />cluster: 8<br />variable: balance<br />value: -0.0373901209\",\".ID: 76<br />cluster: 8<br />variable: day<br />value: -0.3237463127\",\".ID: 76<br />cluster: 8<br />variable: duration<br />value: -0.0295850211\",\".ID: 76<br />cluster: 8<br />variable: campaign<br />value: -0.0301540479\",\".ID: 76<br />cluster: 8<br />variable: previous<br />value:  0.1785714286\",\".ID: 76<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2420782187\",null,\".ID: 91<br />cluster: 8<br />variable: age<br />value:  0.1945999639\",\".ID: 91<br />cluster: 8<br />variable: balance<br />value:  0.0167774965\",\".ID: 91<br />cluster: 8<br />variable: day<br />value:  0.4095870206\",\".ID: 91<br />cluster: 8<br />variable: duration<br />value:  0.4228140649\",\".ID: 91<br />cluster: 8<br />variable: campaign<br />value: -0.0301540479\",\".ID: 91<br />cluster: 8<br />variable: previous<br />value:  0.0357142857\",\".ID: 91<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2080280395\",null,\".ID: 100<br />cluster: 8<br />variable: age<br />value:  0.2150081271\",\".ID: 100<br />cluster: 8<br />variable: balance<br />value: -0.0238339569\",\".ID: 100<br />cluster: 8<br />variable: day<br />value:  0.0429203540\",\".ID: 100<br />cluster: 8<br />variable: duration<br />value: -0.0661425230\",\".ID: 100<br />cluster: 8<br />variable: campaign<br />value: -0.0671910849\",\".ID: 100<br />cluster: 8<br />variable: previous<br />value:  0.0357142857\",\".ID: 100<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2904653154\",null,\".ID: 107<br />cluster: 8<br />variable: age<br />value: -0.2339714647\",\".ID: 107<br />cluster: 8<br />variable: balance<br />value:  0.0191160773\",\".ID: 107<br />cluster: 8<br />variable: day<br />value: -0.4237463127\",\".ID: 107<br />cluster: 8<br />variable: duration<br />value:  0.0717097237\",\".ID: 107<br />cluster: 8<br />variable: campaign<br />value: -0.0671910849\",\".ID: 107<br />cluster: 8<br />variable: previous<br />value:  0.3214285714\",\".ID: 107<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.0951248137\",null,\".ID: 131<br />cluster: 8<br />variable: age<br />value:  0.0517428210\",\".ID: 131<br />cluster: 8<br />variable: balance<br />value:  0.0386803028\",\".ID: 131<br />cluster: 8<br />variable: day<br />value:  0.4095870206\",\".ID: 131<br />cluster: 8<br />variable: duration<br />value: -0.0417708551\",\".ID: 131<br />cluster: 8<br />variable: campaign<br />value: -0.0301540479\",\".ID: 131<br />cluster: 8<br />variable: previous<br />value:  0.1071428571\",\".ID: 131<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2922574301\",null,\".ID: 168<br />cluster: 8<br />variable: age<br />value: -0.0502979953\",\".ID: 168<br />cluster: 8<br />variable: balance<br />value:  0.0750709028\",\".ID: 168<br />cluster: 8<br />variable: day<br />value:  0.1095870206\",\".ID: 168<br />cluster: 8<br />variable: duration<br />value:  0.1120752987\",\".ID: 168<br />cluster: 8<br />variable: campaign<br />value: -0.0671910849\",\".ID: 168<br />cluster: 8<br />variable: previous<br />value:  0.0357142857\",\".ID: 168<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2689599391\",null,\".ID: 177<br />cluster: 8<br />variable: age<br />value: -0.1523388116\",\".ID: 177<br />cluster: 8<br />variable: balance<br />value: -0.0248036124\",\".ID: 177<br />cluster: 8<br />variable: day<br />value:  0.0762536873\",\".ID: 177<br />cluster: 8<br />variable: duration<br />value:  0.1303540497\",\".ID: 177<br />cluster: 8<br />variable: campaign<br />value: -0.0301540479\",\".ID: 177<br />cluster: 8<br />variable: previous<br />value:  0.2500000000\",\".ID: 177<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2653757097\",null,\".ID: 188<br />cluster: 8<br />variable: age<br />value:  0.2558244537\",\".ID: 188<br />cluster: 8<br />variable: balance<br />value: -0.0215524146\",\".ID: 188<br />cluster: 8<br />variable: day<br />value:  0.0429203540\",\".ID: 188<br />cluster: 8<br />variable: duration<br />value: -0.1126010150\",\".ID: 188<br />cluster: 8<br />variable: campaign<br />value:  0.0068829892\",\".ID: 188<br />cluster: 8<br />variable: previous<br />value:  0.1071428571\",\".ID: 188<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.1775620896\",null,\".ID: 193<br />cluster: 8<br />variable: age<br />value: -0.0298898320\",\".ID: 193<br />cluster: 8<br />variable: balance<br />value: -0.0031669860\",\".ID: 193<br />cluster: 8<br />variable: day<br />value:  0.1429203540\",\".ID: 193<br />cluster: 8<br />variable: duration<br />value:  0.2148932729\",\".ID: 193<br />cluster: 8<br />variable: campaign<br />value: -0.0671910849\",\".ID: 193<br />cluster: 8<br />variable: previous<br />value:  0.2500000000\",\".ID: 193<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2456624481\",null,\".ID: 208<br />cluster: 8<br />variable: age<br />value:  0.2354162904\",\".ID: 208<br />cluster: 8<br />variable: balance<br />value: -0.0017410221\",\".ID: 208<br />cluster: 8<br />variable: day<br />value:  0.4095870206\",\".ID: 208<br />cluster: 8<br />variable: duration<br />value:  0.6551065249\",\".ID: 208<br />cluster: 8<br />variable: campaign<br />value: -0.0301540479\",\".ID: 208<br />cluster: 8<br />variable: previous<br />value:  0.1785714286\",\".ID: 208<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.3083864624\",null,\".ID: 214<br />cluster: 8<br />variable: age<br />value: -0.0502979953\",\".ID: 214<br />cluster: 8<br />variable: balance<br />value: -0.0281879002\",\".ID: 214<br />cluster: 8<br />variable: day<br />value:  0.1762536873\",\".ID: 214<br />cluster: 8<br />variable: duration<br />value: -0.1552514339\",\".ID: 214<br />cluster: 8<br />variable: campaign<br />value:  0.0809570633\",\".ID: 214<br />cluster: 8<br />variable: previous<br />value:  0.0357142857\",\".ID: 214<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2976337742\",null,\".ID: 215<br />cluster: 8<br />variable: age<br />value: -0.1727469749\",\".ID: 215<br />cluster: 8<br />variable: balance<br />value:  0.0082597385\",\".ID: 215<br />cluster: 8<br />variable: day<br />value: -0.2570796460\",\".ID: 215<br />cluster: 8<br />variable: duration<br />value:  0.0435299827\",\".ID: 215<br />cluster: 8<br />variable: campaign<br />value: -0.0671910849\",\".ID: 215<br />cluster: 8<br />variable: previous<br />value:  0.0357142857\",\".ID: 215<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2492466775\",null,\".ID: 221<br />cluster: 8<br />variable: age<br />value:  0.2762326169\",\".ID: 221<br />cluster: 8<br />variable: balance<br />value:  0.0011489315\",\".ID: 221<br />cluster: 8<br />variable: day<br />value: -0.3570796460\",\".ID: 221<br />cluster: 8<br />variable: duration<br />value: -0.1339262245\",\".ID: 221<br />cluster: 8<br />variable: campaign<br />value:  0.1550311373\",\".ID: 221<br />cluster: 8<br />variable: previous<br />value:  0.1785714286\",\".ID: 221<br />cluster: 8<br />variable: pdays_recoded<br />value:  0.2922574301\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(255,97,204,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"8\",\"legendgroup\":\"8\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":27.8721461187215,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":48.9497716894977},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,7.6],\"tickmode\":\"array\",\"ticktext\":[\"age\",\"balance\",\"day\",\"duration\",\"campaign\",\"previous\",\"pdays_recoded\"],\"tickvals\":[1,2,3,4,5,6,7],\"categoryorder\":\"array\",\"categoryarray\":[\"age\",\"balance\",\"day\",\"duration\",\"campaign\",\"previous\",\"pdays_recoded\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"variable\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.56314791403287,1.03702064896755],\"tickmode\":\"array\",\"ticktext\":[\"-0.5\",\"0.0\",\"0.5\",\"1.0\"],\"tickvals\":[-0.5,0,0.5,1],\"categoryorder\":\"array\",\"categoryarray\":[\"-0.5\",\"0.0\",\"0.5\",\"1.0\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"value\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.972933070866142},\"annotations\":[{\"text\":\"cluster\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"81040f1add\":{\"colour\":{},\"x\":{},\"y\":{},\"type\":\"scatter\"}},\"cur_data\":\"81040f1add\",\"visdat\":{\"81040f1add\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\nFor example, if we filter the interactive graph by only focusing on Cluster 5 & 8, we can see the average age under Cluster 8 is younger than Cluster 5.\r\nNote that we can select and un-select the cluster in the graph by clicking the cluster number in the legend.\r\n\r\n\r\n\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\nTill next time, happy learning!\r\n\r\n\r\n\r\nPhoto by Nareeta Martin on Unsplash\r\n\r\n\r\n\r\nGoogle Developers. 2020. “Clustering in Machine Learning.” https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages.\r\n\r\n\r\nIBM Support. 2020. “Clustering Binary Data with k-Means (Should Be Avoided).” https://www.ibm.com/support/pages/clustering-binary-data-k-means-should-be-avoided.\r\n\r\n\r\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in r. Springer.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-30-k-means/image/kmeans_1.jpg",
    "last_modified": "2021-10-30T15:40:54+08:00",
    "input_file": "kmeans.knit.md"
  },
  {
    "path": "posts/2021-10-17-clustering/",
    "title": "Clustering",
    "description": "Mirror, mirror on the wall, which data are similar to one another?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-10-17",
    "categories": [
      "Machine Learning",
      "Unsupervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nUnsupervised Learning\r\nApplication of Clustering\r\nConsideration when Performing Clustering\r\nPurpose of Clustering Analysis\r\nAppropriateness of Features in Separating Policies into Clusters\r\nNumber of Data Points within Each Cluster\r\nCluster Selection\r\nLimitation of Selected Clustering Algorithm\r\n\r\nConclusion\r\n\r\nDuring the Q&A session in my last August sharing, there were some questions on different unsupervised learning methods. It has triggered my thought to do a sharing on unsupervised learning methods, in particular clustering.\r\nIt is one of the basic topics one would learn when he/she learns data science.\r\nInstead of having the explanations and demonstration in the same post, I have decided to split up the post into two so that each post is not too long to read.\r\nIn this post, I will focus on what one should know before perform clustering analysis.\r\nThe demonstration on clustering will be included in the next post.\r\nNevertheless, let’s understand a bit more about clustering.\r\n\r\n\r\n\r\nPhoto by v2osk on Unsplash\r\nUnsupervised Learning\r\nIn general, the classical machine learning models can be further split into two broad categories, which are supervised and unsupervised learning methods.\r\nUnsupervised learning is a machine learning method to discover hidden patterns or data groupings without human intervention (IBM Cloud Education 2020).\r\nIn other words, this method aims to model the underlying structure, affinities, or distribution in the data in order to learn more about its intrinsic characteristics (Ivo 2018).\r\nThe unsupervised learning method comprises clustering, association, and dimension reduction as shown in the graph below.\r\n\r\n\r\n\r\nExtracted from vas3k blog\r\nBelow are the descriptions of each unsupervised learning methods:\r\n\r\n\r\nTechniques\r\n\r\n\r\nDescriptions\r\n\r\n\r\nClustering\r\n\r\n\r\nClustering is one of the basic algorithms any beginners would learn when they started their data science journey.\r\nThe clustering algorithms include k-means, hierarchical clustering, latent class analysis, and so on. It is commonly used in many applications as shown following:\r\nFind the different customer segments so that one could derive a more targeted strategy to boost the sales\r\nGroup the customers into different groups based on their behaviors\r\n\r\n\r\nAssociation\r\n\r\n\r\nApriori is one of the common algorithms used to mine the association rules between the different items. In other words, this method allows one to discover how the items are “associated” with one another.\r\n\r\nFollowing are some of the common examples of how the association rules are used:\r\nProduct bundling by bundling products with higher profit margin and closely associated to boost the sales\r\nPerform cross-selling and upselling products\r\nArranging the closely associated products together to increase the sales\r\n\r\nThe more advanced technique includes taking time into considerations while performing association algorithms. This would allow one to understand whether there is a change in the customer purchasing behaviors.\r\n\r\n\r\nDimension Reduction\r\n\r\n\r\nAs the data increases, often one would face the “curve of dimensionality.” This is an issue while building machine learning models as the data points in high-dimensional space are sparse, resulting in a less desirable machine learning model.\r\nTherefore, the common way to resolve this is to perform principal component analysis (PCA). The key idea of PCA is the algorithm attempts to find a low-dimensional representation of a dataset that contains as much as possible of the variation (James et al. 2021).\r\nHowever, as the principal components derived from the algorithm are not directly observable from the dataset, hence it is often more challenging to explain or understand the results.\r\n\r\n\r\nIn this series, I will focus on clustering and leave the other two unsupervised learning methods for future posts.\r\nApplication of Clustering\r\n(Google Developers 2020) has listed a list of common applications for clustering, which includes:\r\nmarket segmentation\r\nsocial network analysis\r\nsearch result grouping\r\nmedical imaging\r\nimage segmentation\r\nanomaly detection\r\nConsideration when Performing Clustering\r\nWhile clustering could be a useful tool to discover insights within the dataset, below are some of the important considerations when performing clustering:\r\nPurpose of Clustering Analysis\r\n\r\n\r\n\r\nPhoto by Mark Fletcher-Brown on Unsplash\r\n(Logan and Pentimonti 2016) recommended the users to think about how the clustering results will be used. For example, what is the business problem we be solving through the clustering results? The required dataset could defer depending on the business problem we are solving.\r\nNevertheless, the answer to this question will guide users in selecting the appropriate variables for the clustering analysis, deciding the appropriate clustering algorithms, and so on.\r\nAppropriateness of Features in Separating Policies into Clusters\r\nAs the name unsupervised machine learning suggested, the clustering algorithm has no mechanism for differentiating relevant or irrelevant variables. Also, the clustering results can be very dependent on the variables included in the analysis (Kam 2019). Therefore, to obtain meaningful clustering results, the selected variables should be able to reflect the inherent differences between the different clusters.\r\nFor example, if we know there are the fundamental structural difference between the different types of customers/businesses, the relevant parameters should be used in clustering so that the clusters are less likely to overlap with one another.\r\n\r\n\r\n\r\nPhoto by David Rotimi on Unsplash\r\nOf course, the variables that could effectively separate the dataset may not be very apparent sometimes. Often, it requires some level of understanding of the underlying data or even the context of the data.\r\nAs the old saying goes, garbage in garbage out. Without thinking through and selecting the appropriate variables, the clustering results may not be meaningful.\r\nBesides, the number of variables used in clustering is also quite important. The rule of thumb recommended by Goodman is to have at least three variables in the clustering analysis (Goodman 1974). The clustering results may not be meaningful if insufficient variables are used in clustering.\r\nMeanwhile, having too many variables may introduce too much noise, resulting in less meaningful clustering results. Therefore, only the variables that could help us in separating the different characteristics of the model points should be included in the analysis.\r\nNumber of Data Points within Each Cluster\r\n\r\n\r\n\r\nPhoto by Monstera from Pexels\r\nThe general rule of thumb is the number of data points within each cluster should not be less than 5. The idea is that the cluster is unlikely to be meaningful when the number of data points contained in the cluster is relatively low.\r\nFor example, imagine the company is leveraging the clustering results to set up the different operations to serve different groups based on the values customers bring in. The company would be better off choosing a lower number of clusters that contain a decent number of data points within the clusters even though the higher number of clusters might be producing a better clustering result. The cost of having an additional headcount or separate process for a cluster without a decent size is likely to be hard to justify.\r\nCluster Selection\r\n\r\n\r\n\r\nPhoto by Edu Grande on Unsplash\r\nFor clustering, the number of clusters is a parameter that needs to be defined upfront before clustering analysis. However, this poses a challenge - how do we know what is the optimal number of clusters for the given dataset?\r\nThe common approach is to run the algorithm under a different number of clusters and use some measurements to compare the clustering results, providing us with a more objective method to determine the optimal number of clusters.\r\nFor example, the elbow curve is one of the common approaches to find the optimal number of clusters. This approach plots the within-cluster sum of the square against the number of clusters. Within-cluster sum of square measures the variability within the cluster. With the same dataset, as the number of clusters increases, the within-cluster sum of squares tends to decrease.\r\nFollowing is an example of how the elbow curve graph looks like, where the x-axis represents the number of clusters and the y-axis represents the total withinness between clusters.\r\n\r\n\r\n\r\nExample of elbow curve graph\r\nHowever, having too many clusters could introduce noises into the clustering results, increasing the within-cluster sum of squares. The rule of thumb is to choose the point that the decrease in the within-cluster sum of squares would be insignificant when the number of clusters increases. This is because a simpler model is preferred if the simpler model can differentiate the segments well.\r\nAlso, note that one could compare the clustering results only if the clustering results are run based on the same set of data. This is because supervised learning methods have an explicit target, so different accuracy measurements can be used to measure the performance of the different methods.\r\nOn the other hand, unlike the supervised learning method, the fundamental idea of the unsupervised learning method is to use the algorithms to find the various underlying patterns within the dataset. Hence, the unsupervised learning method does not have an explicit target for the algorithm to learn and measure on.\r\nHence, it is not meaningful to compare clustering results if the underlying dataset used in the clustering is different. However, we could compare the cluster results qualitatively to see which cluster models provide a more meaningful result. Following are some considerations in comparing the cluster results qualitatively:\r\nDoes each cluster have a reasonable amount of data points within each cluster (eg. not less than 5 data points within each cluster)?\r\nIs there any overlapping between the clusters? A good cluster should be able to show the differences between the data points\r\nLimitation of Selected Clustering Algorithm\r\n\r\n\r\n\r\nPhoto by lucas souza from Pexels\r\nEach clustering algorithm has its limitations, hence appropriate data transformation might require before the analysis.\r\nFor example, the K-means clustering algorithm can only accept numeric variables. Hence the non-numeric variables will need to be transformed before they can be used for clustering purposes.\r\nAlso, the common clustering algorithm assumes the data is time-independent. This assumption is not appropriate when we are dealing with sequential data, such as time-series data, speech data, and so on. Hence, other clustering methods (eg. dynamic time warping) that take time into considerations when performing clustering would be more appropriate. Do check out my previous poster on how we could use dynamic time warping to analyze time-series data in this post.\r\n\r\n\r\n\r\nPoster of my project on Dynamic Time Warping\r\nHence, it is important to understand the limitations of the selected clustering algorithm before the analysis to ensure meaningful results.\r\nConclusion\r\nThat’s all for the day!\r\nAbove are some of the important considerations while performing clustering analysis.\r\nThanks for reading the post until the end.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nGoodman, L. A. 1974. “Exploratory Latent Structure Analysis Using Both Identifiable and Unidentifiable Models.” Biometrika.\r\n\r\n\r\nGoogle Developers. 2020. “Clustering in Machine Learning.” https://developers.google.com/machine-learning/clustering/overview.\r\n\r\n\r\nIBM Cloud Education. 2020. “Unsupervised Learning.” https://www.ibm.com/cloud/learn/unsupervised-learning.\r\n\r\n\r\nIvo, Dinov D. 2018. Data Science and Predictive Analytics: Biomedical and Health Applications Using r. Springer.\r\n\r\n\r\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning with Applications in r. Springer.\r\n\r\n\r\nKam, T. Seong, ed. 2019. “Isss602 Data Analytics Lab Week 4 Lesson Slides - Cluster Analysis: Concepts, Algorithms and Methods.”\r\n\r\n\r\nLogan, Jessica AR., and Jill M. Pentimonti. 2016. Introduction to Latent Class Analysis for Reading Fluency Research. Edited by Kelli D. Cummings and Yaacov Petscher. Springer.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-17-clustering/image/categorize.jpg",
    "last_modified": "2021-10-17T09:29:08+08:00",
    "input_file": "clustering.knit.md"
  },
  {
    "path": "posts/2021-09-17-transfer-learning-with-cnn/",
    "title": "Transfer Learning with Convolution Neural Network",
    "description": "Why learn from scratch when you can leverage the existing work?",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-09-17",
    "categories": [
      "Tensorflow/Keras",
      "Deep Learning",
      "Image Recognition",
      "Transfer Learning"
    ],
    "contents": "\r\n\r\nContents\r\nHow is image recognition relevant for insurance?\r\nShorten operation process\r\nAdditional Pricing Parameters\r\nAdditional Parameters in Claim Estimation\r\n\r\nWhat is ‘Transfer Learning?’\r\nPre-trained Models\r\nConsiderations when using Pre-trained Models\r\nData similarity: Is the current dataset similar to the dataset used to train pre-trained model?\r\nSize of the data set: How big is our dataset?\r\nHow are we training the model?\r\n\r\nModel Building Steps\r\nDemonstration\r\nSetup the environment\r\nImport data\r\nImport the Pre-trained model\r\nAdd on Classifier Layer\r\nImage Augmentation\r\nModel Preparation\r\nModel Fitting\r\nVisualizing the Model Results\r\n\r\n\r\nConclusion\r\n\r\nRecently I happened to come across this post by (Chollet and Allaire 2017) on RStudio AI Blog and inspired me to give it a try to build a convolution neural network (CNN) model to solve an image classification problem.\r\nBefore jumping into the discussion, let’s take a look at how image recognition can be used in the insurance context.\r\nHow is image recognition relevant for insurance?\r\nPapers are suggesting how the image recognition technology can be used in the insurance context.\r\nIn the ‘Applying Image Recognition to Insurance’ report, (Shang 2018) explored a few examples of how insurers could leverage image recognition.\r\nShorten operation process\r\nImage recognition can be utilized at different stages of the insurance stage (eg. policy inception, update policy information, policy claim), where such use cases in this area can be observed in some countries.\r\nFor example, instead of manually inputting necessary info into the different documents at the point of policy inception, insurers could build an algorithm that would capture the details in the images uploaded. This could potentially shorten the turnaround time, resulting in an improvement in customer satisfaction.\r\nAdditional Pricing Parameters\r\n(Shang 2018) also discussed how these techniques can be used for insurance pricing. For example, in property insurance, pictures can be used to understand the riskiness of the property, providing additional parameters for pricing and underwriting purpose.\r\nNevertheless, this could allow the insurers in implementing dynamic pricing on the relevant business lines, improving the profitability of the business.\r\nAdditional Parameters in Claim Estimation\r\n(Shang 2018) also provided an example of how image recognition could be applied in real-time risk monitoring and risk management. Insurers can use the information extracted from the image to predict claim count and claim amount.\r\nNevertheless, in this exercise, instead of building the model from scratch, I will use what is known as “transfer learning” to speed up the model-building problem.\r\n\r\n\r\n\r\nPhoto by cottonbro from Pexels\r\nWhat is ‘Transfer Learning?’\r\n(Brownlee 2019a) explained that transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\r\n(Google 2021) also explained that the intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\r\nPre-trained Models\r\nThere are different types of pre-trained models that are trained to solve different types of problems. Some are trained to classify images, some are trained to handle text-related problems, and so on.\r\n\r\n\r\n\r\nDo check out this link to know about the different pre-trained models in TensorFlow.\r\nFor this analysis, I will focus the pre-trained models on the image classification problem. I will use VGG16 model to build a multi-class classification for the dataset. The relevant paper of this paper can be found in this link.\r\nConsiderations when using Pre-trained Models\r\nThe considerations of using pre-trained models can be summarized as the following diagram:\r\n\r\n\r\n\r\nDiagram on Model Tuning Considerations (Gupta 2017)\r\nData similarity: Is the current dataset similar to the dataset used to train pre-trained model?\r\nThese pre-trained models are usually commonly trained by using image data from ImageNet, where ImageNet is an image database with more than 14 million images. As such, this makes the pre-trained models generalized models.\r\nSo, before using the pre-trained models, we should ask ourselves how similar is between our dataset and the dataset used in pre-trained model. If our images are very different from the image data used in training the pre-trained models, the accuracy of our model is likely to be low.\r\nSize of the data set: How big is our dataset?\r\nAnother important consideration is how big is our dataset. The model is unlikely to perform well if the dataset is not large enough to train the models. Therefore, pre-trained models can be used as an alternative if our dataset is not large enough.\r\nHow are we training the model?\r\nThese pre-trained models typically come with the model weights that are derived by fitting the models with images from ImageNet. Hence, when we are using pre-trained models, one of the key questions we should answer is whether we should be using the model weights in the pre-trained models.\r\nThe usual approach is to freeze all the layers in the pre-trained model, except the few top layers. The few top unfreeze layers will be fine-tuned together with the classifier layer.\r\n\r\n\r\n\r\nCNN structure (Saha 2018)\r\nDo note that the more layers we unfreeze, the longer it would take to fit the model.\r\nModel Building Steps\r\nIn general, the model building steps can be summarized as follows:\r\nSplit the dataset into train and validation dataset\r\nImport pre-trained model into the environment and exclude the classifier layer from the pre-trained model\r\nAdd on classifier layer on top of the pre-trained model\r\nFreeze the weights of the pre-trained model, except for a few top layers to be tuned together with the classifier layer\r\nTrain the unfreeze top layers and classifier layer to make the model more relevant for the specific task\r\nDemonstration\r\nI will be using this image dataset from Kaggle. There are about 28k medium-quality images in this data. These photos are from 10 different categories of animals, which consist of dog, cat, horse, spider, butterfly, chicken, sheep, cow, squirrel, and elephant.\r\nBelow are some of the extracted images from the dataset:\r\n\r\n\r\n\r\nI have chosen these as the examples as I thought they looks funny.\r\nSetup the environment\r\nAs usual, I will start by calling all the relevant packages I would need in the analysis later on.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'keras')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nImport data\r\nNext, I will import the dataset into the environment.\r\n\r\n\r\n\r\nAs mentioned in the post earlier, there are 10 different categories in the dataset. Hence this is a multi-class classification problem. Since the photos are saved under the respective categories, I will use dir function to extract out the list of categories, where train_dir is the link to the folder I stored the image dataset.\r\n\r\n\r\nlabel_list <- dir(train_dir)\r\n\r\n\r\n\r\nNext, length function is used to perform a count on the number of categories.\r\n\r\n\r\noutput_n <- length(label_list)\r\n\r\n\r\n\r\nNext, I will define the input size to be passed into the CNN model later. I have also defined the channel to be 3 as the photos are colored photos.\r\n\r\n\r\nwidth <- 224\r\nheight<- 224\r\nrgb <- 3 \r\n\r\ntarget_size <- c(width, height)\r\n\r\n\r\n\r\nImport the Pre-trained model\r\nAs discussed earlier, I would use the pre-trained model to classify the image. Over here, I will be using VGG16 as shown in the code chunk below. As I won’t be re-trained the entire network, I will indicate the model weights should follow the derived weights when the model is trained by using ImageNet dataset.\r\nAlso, to include our classifier layer, I will indicate the include_top argument to be false so that the pre-trained model is imported without the classifier layer.\r\n\r\n\r\nconv_base <- application_vgg16(\r\n  weights = \"imagenet\",\r\n  include_top = FALSE,\r\n  input_shape = c(width, height, rgb)\r\n)\r\n\r\n\r\n\r\nFor other pre-trained models, do refer to the ‘Applications’ section under either Keras documentation page or TensorFlow documentation page for more information.\r\nNext, summary function can be used to visualize how the pre-trained model looks like.\r\n\r\n\r\nsummary(conv_base)\r\n\r\n\r\nModel: \"vgg16\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\ninput_1 (InputLayer)           [(None, 224, 224, 3)]       0          \r\n______________________________________________________________________\r\nblock1_conv1 (Conv2D)          (None, 224, 224, 64)        1792       \r\n______________________________________________________________________\r\nblock1_conv2 (Conv2D)          (None, 224, 224, 64)        36928      \r\n______________________________________________________________________\r\nblock1_pool (MaxPooling2D)     (None, 112, 112, 64)        0          \r\n______________________________________________________________________\r\nblock2_conv1 (Conv2D)          (None, 112, 112, 128)       73856      \r\n______________________________________________________________________\r\nblock2_conv2 (Conv2D)          (None, 112, 112, 128)       147584     \r\n______________________________________________________________________\r\nblock2_pool (MaxPooling2D)     (None, 56, 56, 128)         0          \r\n______________________________________________________________________\r\nblock3_conv1 (Conv2D)          (None, 56, 56, 256)         295168     \r\n______________________________________________________________________\r\nblock3_conv2 (Conv2D)          (None, 56, 56, 256)         590080     \r\n______________________________________________________________________\r\nblock3_conv3 (Conv2D)          (None, 56, 56, 256)         590080     \r\n______________________________________________________________________\r\nblock3_pool (MaxPooling2D)     (None, 28, 28, 256)         0          \r\n______________________________________________________________________\r\nblock4_conv1 (Conv2D)          (None, 28, 28, 512)         1180160    \r\n______________________________________________________________________\r\nblock4_conv2 (Conv2D)          (None, 28, 28, 512)         2359808    \r\n______________________________________________________________________\r\nblock4_conv3 (Conv2D)          (None, 28, 28, 512)         2359808    \r\n______________________________________________________________________\r\nblock4_pool (MaxPooling2D)     (None, 14, 14, 512)         0          \r\n______________________________________________________________________\r\nblock5_conv1 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_conv2 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_conv3 (Conv2D)          (None, 14, 14, 512)         2359808    \r\n______________________________________________________________________\r\nblock5_pool (MaxPooling2D)     (None, 7, 7, 512)           0          \r\n======================================================================\r\nTotal params: 14,714,688\r\nTrainable params: 14,714,688\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nAdd on Classifier Layer\r\nNext, I will add on the classifier layer as discussed in the earlier post. As this is a multi-class classification problem, hence softmax is selected to be the last activation function.\r\n\r\n\r\nmodel <- keras_model_sequential() %>% \r\n  conv_base %>% \r\n  layer_flatten() %>% \r\n  layer_dense(units = 256, activation = \"relu\") %>% \r\n  layer_dense(units = output_n, activation = \"softmax\")\r\n\r\n\r\n\r\nI will call the summary function to check on the model before fitting the model.\r\n\r\n\r\nsummary(model)\r\n\r\n\r\nModel: \"sequential\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\nvgg16 (Functional)             (None, 7, 7, 512)           14714688   \r\n______________________________________________________________________\r\nflatten (Flatten)              (None, 25088)               0          \r\n______________________________________________________________________\r\ndense_1 (Dense)                (None, 256)                 6422784    \r\n______________________________________________________________________\r\ndense (Dense)                  (None, 10)                  2570       \r\n======================================================================\r\nTotal params: 21,140,042\r\nTrainable params: 21,140,042\r\nNon-trainable params: 0\r\n______________________________________________________________________\r\n\r\nImage Augmentation\r\nImage augmentation is a commonly used technique to ensure the model is not overfit.\r\n(Brownlee 2019b) explained this technique allows one to artificially created new training data from existing data. The author further explained that the reason for using such a method is this method will aid the modern deep learning algorithms to learn the different features that are similar, but not entirely the same as the training photo.\r\nFor example, it would be a problem if the objects always appear on the same side of the photo in our dataset. The algorithm may not be able to identify the objects if the objects do not appear on the same side as what is observed under the training dataset in the new photo.\r\nSo, the code chunk below will perform image augmentation on the training data.\r\n\r\n\r\ntrain_datagen = image_data_generator(\r\n  rescale = 1/255,\r\n  rotation_range = 40,\r\n  width_shift_range = 0.2,\r\n  height_shift_range = 0.2,\r\n  shear_range = 0.2,\r\n  zoom_range = 0.2,\r\n  horizontal_flip = TRUE,\r\n  fill_mode = \"nearest\",\r\n  validation_split = 0.2\r\n)\r\n\r\n\r\n\r\nApart from that, 20% of the training dataset is being held back as the validation dataset.\r\nAlso, one important note to take note of while performing image augmentation is the validation & test dataset should not be augmented.\r\n\r\n\r\nvalidation_datagen <- image_data_generator(rescale = 1/255)  \r\n\r\n\r\n\r\nModel Preparation\r\nTo leverage on transfer learning, typically we will freeze the base model and only unfreeze the connecting layers so that we can fine-tune the layers together with the classifier layer.\r\n\r\n\r\nfreeze_weights(conv_base)\r\n\r\nunfreeze_weights(conv_base, from = \"block5_conv1\")\r\n\r\n\r\n\r\nNext, I will generate batches of data in the code chunk below.\r\n\r\n\r\ntrain_generator <- flow_images_from_directory(\r\n  train_dir,                  # Target directory  \r\n  train_datagen,              # Data generator\r\n  target_size = target_size,  # Resizes all images to 150 × 150\r\n  class_mode = \"categorical\"       # binary_crossentropy loss for binary labels\r\n)\r\n\r\nvalidation_generator <- flow_images_from_directory(\r\n  train_dir,\r\n  validation_datagen,\r\n  target_size = target_size,\r\n  class_mode = \"categorical\"\r\n)\r\n\r\n\r\n\r\nLastly, I will define all the different model components before fitting the model.\r\nI will be using optimizer_sgd (ie. stochastic gradient descent optimizer) to fit the model. Do check out the documentation page on the different optimizers.\r\nAs this is a multi-class classification problem, hence I will be using categorical_accuracy as the performance metric.\r\n\r\n\r\nmodel %>% compile(\r\n  loss = \"binary_crossentropy\",\r\n  optimizer = optimizer_sgd(),\r\n  metrics = c(\"categorical_accuracy\")\r\n)\r\n\r\n\r\n\r\nModel Fitting\r\nOnce the different model components are being defined, I will start the model fitting as shown below.\r\n\r\n\r\nhistory <- model %>% fit_generator(\r\n  train_generator,\r\n  steps_per_epoch = 100,\r\n  epochs = 10,\r\n  validation_data = validation_generator,\r\n  validation_steps = 50\r\n)\r\n\r\n\r\n\r\nVisualizing the Model Results\r\nOnce the model is fit, the history from the model fitting is passed into plot function to visualize how the results change when the epochs increase.\r\n\r\n\r\nplot(history)\r\n\r\n\r\n\r\n\r\nYay! Based on the accuracy shown above, it shows that the built model does have some levels of predictability, ie. the model will perform better than a random guess on the image category. Indeed, as the epoch increases, the model accuracy increases as well.\r\nAlso, as we can see in the graph above, the accuracy increases when the epoch increases. This suggests the accuracy could increase further if we increase the number of epoch.\r\nConclusion\r\nThat’s all for today!\r\nThanks for reading the post until the end.\r\nDo check out on the Keras documentation page or TensorFlow documentation page if you want to find out more on deep learning.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nPhoto by Александар Цветановић from Pexels\r\n\r\n\r\n\r\nBrownlee, Jason. 2019a. “Machine Learning Mastery: A Gentle Introduction to Transfer Learning for Deep Learning.” https://machinelearningmastery.com/transfer-learning-for-deep-learning/.\r\n\r\n\r\n———. 2019b. “Machine Learning Mastery: How to Configure Image Data Augmentation in Keras.” https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/.\r\n\r\n\r\nChollet, François, and J. J. Allaire. 2017. “RStudio AI Blog: Image Classification on Small Datasets with Keras.” https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/.\r\n\r\n\r\nGoogle. 2021. “TensorFlow: Transfer Learning and Fine-Tuning.” https://www.tensorflow.org/tutorials/images/transfer_learning.\r\n\r\n\r\nGupta, Dishashree. 2017. “Analytics Vidhya: Transfer Learning and the Art of Using Pre-Trained Models in Deep Learning.” https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/.\r\n\r\n\r\nSaha, Sumit. 2018. “Towards Data Science: A Comprehensive Guide to Convolutional Neural Networks — the Eli5 Way.” https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.\r\n\r\n\r\nShang, Kailan, ed. 2018. “Applying Image Recognition to Insurance.” Society of Actuaries. https://www.soa.org/globalassets/assets/Files/resources/research-report/2018/applying-image-recognition.pdf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-17-transfer-learning-with-cnn/image/image_on_wall.jpg",
    "last_modified": "2021-09-17T00:01:09+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-31-naive-bayes/",
    "title": "Naive Bayes Classifier",
    "description": "When the \"naive\" one outperform the conventional",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-09-04",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nDiscriminative Model vs Generative Model\r\nNaive Bayes Classifier\r\nDiscrim R Package\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Checking & Wrangling\r\nData Cleaning\r\nModel Building\r\nLogistic Regression\r\nNaive Bayes Classifier\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Shiva Smyth from Pexels\r\nConventionally, logistic regression is often used to solve classification problems.\r\nIn this post, I will explore an alternative popular machine learning algorithms, i.e. naive bayes classifier to solve classification problems.\r\nBefore diving into naive bayes classifier, let’s understand the difference between discriminative model and generative model.\r\nDiscriminative Model vs Generative Model\r\nDiscriminative models directly model the posterior probability \\(Pr(Y|X)\\). One good example of such model is logistic regression models.\r\nFor generative models, instead of directly model the posterior probability, this type of model will model the join distribution of X & Y before predicting the posterior probability given as \\(Pr(Y|X)\\) (Ottesen 2017). Naive bayes classifier is an example of such model.\r\nNaive Bayes Classifier\r\nIn this algorithm, the prior knowledge is being included when making predictions. Essentially the idea for this algorithm is given the features we observed in the predictors, what is likelihood the ‘y’ belong to the particular class?\r\nFollowing is the mathematical expression for naive bayes classifier obtained from Scikit-learn (scikit-learn):\r\n\\[\\hat{y} = arg max_y P(y)\\prod_{i=1}^n P(x_i|y)\\]\r\nOne of the key assumptions in Naive Bayes classifier is all the predictors are assumed to be independent from one another (Kuhn and Johnson 2013). This assumption has effectively simplified the calculations, allowing one to multiply the conditional probabilities together as shown in the mathematical expression above.\r\nAlthough the predictors are unlikely to be independent from one another, this model has a record of decent model performance.\r\nNevertheless, as what Dr Brownlee suggested in his post of tactics to combat imbalanced classes (Brownlee, n.d.), it is probably worthwhile to try different machine learning algorithms, instead of always sticking to our favorite algorithm. This would enable us in selecting the algorithm has a higher accuracy.\r\nDiscrim R Package\r\nIn this demonstration, naive bayes wrapper function from discrim package will be used.\r\nDiscrim packages contains various discriminant analysis models, including linear discriminant models and Naive Bayes Classifier.\r\nTo contrast the model performance, I will be using logistic_reg function from parnsip package to build logistic regression models.\r\nDemonstration\r\nFor the dataset, I will be using a travel insurance dataset from Kaggle.\r\n\r\n\r\n\r\nPhoto by Annie Spratt on Unsplash\r\nSetup the environment\r\nAs usual, I will start by calling all the relevant packages I would need in the analysis later on.\r\n\r\n\r\npackages <- c('tidyverse', 'readr', 'skimr', 'tidymodels', 'discrim', \r\n              'naivebayes', 'glmnet', 'tictoc', 'vip', 'shapr', \r\n              'DALEXtra', 'funModeling', 'plotly', 'readxl', 'ggmosaic')\r\n\r\nfor(p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nI will import the dataset obtained from Kaggle website.\r\n\r\n\r\ndf <- read_csv(\"data/travel insurance.csv\") %>%\r\n  rename(\"Commission\" = \"Commision (in value)\")\r\n\r\n\r\n\r\nData Checking & Wrangling\r\nAs usual, I will start with some basic data quality checking.\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n63326\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n7\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nAgency\r\n0\r\n1.00\r\n3\r\n3\r\n0\r\n16\r\n0\r\nAgency Type\r\n0\r\n1.00\r\n8\r\n13\r\n0\r\n2\r\n0\r\nDistribution Channel\r\n0\r\n1.00\r\n6\r\n7\r\n0\r\n2\r\n0\r\nProduct Name\r\n0\r\n1.00\r\n9\r\n36\r\n0\r\n26\r\n0\r\nClaim\r\n0\r\n1.00\r\n2\r\n3\r\n0\r\n2\r\n0\r\nDestination\r\n0\r\n1.00\r\n4\r\n42\r\n0\r\n149\r\n0\r\nGender\r\n45107\r\n0.29\r\n1\r\n1\r\n0\r\n2\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nDuration\r\n0\r\n1\r\n49.32\r\n101.79\r\n-2\r\n9\r\n22.00\r\n53.00\r\n4881.0\r\n▇▁▁▁▁\r\nNet Sales\r\n0\r\n1\r\n40.70\r\n48.85\r\n-389\r\n18\r\n26.53\r\n48.00\r\n810.0\r\n▁▇▁▁▁\r\nCommission\r\n0\r\n1\r\n9.81\r\n19.80\r\n0\r\n0\r\n0.00\r\n11.55\r\n283.5\r\n▇▁▁▁▁\r\nAge\r\n0\r\n1\r\n39.97\r\n14.02\r\n0\r\n35\r\n36.00\r\n43.00\r\n118.0\r\n▁▇▂▁▁\r\n\r\nGender\r\nThere is excessive missing value for Gender, i.e. only about 28.8% of the data contains values in Gender column. Hence, I will drop this columns since this variable is unlikely to be going to be meaningful in explaining the target variable.\r\nDestinations\r\nThere are about 149 unique values under destinations. This could be an issue when we use this feature to build machine learning model as there are too many unique categories.\r\nTo further group the destinations, I have extracted the mapping between destinations and continents from internet and imported the mapping results into the environment. This allows me to join with the existing dataset.\r\nBelow is code chunk I have imported into the environment and performed a left join with the original dataset:\r\n\r\n\r\ndestination_state <- read_excel(\"data/Destination_Continent_Mapping.xlsx\")\r\n\r\ndf <- df %>%\r\n  left_join(destination_state, by = c(\"Destination\" = \"Destination\"))\r\n\r\n\r\n\r\nNext, I use bar chart to plot the claim proportion by continent.\r\n\r\n\r\nggplot(df, aes(Continent, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion by Continents\")\r\n\r\n\r\n\r\n\r\nAs the proportion of claim policies is rather low, it is hard to compare the claim proportion across different continents.\r\n\r\n\r\ndf %>%\r\n  group_by(Continent, Claim) %>%\r\n  summarise(count = n()) %>%\r\n  pivot_wider(names_from = Claim, names_prefix = \"Claim_\", values_from = count) %>%\r\n  mutate(total = Claim_No + Claim_Yes,\r\n         Claim_perc = Claim_Yes / total * 100)\r\n\r\n\r\n# A tibble: 6 x 5\r\n# Groups:   Continent [6]\r\n  Continent     Claim_No Claim_Yes total Claim_perc\r\n  <chr>            <int>     <int> <int>      <dbl>\r\n1 Africa             298         5   303      1.65 \r\n2 Asia             49596       771 50367      1.53 \r\n3 Europe            4991        63  5054      1.25 \r\n4 North America     3041        45  3086      1.46 \r\n5 Oceania           4234        42  4276      0.982\r\n6 South America      239         1   240      0.417\r\n\r\nProduct Name\r\nI will plot the claim proportion by product names.\r\n\r\n\r\nggplot(df, aes(x = `Product Name`, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion by Product Name\")\r\n\r\n\r\n\r\n\r\nThere are too many categories under product name. This would affect the performance of machine learning models if we use this variable to build models.\r\nHence, I will further group the product names so that this feature does not have too many unique categories.\r\nAs I am unable to find any further information on the product, hence I have kept products with top 90% of the sales as their original naming and renamed the remaining products as ‘Others.’\r\nOnce the product names are being recoded, I will join the recoded product names back to the original dataset.\r\n\r\n\r\ndf_prod <- df %>%\r\n  group_by(`Product Name`) %>%\r\n  summarise(count = n(),\r\n            claim_ind = sum(Claim == \"Yes\")) %>%\r\n  mutate(claim_perc = claim_ind/count) %>%\r\n  arrange(desc(count)) %>%\r\n  mutate(claim_cum_perc = cumsum(count)/sum(count),\r\n         product_name_recoded = case_when(claim_cum_perc > 0.9 ~ \"Others\",\r\n                                          TRUE ~ as.character(`Product Name`))) %>%\r\n  select(-c(\"claim_perc\", \"claim_cum_perc\", \"claim_ind\", \"count\")) %>%\r\n  ungroup()\r\n\r\ndf <- df %>%\r\n  left_join(df_prod, by = c(\"Product Name\" = \"Product Name\"))\r\n\r\n\r\n\r\n\r\n\r\ndf %>%\r\n  filter(Duration < 1000) %>%\r\n  ggplot(aes(product_name_recoded, fill = Claim)) +\r\n  geom_bar(position = \"fill\") +\r\n  theme(axis.text.x = element_text(angle = 90)) +\r\n  labs(title = \"Claim Proportion of Product Name Recoded\")\r\n\r\n\r\n\r\n\r\nAs shown above, we can see that claim proportion for Bronze Plan and Others are higher than other plans.\r\nDuration\r\nBelow is the duration density plot:\r\n\r\n\r\nduration_mean <- df %>%\r\n  summarise(duration_mean = mean(Duration))\r\n\r\nggplot(df, aes(Duration)) +\r\n  geom_density() +\r\n  geom_vline(data = duration_mean, aes(xintercept = duration_mean), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 800, y = 0.025, label = paste0(\"Mean of Duration: \", round(duration_mean$duration_mean, 2))) +\r\n  labs(title = \"Density of Duration\")\r\n\r\n\r\n\r\n\r\nAs shown in the graph, most of the trips were very short trip.\r\nIf we were removed any data points with duration longer than 1000 and plot duration density between claim and no claim policies, following is the chart:\r\n\r\n\r\nduration_mean_claim <- df %>%\r\n  filter(Duration < 1000, Claim == \"Yes\") %>%\r\n  summarise(mean_dur = mean(Duration),\r\n            mean_dur_log = log(mean_dur + 1))\r\n\r\nduration_mean_Noclaim <- df %>%\r\n  filter(Duration < 1000, Claim == \"No\") %>%\r\n  summarise(mean_dur = mean(Duration),\r\n            mean_dur_log = log(mean_dur + 1))\r\n  \r\n  \r\ndf %>%\r\n  filter(Duration < 1000) %>%\r\n  ggplot(aes(x = log(Duration + 1), color = Claim)) +\r\n  geom_density(alpha = 0.2) + \r\n  geom_vline(data = duration_mean_claim, aes(xintercept = mean_dur_log), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 5.65, y = 0.32, label = \"Average Duration - \") +\r\n  annotate(\"text\", x = 5.65, y = 0.3, label = \"Claim Policies\") +\r\n  geom_vline(data = duration_mean_Noclaim, aes(xintercept = mean_dur_log), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 3, y = 0.03, label = \"Average Duration - \") +\r\n  annotate(\"text\", x = 3, y = 0.01, label = \"No Claim Policies\") +\r\n  labs(title = \"Density of Duration between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nIt seems like the average duration of the policies with no claim is shorter than the average duration of the duration with claim. This is logical since the duration is likely to represent the coverage duration of the travel insurance. Typically the coverage of the travel insurance starts from the day the insurance is purchased till the day the trip is being completed. Therefore, with a higher duration, it is more likely the customers would make a claim during the coverage period.\r\nMeanwhile, I do note that there are negative values within this variable, which the values do not seem to be reasonable. Therefore, I will remove them from the dataset.\r\nSales\r\nIn the earlier data summary, there are some policies with sales with zero or even negative. This is not logical as the sales of insurance policies would be positive. As lack of information, I would remove these policies from the dataset so that it will not affect the model performance.\r\n\r\n\r\nnetSales_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\",\r\n         `Net Sales` > 0) %>%\r\n  summarise(netSales_mean_claim = mean(`Net Sales`))\r\n\r\nnetSales_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\",\r\n         `Net Sales` > 0) %>%\r\n  summarise(netSales_mean_Noclaim = mean(`Net Sales`))\r\n\r\ndf %>%\r\n  filter(`Net Sales` > 0) %>%\r\n  ggplot(aes(`Net Sales`, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = netSales_mean_claim, aes(xintercept = netSales_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 240, y = 0.03, label = \"Average Sales - Claim Policies\") +\r\n  geom_vline(data = netSales_mean_Noclaim, aes(xintercept = netSales_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 200, y = 0.02, label = \"Average Sales - No Claim Policies\") +\r\n  labs(title = \"Density of Sales between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nCommission\r\nSimilarly, I will plot the commission distribution between claim and no claim policies.\r\n\r\n\r\ncomm_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\") %>%\r\n  summarise(comm_mean_claim = mean(Commission))\r\n\r\ncomm_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\") %>%\r\n  summarise(comm_mean_Noclaim = mean(Commission))\r\n\r\ndf %>%\r\n  ggplot(aes(Commission, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = comm_mean_claim, aes(xintercept = comm_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 80, y = 0.1, label = \"Average Commission - Claim Policies\") +\r\n  geom_vline(data = comm_mean_Noclaim, aes(xintercept = comm_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 100, y = 0.15, label = \"Average Commission - No Claim Policies\") +\r\n  labs(title = \"Density of Commission between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nFrom the graph, the average commission for policies that have made a claim is lower than the average commission for policies that does not make a claim.\r\nAge\r\n\r\n\r\nage_mean_claim <- df %>%\r\n  filter(Claim == \"Yes\") %>%\r\n  summarise(age_mean_claim = mean(Age))\r\n\r\nage_mean_Noclaim <- df %>%\r\n  filter(Claim == \"No\") %>%\r\n  summarise(age_mean_Noclaim = mean(Age))\r\n\r\ndf %>%\r\n  ggplot(aes(Age, color = Claim)) +\r\n  geom_density() +\r\n  geom_vline(data = age_mean_claim, aes(xintercept = age_mean_claim), linetype=\"dashed\", size=0.5) +\r\n  annotate(\"text\", x = 20, y = 0.1, label = \"Average Age - \") +\r\n  annotate(\"text\", x = 20, y = 0.088, label = \"Claim Policies\") +\r\n  geom_vline(data = age_mean_Noclaim, aes(xintercept = age_mean_Noclaim), linetype=\"solid\", size=0.5) +\r\n  annotate(\"text\", x = 68, y = 0.15, label = \"Average Age - No Claim Policies\") +\r\n  labs(title = \"Density of Age between Claim and No Claim Policies\")\r\n\r\n\r\n\r\n\r\nUnlike the previous numerical variables, it seems like the average age between claim and no claim policies is quite minimal.\r\nAlso, it seems like there are weird ages within the dataset as well. There are 984 policies with age 118, which seem to be what we know as ‘magic value’ in feature engineering. One possible explanation for this is the system will default the age to be 118 if the age is not being input into the system.\r\n\r\n\r\ndf %>%\r\n  filter(Age > 100) %>%\r\n  tally()\r\n\r\n\r\n# A tibble: 1 x 1\r\n      n\r\n  <int>\r\n1   984\r\n\r\nAs I do not have any further information to estimate the ages for these policies, I will remove them from the dataset before building the model.\r\nData Cleaning\r\nBefore building the model, I will proceed and clean the data.\r\n\r\n\r\ndf_1 <- df %>%\r\n  select(-c(Gender, `Product Name`, Destination)) %>%\r\n  filter(`Net Sales` > 0,\r\n         Duration > 0,\r\n         Age < 100) %>%\r\n  rename_with(~gsub(\" \", \"_\", .x, fixed = TRUE))\r\n\r\n\r\n\r\nModel Building\r\nBelow is the generic parameters set for this analysis:\r\n\r\n\r\nset.seed(1234)\r\n\r\nprop_split <- 0.6\r\n\r\ngrid_num <- 5\r\n\r\n\r\n\r\nIn this section, I will split the data into training and testing dataset. Once this is done, I will prepare the dataset for cross validation later.\r\n\r\n\r\ndf_split <- initial_split(df_1, prop = prop_split, strata = Claim)\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\ndf_folds <- vfold_cv(df_train, strata = Claim)\r\n\r\n\r\n\r\nNext, I will define the formula and dataset to be used to train the model.\r\n\r\n\r\ngen_recipe <- recipe(Claim ~ ., data = df_train)\r\n\r\n\r\n\r\nOkay, let’s start the analysis by using our classic machine learning model for classification problem - logistic regression!\r\nLogistic Regression\r\nTo record how long it takes to build a logistic regression model, I use tic & toc functions from tictoc package to capture the time spent in building model.\r\n\r\n\r\ntic(\"Time to Build Logistic Regression\")\r\n\r\n\r\n\r\nFirst, I will define the data pre-processing steps to be performed.\r\n\r\n\r\nlogit_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal_predictors()) %>%\r\n  step_zv(all_predictors()) %>%\r\n  step_normalize(all_predictors())\r\n\r\n\r\n\r\nThen, I will define the model to be built. I have also indicated the parameters to be tuned by using tune function to mark the parameters.\r\n\r\n\r\nlogit_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"glmnet\")\r\n\r\n\r\n\r\nOnce this is done, I will chain all the created objects as a workflow.\r\n\r\n\r\nlogit_workflow <- workflow() %>% \r\n  add_recipe(logit_recipe) %>%\r\n  add_model(logit_spec)\r\n\r\n\r\n\r\nThen, I will start performing cross validation to find the best set of parameters.\r\n\r\n\r\nlogit_grid <- tidyr::crossing(penalty = c(1,2,3,4,5,6,7,8,9,10), mixture = c(0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1))\r\n\r\nlogit_tune <- tune_grid(logit_workflow, \r\n                        resample = df_folds, \r\n                        grid = logit_grid)\r\n\r\n\r\n\r\nThe model is then fitted with the best set of parameters.\r\n\r\n\r\nlogit_fit <- logit_workflow %>%\r\n  finalize_workflow(select_best(logit_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n\r\n\r\nThe predicted values are then “collected” so that I could use them to calculate the necessary model performance.\r\n\r\n\r\nlogit_pred <- logit_fit %>%\r\n  collect_predictions()\r\n\r\n\r\n\r\nOnce the predicted values are being “collected,” toc function is used to calculate how long it takes to fit the model and make the necessary predictions.\r\n\r\n\r\ntoc()\r\n\r\n\r\nTime to Build Logistic Regression: 196.01 sec elapsed\r\n\r\nAccuracy is typically used to measure classification model.\r\n\r\n\r\naccuracy(logit_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.984\r\n\r\nBased on the accuracy results, it seems like this logistic model is a pretty decent model given it has such a high accuracy. More than 98% of the policies are being accurately classified.\r\nHowever, it can be very misleading if our decision is just based on accuracy measurement. Accuracy looks the number of policies are being accurately classified whether they have claimed or not. It does not differentiate the true positive and true negative in this case.\r\nHowever, in insurance, we are more interested in the policies that have claimed. One way is to compute the confusion matrix for the relevant model.\r\n\r\n\r\nconf_mat(logit_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n          Truth\r\nPrediction    No   Yes\r\n       No  23539   372\r\n       Yes     0     0\r\n\r\nOh no! According to the results under confusion matrix, our model always predict that the policies will not make a claim. This is bad as this would result in the expected claim severely unstated.\r\nNow let’s us plot the ROC curve.\r\n\r\n\r\nautoplot(roc_curve(logit_pred, \r\n                   truth = Claim, \r\n                   estimate = .pred_No))\r\n\r\n\r\n\r\n\r\nThe ROC curve for this model lie at the diagonal line, suggesting that this fitted model has no predictive power. It is as good as we take a random guess whether the selected policy will make a claim.\r\nIf we were to visualize the claim count by using a histogram, we can see that the proportion of policyholders claimed is very low.\r\n\r\n\r\nggplot(df_1, aes(Claim)) +\r\n  geom_histogram(stat = \"count\")\r\n\r\n\r\n\r\n\r\nNaive Bayes Classifier\r\nOkay, let’s move on and build a naive bayes classifier model.\r\nFirst, I will use tic function to indicate where I should start measuring the time taken to build the model.\r\n\r\n\r\ntic(\"Time to Build Naive Bayes Classifier\")\r\n\r\n\r\n\r\nThen, I will start building the model, same as how I did it for logistic regression.\r\n\r\n\r\nnaive_recipe <- gen_recipe %>%\r\n  step_zv(all_predictors())\r\n\r\nnaive_spec <- naive_Bayes(smoothness = tune(), Laplace = tune()) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"naivebayes\")\r\n\r\nnaive_workflow <- workflow() %>%\r\n  add_recipe(naive_recipe) %>%\r\n  add_model(naive_spec)\r\n\r\nnaive_tune <- tune_grid(naive_workflow, \r\n                        resample = df_folds, \r\n                        grid = grid_num)\r\n\r\nnaive_fit <- naive_workflow %>%\r\n  finalize_workflow(select_best(naive_tune)) %>%\r\n  last_fit(df_split)\r\n\r\nnaive_pred <- naive_fit %>%\r\n  collect_predictions()\r\n\r\n\r\n\r\n\r\n\r\ntoc()\r\n\r\n\r\nTime to Build Naive Bayes Classifier: 36.62 sec elapsed\r\n\r\nNote that the naive bayes classifier takes lesser time to build the model. In fact, it took less time to build naive bayes classifier than build logistic regression.\r\nThis is consistent with what we have discussed in the earlier section of this post.\r\n\r\n\r\naccuracy(naive_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric  .estimator .estimate\r\n  <chr>    <chr>          <dbl>\r\n1 accuracy binary         0.941\r\n\r\nSimilarly, let’s check the confusion matrix results for our naive bayes classifier.\r\n\r\n\r\nconf_mat(naive_pred, \r\n         truth = Claim,\r\n         estimate = .pred_class)\r\n\r\n\r\n          Truth\r\nPrediction    No   Yes\r\n       No  22391   252\r\n       Yes  1148   120\r\n\r\nNote that the model no longer always predict policyholders do not claim from their travel insurance.\r\nWhen we plot the ROC curve, we noted that the ROC curve no longer lie on the diagonal line. This suggests that although the accuracy of this fitted model is lower than logistic regression, this fitted model does have some levels of predictive power.\r\n\r\n\r\nautoplot(roc_curve(naive_pred, \r\n                   truth = Claim, \r\n                   estimate = .pred_No))\r\n\r\n\r\n\r\n\r\nOverall, we can see that naive bayes classifier performs better than logistic regression in this data.\r\nOne of the possible reason why naive bayes classifier outperforms logistic regression is due to the small dataset. (Ng and Jordan) discussed that how the model performance for logistic regression will outperform naive bayes classifier when the training size reaches infinity.\r\nHowever, for this scenario, the dataset seems to be too small for logistic regression to converge and outperform naive bayes classifier. The performance of logistic regression model might improve further when we increase the training data.\r\nConclusion\r\nThat’s all for the day! We have finished “sorting” the policies in whether they are expected to claim or does not claim.\r\n\r\n\r\n\r\nPhoto by emrecan arık on Unsplash\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page if you want to find out more on naive bayes classifier.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nBrownlee, Jason, ed. n.d. “8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset.” https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/.\r\n\r\n\r\nKuhn, Max, and Kjell Johnson, eds. 2013. Applied Predictive Modeling. Springer.\r\n\r\n\r\nNg, Andrew, and Michael Jordan, eds. “On Discriminative Vs. Genererative Classifiers: A Comparison of Logistic Regression and Naive Bayes.” http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf.\r\n\r\n\r\nOttesen, Christopher. 2017. “Comparison Between Naïve Bayes and Logistic Regression.” https://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/.\r\n\r\n\r\nscikit-learn, ed. “1.9 Naive Bayes.” https://scikit-learn.org/stable/modules/naive_bayes.html.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-31-naive-bayes/image/balance.jpg",
    "last_modified": "2021-09-04T22:45:44+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-23-model-stacking/",
    "title": "Model Stacking",
    "description": "When the sum of all component is greater than individual component",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-05-23",
    "categories": [
      "Machine Learning",
      "Supervised Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is Ensemble method?\r\nSo, how does stacking work?\r\nWhy is this method popular?\r\nStacks R Package\r\nDemonstration\r\nSetup the environment\r\nImport Data\r\nData Splitting\r\nDefine Recipe\r\nFitting Individual Machine Learning Models\r\nStack the Models!\r\nCompare the Model Performance\r\nPerformance Metric of Individual Models\r\nPerformance Metric of Stack Model\r\nSummary\r\n\r\n\r\nConclusion\r\n\r\n\r\n\r\n\r\nPhoto by Crissy Jarvis on Unsplash\r\nIn this series of modeling, I will be exploring model stacking.\r\nIt has gained popularity especially in data science competition as such method could further improve the accuracy of the machine learning models.\r\nBefore jumping straight into model stacking, let’s understand what is ensemble method.\r\nWhat is Ensemble method?\r\n\r\n\r\n\r\nExtracted from vas3k blog (vas3k)\r\nEnsemble method is one of the machine learning method.\r\n(Geron 2019) described that ensemble method is similar to aggregate the answer of a complex question from thousands of random people. This anchored on the idea of wisdom of the crowd. Often this approach would produce a better answer than an answer from an expert.\r\nSuch method can be either applied on the same machine learning algorithms or aggregate across different machine learning algorithms.\r\nAs the graph shown above, following are the three common ensemble methods:\r\nBagging: Different models are trained by using different dataset randomly drawn with replacement from the training dataset. The final prediction is the average of predictions from all the fitted models. Random forest is one of the algorithm that leverages on bagging to improve the model accuracy.\r\nBoosting: The idea is to a combination of weak learners could form a strong learner. XGBoost is one commonly used model algorithm that uses boosting method.\r\nStacking: As the name suggested, we “stack” the models on one another and create a new model.\r\nIn this post, I will be exploring model stacking and mainly covering the following topics:\r\nHow does model stacking work?\r\nHow to implement model stacking in R?\r\nSo, how does stacking work?\r\nIf we were to visualize the steps for model stacking, this would be how it would look like:\r\n\r\n\r\n\r\nFollowing are the steps to perform model stacking:\r\nFirst, fit different individual machine learning models\r\nNext, use target variable from the training dataset and the predictions from the different machine learning models as input to fit a model\r\nThe conventional way to stack the model is to use linear model. Although in theory we could use other machine learning algorithm to stack the models, this would increase the complexity of the final model, making it even harder to interpret.\r\nWhy is this method popular?\r\n(Funda Gunes and Tan 2017) discussed in their paper that ensemble method enables the users to average out the noise from the diverse models and thereby enhance the generalizable signal.\r\nTherefore, such method has proven to increase model accuracy in Kaggle competitions. Often, even a slight uplift in improvement could affect our model performance rankings in the Kaggle competitions.\r\nWhile this method does not guarantee an improvement in the accuracy, it is worthwhile to fit such models to attempt to improve the accuracy of the predictions.\r\nHowever, by stacking the different models together, this has increased the complexity of the models, making it even harder to understand on how the model reaches the decisions.\r\nThe time required for model fitting would be longer as well since we would need to fit the base model before perform model stacking.\r\nStacks R Package\r\nOkay, let’s perform some model stacking!\r\n\r\n\r\n\r\nPhoto by La-Rel Easter on Unsplash\r\nIn this demonstration, I will be using a package from tidymodels that allows the users to perform model stacks with just a few lines of codes. This package is called stacks.\r\n\r\n\r\n\r\nstacks package\r\nAnd yes, this package is it conforms to the tidy data concept. It is also designed to work together with the various packages under tidymodels.\r\nWHAT A BIG RELIEF!\r\n\r\n\r\n\r\nPhoto by Andrea Piacquadio from Pexels\r\nThis would make it easier for one to use perform model stacking.\r\nDemonstration\r\nSetup the environment\r\nBefore I start the demonstration, I will setup the environment by calling the necessary packages.\r\n\r\n\r\npackages <- c(\"tidyverse\", \"tidymodels\", \"stacks\")\r\n\r\nfor (p in packages){\r\n  if(!require (p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\nI will also define the common parameters that will be used in the analysis below.\r\n\r\n\r\n# Define the random seeds for reproducibility\r\nset.seed(1234)\r\n\r\n# Proportion between training and testing dataset\r\nprop_train_test <- 0.6\r\n\r\n# Define the model performance metrics we would like to output later\r\nmodel_metrics <- metric_set(rmse, rsq)\r\n\r\n# The number of grid to be used in the analysis later\r\ngrid_num <- 5\r\n\r\n\r\n\r\nTo stack model, we need to include following as well. According to the documentation page, this is to ensure the predictions and necessary info are output in the necessary format required by stacks package later.\r\n\r\n\r\nctrl_grid <- control_stack_grid()\r\nctrl_res <- control_stack_resamples()\r\n\r\n\r\n\r\n\r\n\r\n\r\nImport Data\r\nFor this demonstration, I will be using a dataset from Actuarial Loss Prediction Kaggle Competition.\r\nAlso, this sharing will be focusing on the model stacking, instead of end to end process. Hence, I will import the data I have previously cleaned.\r\nRefer to my EDA code file on my Github page for the steps taken to clean the data.\r\n\r\n\r\ndf <- read_csv(\"data/data_eda_actLoss_3.csv\") %>%\r\n  dplyr::select(-c(ClaimNumber,\r\n                   num_week_paid_ult,\r\n                   InitialIncurredClaimCost,\r\n                   UltimateIncurredClaimCost)) %>%\r\n  dplyr::select(-starts_with(c(\"acc\", \"report\"))) %>%\r\n  filter(Gender != \"U\") %>%\r\n  drop_na() %>%\r\n  sample_frac(0.3)\r\n\r\n\r\n\r\nWhile importing the data, I have also:\r\nDrop ‘ClaimNumber,’ ‘num_week_paid_ult’ & column headers that starts with ‘acc’ & ‘report’ since I am not using them later\r\nFilter out gender ‘U’ as according to data dictionary, indicator ‘U’ means missing values\r\nDrop the data with missing values\r\nSample 30% of the data to use for the analysis, otherwise it would take forever to run on my laptop 😢\r\nData Splitting\r\nNext, I will split the dataset into training and testing dataset.\r\n\r\n\r\n# Split dataset\r\ndf_split <- initial_split(df,\r\n                            prop = prop_train_test,\r\n                            strata = init_ult_diff)\r\n\r\ndf_train <- training(df_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nThen, I will prepare the dataset for the k-fold validation later.\r\n\r\n\r\n# Cross validation\r\ndf_folds <- vfold_cv(df_train, strata = init_ult_diff)\r\n\r\n\r\n\r\nDefine Recipe\r\nIn this section, I will define the general recipe for the different machine learning models.\r\n\r\n\r\ngen_recipe <- recipe(init_ult_diff ~ ., data = df_train) %>%\r\n  update_role(c(DateTimeOfAccident, DateReported, ClaimDescription), new_role = \"id\") %>% # update the roles of original date variables to \"id\"\r\n  prep()\r\n\r\n\r\n\r\nBasically, I try to perform the following in the code chunk above:\r\nExtract the date information from date variable by using step_date & step_mutate functions\r\nConvert the extracted date features into ordinal variables by using factor function\r\nUpdate the roles for ‘DateTimeOfAccident’ & ‘Date Reported’ to ‘id’ so that they will not be used in fitting the model\r\nFitting Individual Machine Learning Models\r\nBefore fitting a stack model, let’s fit individual models first.\r\nRandom Forest\r\n\r\n\r\nranger_spec <- \r\n  rand_forest() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"ranger\", importance = \"impurity\")\r\n\r\nranger_workflow <- \r\n  workflow() %>% \r\n  add_recipe(gen_recipe) %>% \r\n  add_model(ranger_spec) \r\n\r\nset.seed(51107)\r\nranger_tune <-\r\n  tune_grid(ranger_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\nxgboost_recipe <- gen_recipe %>%\r\n  step_dummy(all_nominal())\r\n\r\nxgboost_spec <- \r\n  boost_tree() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"xgboost\") \r\n\r\nxgboost_workflow <- \r\n  workflow() %>% \r\n  \r\n  add_recipe(xgboost_recipe) %>% \r\n  add_model(xgboost_spec) \r\n\r\nset.seed(12071)\r\nxgboost_tune <-\r\n  tune_grid(xgboost_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\nearth_recipe <- gen_recipe %>% \r\n  step_novel(all_nominal(), -all_outcomes()) %>% \r\n  step_dummy(all_nominal(), -all_outcomes()) %>% \r\n  step_zv(all_predictors()) \r\n\r\nearth_spec <- \r\n  mars() %>%\r\n  set_mode(\"regression\") %>% \r\n  set_engine(\"earth\") \r\n\r\nearth_workflow <- \r\n  workflow() %>% \r\n  add_recipe(earth_recipe) %>% \r\n  add_model(earth_spec) \r\n\r\nearth_grid <- tidyr::crossing(num_terms = 2 * (1:6), prod_degree = 1:2) \r\n\r\nearth_tune <- \r\n  tune_grid(earth_workflow, \r\n            resamples = df_folds, \r\n            grid = grid_num,\r\n            control = ctrl_grid)\r\n\r\n\r\n\r\nStack the Models!\r\nOkay, we have fitted all the individual models. I will move on and fit a stack model.\r\nTo do so, I will use stack function to create an empty stack model and add on the individual fitted models earlier on.\r\n\r\n\r\nstack_model <-\r\n  stacks() %>%\r\n  add_candidates(ranger_tune) %>%\r\n  add_candidates(xgboost_tune) %>%\r\n  add_candidates(earth_tune)\r\n\r\n\r\n\r\nNext, I will use blend_predictions function to find the coefficient of individual models.\r\n\r\n\r\nstack_model_pred <-\r\n  stack_model %>%\r\n  blend_predictions()\r\n\r\n\r\n\r\nTo see the weights of the models, we can call the object we have created in the previous step.\r\n\r\n\r\nstack_model_pred\r\n\r\n\r\n# A tibble: 3 x 3\r\n  member           type        weight\r\n  <chr>            <chr>        <dbl>\r\n1 xgboost_tune_1_1 boost_tree   0.477\r\n2 ranger_tune_1_1  rand_forest  0.475\r\n3 earth_tune_1_1   mars         0.158\r\n\r\nAs the results shown above, random forest, xgboost and mars are used to create the stacked model.\r\nWe can also use the autoplot function to visualize the weight for different models.\r\n\r\n\r\nautoplot(stack_model_pred, type = \"weights\")\r\n\r\n\r\n\r\n\r\nOnce the weights of the different models are determined, I will prepare the fitted stack model by using fit_members function.\r\n\r\n\r\nstack_model_fit <- stack_model_pred %>%\r\n  fit_members()\r\n\r\n\r\n\r\nCompare the Model Performance\r\nPerformance Metric of Individual Models\r\nTo compare the model performance from different models, I will do the following:\r\nFirst, finalize the workflow by selecting the best parameters & perform last_fit on the model\r\nThen, collect the model predictions by using collect_predictions\r\nLastly, calculate the model performance, create a column named ‘model’ and pivot the columns\r\nRanger metrics\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nranger_fit <- ranger_workflow %>%\r\n  finalize_workflow(select_best(ranger_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nranger_pred <- ranger_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nranger_metric <- model_metrics(ranger_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"ranger\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nXGBoost\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nxgboost_fit <- xgboost_workflow %>%\r\n  finalize_workflow(select_best(xgboost_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nxgboost_pred <- xgboost_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nxgboost_metric <- model_metrics(xgboost_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"xgboost\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nMARS\r\n\r\n\r\n# Finalize the workflow by selecting the best parameters\r\nearth_fit <- earth_workflow %>%\r\n  finalize_workflow(select_best(earth_tune)) %>%\r\n  last_fit(df_split)\r\n\r\n# Extract the predictions from the fitted model\r\nearth_pred <- earth_fit %>%\r\n  collect_predictions()\r\n\r\n# Calculate the model performance metric\r\nearth_metric <- model_metrics(earth_pred, \r\n                               truth = init_ult_diff, \r\n                               estimate = .pred) %>%\r\n  mutate(model = \"earth\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nPerformance Metric of Stack Model\r\nNext, I will calculate the model performance for the stack model.\r\n\r\n\r\nstack_metric <- predict(stack_model_fit, df_test) %>%\r\n  bind_cols(init_ult_diff = df_test$init_ult_diff) %>%\r\n  model_metrics(truth = init_ult_diff, estimate = .pred) %>%\r\n  mutate(model = \"stack\") %>%\r\n  pivot_wider(names_from = .metric,\r\n              values_from = .estimate)\r\n\r\n\r\n\r\nSummary\r\nNow I will combine all the results in a tibble table so that its easier to compare the performance metrics from different models.\r\n\r\n\r\ntibble() %>%\r\n  bind_rows(stack_metric) %>%\r\n  bind_rows(ranger_metric) %>%\r\n  bind_rows(xgboost_metric) %>%\r\n  bind_rows(earth_metric)\r\n\r\n\r\n# A tibble: 4 x 4\r\n  .estimator model    rmse   rsq\r\n  <chr>      <chr>   <dbl> <dbl>\r\n1 standard   stack   1547. 0.437\r\n2 standard   ranger  1580. 0.416\r\n3 standard   xgboost 1563. 0.427\r\n4 standard   earth   1662. 0.356\r\n\r\nAs shown above, both model performance metrics improved after we stacked the models.\r\nConclusion\r\nThat’s all for the day!\r\nThanks for reading the post until the end.\r\nDo check out on the documentation page if you want to find out more on model stacking.\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\nFunda Gunes, Russ Wolfinger, and Pei-Yi Tan, eds. 2017. “Stacked Ensemble Models for Improved Prediction Accuracy.” http://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf.\r\n\r\n\r\nGeron, Aurelien, ed. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media.\r\n\r\n\r\nvas3k, ed. “Machine Learning for Everyone - in Simple Words. With Real-World Examples. Yes, Again.” https://vas3k.com/blog/machine_learning/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-23-model-stacking/image/cup_stack.jpg",
    "last_modified": "2021-09-04T21:42:23+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-22-data-wrangling/",
    "title": "Data Wrangling - The Quest to Tame The 'Beast'",
    "description": "Rawwwwwwwwww!",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "Data Wrangling"
    ],
    "contents": "\r\n\r\n\r\n\r\nArtwork on Data Wrangling by Allison Horst\r\nAs mentioned in my last post, it has been a long journey to learn data science pretty much from scratch. I intend to share some of my learning to help others to start their data science journey as well.\r\nTherefore, for my second post, I will share about data wrangling, which is often the essential task before building any machine learning models. Hence this post will serve as a building block of my future machine learning posts.\r\nAlso, I intend to share some of the modern data science R package and how these packages could benefit some of the analysis tasks.\r\nBackground\r\nIn R for Data Science book, Wickham & Grolemund mentioned that a typical data science project would look as following:\r\n\r\n\r\n\r\nExtracted from R for Data Science\r\nAfter importing the data, the very first step usually involves some levels of data wrangling. This is probably one of the most important steps as without a good dataset, the analysis is unlikely to be meaningful.\r\nGolden rule –> Garbage in, garbage out\r\nA good data exploratory will help to answer the following questions (not limited to):  - Does this dataset answer the business questions (eg. does the dataset have the necessary info to predict which customers are likely to purchase in the next promotion?)?  - Are there any issues that exist in the dataset (eg. missing value, excessive categories, value range vary widely)?  - Any data transformation required? As some of the machine learning models can accept certain data types. \r\nThrough understanding the data, it will also give us some insights on whether there is any more “gold” we can squeeze out from data through feature engineering.\r\nIt will also provide us some clues whether the variable is a good predictor even without needing one to fit the model.\r\nThe conventional method of data wrangling is to perform data cleaning and transformation in Excel.\r\n\r\n\r\n\r\nPhoto by Mika Baumeister on Unsplash\r\nHowever, this method is not very sustainable/ideal due to the following reasons:  - Excel has memory limitation and the software would keep on crashing as the data size increases  - Become a hurdle when we want to join different tables together or perform a transformation on the data  - More challenging to extract insights from the unstructured data (eg. perform text mining) \r\nIntroduction to Tidyverse\r\nWhen my professor first introduced this R package to me, I was amazed by what we could perform by using this package. And in fact, the more I use it, the more I love it.\r\ntidyverse is a collection of R packages designed by RStudio.\r\n\r\n\r\n\r\nThe very awesome thing about tidyverse is all of the packages are following the tidy data concept.\r\nSo what the heck is tidy data?\r\nTidy data is a concept on how to store/structure the data/results introduced by Hadley Wickham.\r\n\r\n\r\n\r\nArtwork on Tidy Data by Allison Horst\r\nBelow are the definition of tidy data:  - Each variable must have its column  - Each observation must have its row  - Each value must have its cell \r\n\r\n\r\n\r\nExtraction from R for Data Science book\r\nWhy is tidy data awesome?\r\nThis is as simple as the time spent transforming the dataset/output is now lesser.\r\nWhy less time is required?\r\nThe functions are aligned on the input data formats they require.\r\nOften this allows us to ‘join’ the function together (also often known as “pipe” in tidyverse context. Don’t worry about the meaning of “pipe” for now, will be covering what I mean by “pipe” later in this post).\r\nTherefore, as a user, I find that the learning curve of using tidyverse to perform data wrangling is less steep than other programming languages. This has effectively allowed me to spend more time on the analysis itself.\r\n\r\n\r\n\r\nArtwork by Allison Horst\r\nEnough talking, let’s get our hands dirty.\r\nIllustration\r\nI will be using the motor insurance dataset taken from CASdataset as a demonstration. In this illustration, I am interested to find out how the different profiles of the insured affected the total premium.\r\nPreparation of the “equipment”\r\nOver here, I call the necessary R packages by using a for-loop function.\r\n\r\n\r\npackage <- c('CASdatasets', 'tidyverse', 'skimr', 'funModeling', 'ggplot2', 'plotly', 'ggstatsplot')\r\n\r\nfor (p in package){\r\n  if(!require (p, character.only = TRUE)){\r\n    install(p)\r\n  }\r\n  library(p, character.only = TRUE)\r\n}\r\n\r\n\r\n\r\nThe code chunk will first check whether the relevant packages are installed in the machine.\r\nIf it is not installed, it will first install the required package.\r\nAfter that, the relevant packages will be attached to the environment.\r\nCalling the “beast”\r\nAfter loading the relevant packages, I will indicate to R the dataset I want through data function as there are many datasets within CASdatasets package. I have renamed the dataset to df to shorten the name after attaching the dataset into the environment.\r\n\r\n\r\ndata(fremotor1prem0304a)\r\ndf <- fremotor1prem0304a\r\n\r\n\r\n\r\nNow, tame the “beast”\r\nConventionally, summary function is used to check the quality of the data.\r\n\r\n\r\nsummary(df)\r\n\r\n\r\n    IDpol                Year         DrivAge      DrivGender\r\n Length:51949       Min.   :2003   Min.   :18.00   F:17794   \r\n Class :character   1st Qu.:2003   1st Qu.:31.00   M:34155   \r\n Mode  :character   Median :2003   Median :38.00             \r\n                    Mean   :2003   Mean   :39.84             \r\n                    3rd Qu.:2004   3rd Qu.:47.00             \r\n                    Max.   :2004   Max.   :97.00             \r\n                                                             \r\n    MaritalStatus     BonusMalus       LicenceNb    \r\n Cohabiting:10680   Min.   : 50.00   Min.   :1.000  \r\n Divorced  :  189   1st Qu.: 50.00   1st Qu.:1.000  \r\n Married   : 3554   Median : 57.00   Median :2.000  \r\n Single    : 2037   Mean   : 62.98   Mean   :1.885  \r\n Widowed   :  541   3rd Qu.: 72.00   3rd Qu.:2.000  \r\n NA's      :34948   Max.   :156.00   Max.   :7.000  \r\n                                                    \r\n        PayFreq                  JobCode          VehAge      \r\n Annual     :17579   Private employee: 9147   Min.   : 0.000  \r\n Half-yearly:28978   Public employee : 5045   1st Qu.: 4.000  \r\n Monthly    : 1486   Retiree         : 1035   Median : 7.000  \r\n Quarterly  : 3906   Other           :  856   Mean   : 7.525  \r\n                     Craftsman       :  637   3rd Qu.:10.000  \r\n                     (Other)         :  281   Max.   :89.000  \r\n                     NA's            :34948                   \r\n        VehClass        VehPower        VehGas     \r\n Cheapest   :17894   P10    :9148   Diesel :20615  \r\n Cheaper    :15176   P12    :8351   Regular:31334  \r\n Cheap      : 9027   P11    :8320                  \r\n Medium low : 5566   P9     :6671                  \r\n Medium     : 2021   P13    :6605                  \r\n Medium high: 1385   P8     :5188                  \r\n (Other)    :  880   (Other):7666                  \r\n                   VehUsage                           Garage     \r\n Private+trip to office:50435   Closed collective parking: 9820  \r\n Professional          : 1089   Closed zbox              :26318  \r\n Professional run      :  425   Opened collective parking: 8620  \r\n                                Street                   : 7191  \r\n                                                                 \r\n                                                                 \r\n                                                                 \r\n      Area                Region      Channel   Marketing \r\n A5     :15108   Center      :27486   A:30220   M1:26318  \r\n A3     :12696   Headquarters: 9788   B: 6111   M2: 8620  \r\n A2     : 7414   Paris area  : 7860   L:15618   M3: 9820  \r\n A7     : 6879   South West  : 6815             M4: 7191  \r\n A4     : 3779                                            \r\n A9     : 3397                                            \r\n (Other): 2676                                            \r\n PremWindscreen     PremDamAll         PremFire         PremAcc1    \r\n Min.   :  0.00   Min.   :   0.00   Min.   : 0.000   Min.   : 0.00  \r\n 1st Qu.: 13.00   1st Qu.:   0.00   1st Qu.: 0.000   1st Qu.: 0.00  \r\n Median : 22.00   Median :   0.00   Median : 4.000   Median : 0.00  \r\n Mean   : 25.78   Mean   :  83.06   Mean   : 4.421   Mean   :12.94  \r\n 3rd Qu.: 35.00   3rd Qu.: 144.00   3rd Qu.: 7.000   3rd Qu.:32.00  \r\n Max.   :264.00   Max.   :1429.00   Max.   :50.000   Max.   :77.00  \r\n                                                                    \r\n    PremAcc2       PremLegal        PremTPLM         PremTPLV     \r\n Min.   :  0.0   Min.   : 0.00   Min.   :  38.9   Min.   : 0.000  \r\n 1st Qu.:  0.0   1st Qu.: 8.00   1st Qu.: 102.6   1st Qu.: 5.000  \r\n Median :  0.0   Median :10.00   Median : 141.5   Median : 7.000  \r\n Mean   : 15.4   Mean   :10.43   Mean   : 167.7   Mean   : 8.571  \r\n 3rd Qu.: 45.0   3rd Qu.:12.00   3rd Qu.: 204.8   3rd Qu.:11.000  \r\n Max.   :198.0   Max.   :50.00   Max.   :1432.7   Max.   :68.000  \r\n                                                                  \r\n    PremServ        PremTheft         PremTot      \r\n Min.   :  0.00   Min.   :  0.00   Min.   :  91.0  \r\n 1st Qu.: 51.00   1st Qu.:  0.00   1st Qu.: 269.7  \r\n Median : 53.00   Median : 38.00   Median : 381.4  \r\n Mean   : 53.77   Mean   : 46.58   Mean   : 428.7  \r\n 3rd Qu.: 57.00   3rd Qu.: 68.00   3rd Qu.: 530.4  \r\n Max.   :237.00   Max.   :642.00   Max.   :3163.3  \r\n                                                   \r\n\r\nHowever, the data quality result shown under summary function is often not sufficient. Instead of just looking at the quantile, it’s also important to check other information, such as there is any missing value, what is the distribution of the variable look like, what is coefficient variation, and so on.\r\nBelow are three examples of modern R functions to check the data quality:\r\nskim\r\n\r\n\r\nskim(df)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n30\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n1\r\nfactor\r\n13\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nIDpol\r\n0\r\n1\r\n5\r\n13\r\n0\r\n32117\r\n0\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nDrivGender\r\n0\r\n1.00\r\nFALSE\r\n2\r\nM: 34155, F: 17794\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nPayFreq\r\n0\r\n1.00\r\nFALSE\r\n4\r\nHal: 28978, Ann: 17579, Qua: 3906, Mon: 1486\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\nVehClass\r\n0\r\n1.00\r\nFALSE\r\n9\r\nChe: 17894, Che: 15176, Che: 9027, Med: 5566\r\nVehPower\r\n0\r\n1.00\r\nFALSE\r\n15\r\nP10: 9148, P12: 8351, P11: 8320, P9: 6671\r\nVehGas\r\n0\r\n1.00\r\nFALSE\r\n2\r\nReg: 31334, Die: 20615\r\nVehUsage\r\n0\r\n1.00\r\nFALSE\r\n3\r\nPri: 50435, Pro: 1089, Pro: 425\r\nGarage\r\n0\r\n1.00\r\nFALSE\r\n4\r\nClo: 26318, Clo: 9820, Ope: 8620, Str: 7191\r\nArea\r\n0\r\n1.00\r\nFALSE\r\n10\r\nA5: 15108, A3: 12696, A2: 7414, A7: 6879\r\nRegion\r\n0\r\n1.00\r\nFALSE\r\n4\r\nCen: 27486, Hea: 9788, Par: 7860, Sou: 6815\r\nChannel\r\n0\r\n1.00\r\nFALSE\r\n3\r\nA: 30220, L: 15618, B: 6111\r\nMarketing\r\n0\r\n1.00\r\nFALSE\r\n4\r\nM1: 26318, M3: 9820, M2: 8620, M4: 7191\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nprofiling_num\r\n\r\n\r\nprofiling_num(df)\r\n\r\n\r\n         variable        mean     std_dev variation_coef   p_01\r\n1            Year 2003.381759   0.4858226   0.0002425013 2003.0\r\n2         DrivAge   39.835146  11.8652020   0.2978576243   21.0\r\n3      BonusMalus   62.983330  15.3002405   0.2429252393   50.0\r\n4       LicenceNb    1.884733   0.6664325   0.3535951452    1.0\r\n5          VehAge    7.525477   4.7817572   0.6354091975    0.0\r\n6  PremWindscreen   25.779053  20.4951432   0.7950308962    0.0\r\n7      PremDamAll   83.055285 105.7145996   1.2728220676    0.0\r\n8        PremFire    4.421317   4.4846822   1.0143317368    0.0\r\n9        PremAcc1   12.942001  16.7326001   1.2928912922    0.0\r\n10       PremAcc2   15.400219  23.8030155   1.5456283309    0.0\r\n11      PremLegal   10.426399   3.9025924   0.3742991579    4.0\r\n12       PremTPLM  167.737897  96.9041558   0.5777117614   51.8\r\n13       PremTPLV    8.570521   5.2053077   0.6073501958    0.0\r\n14       PremServ   53.770852   5.1332147   0.0954646327   42.0\r\n15      PremTheft   46.584169  48.7219715   1.0458911785    0.0\r\n16        PremTot  428.687713 224.5838408   0.5238868156  129.1\r\n     p_05   p_25   p_50   p_75   p_95     p_99  skewness  kurtosis\r\n1  2003.0 2003.0 2003.0 2004.0 2004.0 2004.000 0.4867707  1.236946\r\n2    24.0   31.0   38.0   47.0   63.0   73.000 0.8090189  3.339693\r\n3    50.0   50.0   57.0   72.0   93.0  106.000 1.1457567  3.756333\r\n4     1.0    1.0    2.0    2.0    3.0    4.000 0.9380943  5.925007\r\n5     1.0    4.0    7.0   10.0   15.0   21.000 1.5128291 15.246811\r\n6     0.0   13.0   22.0   35.0   65.0   92.000 1.5801010  8.575197\r\n7     0.0    0.0    0.0  144.0  283.0  420.000 1.5981568  7.474360\r\n8     0.0    0.0    4.0    7.0   13.0   20.000 1.7369281  9.079772\r\n9     0.0    0.0    0.0   32.0   39.0   44.000 0.5764758  1.461663\r\n10    0.0    0.0    0.0   45.0   57.0   64.000 0.9548824  2.088081\r\n11    5.0    8.0   10.0   12.0   17.0   23.000 1.3283374  6.830506\r\n12   68.0  102.6  141.5  204.8  352.0  512.100 2.1710206 11.739705\r\n13    3.0    5.0    7.0   11.0   18.0   26.000 1.9178789 10.133990\r\n14   46.0   51.0   53.0   57.0   62.0   68.000 1.8713567 51.639566\r\n15    0.0    0.0   38.0   68.0  136.0  216.000 1.9999025 11.271121\r\n16  169.8  269.7  381.4  530.4  852.3 1187.504 1.7016729  8.533766\r\n     iqr          range_98         range_80\r\n1    1.0      [2003, 2004]     [2003, 2004]\r\n2   16.0          [21, 73]         [27, 56]\r\n3   22.0         [50, 106]         [50, 85]\r\n4    1.0            [1, 4]           [1, 3]\r\n5    6.0           [0, 21]          [2, 14]\r\n6   22.0           [0, 92]          [0, 52]\r\n7  144.0          [0, 420]         [0, 221]\r\n8    7.0           [0, 20]          [0, 10]\r\n9   32.0           [0, 44]          [0, 36]\r\n10  45.0           [0, 64]          [0, 53]\r\n11   4.0           [4, 23]          [6, 15]\r\n12 102.2     [51.8, 512.1]    [78.8, 290.8]\r\n13   6.0           [0, 26]          [4, 15]\r\n14   6.0          [42, 68]         [48, 60]\r\n15  68.0          [0, 216]         [0, 106]\r\n16 260.7 [129.1, 1187.504] [199.48, 713.12]\r\n\r\nstatus\r\n\r\n\r\nstatus(df)\r\n\r\n\r\n                     variable q_zeros      p_zeros  q_na      p_na\r\nIDpol                   IDpol       0 0.0000000000     0 0.0000000\r\nYear                     Year       0 0.0000000000     0 0.0000000\r\nDrivAge               DrivAge       0 0.0000000000     0 0.0000000\r\nDrivGender         DrivGender       0 0.0000000000     0 0.0000000\r\nMaritalStatus   MaritalStatus       0 0.0000000000 34948 0.6727367\r\nBonusMalus         BonusMalus       0 0.0000000000     0 0.0000000\r\nLicenceNb           LicenceNb       0 0.0000000000     0 0.0000000\r\nPayFreq               PayFreq       0 0.0000000000     0 0.0000000\r\nJobCode               JobCode       0 0.0000000000 34948 0.6727367\r\nVehAge                 VehAge    1712 0.0329553986     0 0.0000000\r\nVehClass             VehClass       0 0.0000000000     0 0.0000000\r\nVehPower             VehPower       0 0.0000000000     0 0.0000000\r\nVehGas                 VehGas       0 0.0000000000     0 0.0000000\r\nVehUsage             VehUsage       0 0.0000000000     0 0.0000000\r\nGarage                 Garage       0 0.0000000000     0 0.0000000\r\nArea                     Area       0 0.0000000000     0 0.0000000\r\nRegion                 Region       0 0.0000000000     0 0.0000000\r\nChannel               Channel       0 0.0000000000     0 0.0000000\r\nMarketing           Marketing       0 0.0000000000     0 0.0000000\r\nPremWindscreen PremWindscreen    7153 0.1376927371     0 0.0000000\r\nPremDamAll         PremDamAll   26087 0.5021655855     0 0.0000000\r\nPremFire             PremFire   14450 0.2781574236     0 0.0000000\r\nPremAcc1             PremAcc1   32183 0.6195114439     0 0.0000000\r\nPremAcc2             PremAcc2   36379 0.7002829698     0 0.0000000\r\nPremLegal           PremLegal       2 0.0000384993     0 0.0000000\r\nPremTPLM             PremTPLM       0 0.0000000000     0 0.0000000\r\nPremTPLV             PremTPLV    1353 0.0260447747     0 0.0000000\r\nPremServ             PremServ       2 0.0000384993     0 0.0000000\r\nPremTheft           PremTheft   14654 0.2820843520     0 0.0000000\r\nPremTot               PremTot       0 0.0000000000     0 0.0000000\r\n               q_inf p_inf      type unique\r\nIDpol              0     0 character  32117\r\nYear               0     0   numeric      2\r\nDrivAge            0     0   numeric     72\r\nDrivGender         0     0    factor      2\r\nMaritalStatus      0     0    factor      5\r\nBonusMalus         0     0   numeric     68\r\nLicenceNb          0     0   numeric      7\r\nPayFreq            0     0    factor      4\r\nJobCode            0     0    factor      7\r\nVehAge             0     0   numeric     49\r\nVehClass           0     0    factor      9\r\nVehPower           0     0    factor     15\r\nVehGas             0     0    factor      2\r\nVehUsage           0     0    factor      3\r\nGarage             0     0    factor      4\r\nArea               0     0    factor     10\r\nRegion             0     0    factor      4\r\nChannel            0     0    factor      3\r\nMarketing          0     0    factor      4\r\nPremWindscreen     0     0   numeric    177\r\nPremDamAll         0     0   numeric    612\r\nPremFire           0     0   numeric     49\r\nPremAcc1           0     0   numeric     45\r\nPremAcc2           0     0   numeric     63\r\nPremLegal          0     0   numeric     43\r\nPremTPLM           0     0   numeric   1097\r\nPremTPLV           0     0   numeric     58\r\nPremServ           0     0   numeric     70\r\nPremTheft          0     0   numeric    377\r\nPremTot            0     0   numeric   9140\r\n\r\nThese functions show more info than just quantile. They also show info such as:  - Number of unique categories under categorical variables  - The proportion of missing values in the data  - Standard deviation of the numeric variables \r\nBelow are some insights we could draw from the functions above: \r\nExcessive missing value (i.e. more than half of the values are missing) in MaritalStatus & JobCode. More than 65% of the data from these two columns have missing values. So, it doesn’t like the variables will yield meaningful results if they are used to build machine learning models \r\n\r\n\r\ndf %>% dplyr::select(MaritalStatus, JobCode) %>% skim()\r\n\r\n\r\nTable 2: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n2\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nMaritalStatus\r\n34948\r\n0.33\r\nFALSE\r\n5\r\nCoh: 10680, Mar: 3554, Sin: 2037, Wid: 541\r\nJobCode\r\n34948\r\n0.33\r\nFALSE\r\n7\r\nPri: 9147, Pub: 5045, Ret: 1035, Oth: 856\r\n\r\nAbout 15 unique categories under VehPower. Might not ideal to keep so many unique categories within the variables as it might create noise when fitting machine learning models \r\n\r\n\r\ndf %>% group_by(VehPower) %>% tally()\r\n\r\n\r\n# A tibble: 15 x 2\r\n   VehPower     n\r\n   <fct>    <int>\r\n 1 P10       9148\r\n 2 P11       8320\r\n 3 P12       8351\r\n 4 P13       6605\r\n 5 P14       3773\r\n 6 P15       1510\r\n 7 P16        720\r\n 8 P17         58\r\n 9 P2          15\r\n10 P4          46\r\n11 P5         597\r\n12 P6           2\r\n13 P7         945\r\n14 P8        5188\r\n15 P9        6671\r\n\r\nAlso, note that the count of some categories is relatively low. Perhaps it is better to group these ‘low count categories’ since they are unlikely to have a significant impact on the predicted results.\r\nMaximum BonusMalus can go up to 156. According to the data dictionary, <100 means bonus, >100 means malus. Hence, this is okay. \r\n\r\n\r\ndf %>% dplyr::select(BonusMalus) %>% skim()\r\n\r\n\r\nTable 3: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.3\r\n50\r\n50\r\n57\r\n72\r\n156\r\n▇▂▁▁▁\r\n\r\nInterestingly, the premium for different benefits can range very widely. This is probably due to the difference in the respective burning cost under each benefit. \r\n\r\n\r\ndf %>% dplyr::select(starts_with(\"Prem\")) %>% skim()\r\n\r\n\r\nTable 4: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n11\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nI prefer skim function as its output is in a nice & neat format. It also provides most of the crucial info one is looking for in checking the data quality.\r\nIf I am only interested in checking the data quality for the numeric variables, this can be found by piping the variables together.\r\nFirst, I have specified the dataset. Next, I use select_if function to select all the numeric columns before using skim function to skim through the dataset.\r\n\r\n\r\ndf %>% \r\n  select_if(is.numeric) %>%\r\n  skim()\r\n\r\n\r\nTable 5: Data summary\r\nName\r\nPiped data\r\nNumber of rows\r\n51949\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n16\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nYear\r\n0\r\n1\r\n2003.38\r\n0.49\r\n2003.0\r\n2003.0\r\n2003.0\r\n2004.0\r\n2004.0\r\n▇▁▁▁▅\r\nDrivAge\r\n0\r\n1\r\n39.84\r\n11.87\r\n18.0\r\n31.0\r\n38.0\r\n47.0\r\n97.0\r\n▆▇▃▁▁\r\nBonusMalus\r\n0\r\n1\r\n62.98\r\n15.30\r\n50.0\r\n50.0\r\n57.0\r\n72.0\r\n156.0\r\n▇▂▁▁▁\r\nLicenceNb\r\n0\r\n1\r\n1.88\r\n0.67\r\n1.0\r\n1.0\r\n2.0\r\n2.0\r\n7.0\r\n▇▁▁▁▁\r\nVehAge\r\n0\r\n1\r\n7.53\r\n4.78\r\n0.0\r\n4.0\r\n7.0\r\n10.0\r\n89.0\r\n▇▁▁▁▁\r\nPremWindscreen\r\n0\r\n1\r\n25.78\r\n20.50\r\n0.0\r\n13.0\r\n22.0\r\n35.0\r\n264.0\r\n▇▁▁▁▁\r\nPremDamAll\r\n0\r\n1\r\n83.06\r\n105.71\r\n0.0\r\n0.0\r\n0.0\r\n144.0\r\n1429.0\r\n▇▁▁▁▁\r\nPremFire\r\n0\r\n1\r\n4.42\r\n4.48\r\n0.0\r\n0.0\r\n4.0\r\n7.0\r\n50.0\r\n▇▁▁▁▁\r\nPremAcc1\r\n0\r\n1\r\n12.94\r\n16.73\r\n0.0\r\n0.0\r\n0.0\r\n32.0\r\n77.0\r\n▇▁▃▁▁\r\nPremAcc2\r\n0\r\n1\r\n15.40\r\n23.80\r\n0.0\r\n0.0\r\n0.0\r\n45.0\r\n198.0\r\n▇▃▁▁▁\r\nPremLegal\r\n0\r\n1\r\n10.43\r\n3.90\r\n0.0\r\n8.0\r\n10.0\r\n12.0\r\n50.0\r\n▇▆▁▁▁\r\nPremTPLM\r\n0\r\n1\r\n167.74\r\n96.90\r\n38.9\r\n102.6\r\n141.5\r\n204.8\r\n1432.7\r\n▇▁▁▁▁\r\nPremTPLV\r\n0\r\n1\r\n8.57\r\n5.21\r\n0.0\r\n5.0\r\n7.0\r\n11.0\r\n68.0\r\n▇▁▁▁▁\r\nPremServ\r\n0\r\n1\r\n53.77\r\n5.13\r\n0.0\r\n51.0\r\n53.0\r\n57.0\r\n237.0\r\n▁▇▁▁▁\r\nPremTheft\r\n0\r\n1\r\n46.58\r\n48.72\r\n0.0\r\n0.0\r\n38.0\r\n68.0\r\n642.0\r\n▇▁▁▁▁\r\nPremTot\r\n0\r\n1\r\n428.69\r\n224.58\r\n91.0\r\n269.7\r\n381.4\r\n530.4\r\n3163.3\r\n▇▁▁▁▁\r\n\r\nThe piping method is made possible because the functions are following the tidy data concept, which makes it easier to “play” with the data.\r\nOften, visualizing data might provide us unexpected insights. The data can “tell” us underlying/hidden stories by leveraging on the power of data visualization.\r\nFor example, I would like to find out the scatterplot between PremTot against the selected numeric variables.\r\n\r\n\r\nnum_list <- c(\"Year\", \"DrivAge\", \"BonusMalus\", \"LicenceNb\", \"VehAge\")\r\n\r\n\r\nfor (i in num_list){\r\n  print(ggplot(df, aes(x = get(i), y = log(PremTot))) +\r\n      geom_point() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nThe graphs show that the data type for Year and LicenceNb is incorrect. They are supposed to read as factors, instead of numeric variables.\r\nTo fix this, I will use mutate and factor function to transform the data into the correct data type.\r\n\r\n\r\ndf_1 <- df %>%\r\n  mutate(Year = factor(Year),\r\n         LicenceNb = factor(LicenceNb))\r\n\r\n\r\n\r\nAlternatively, sometimes we just want to find out the graphs of a list of variables, instead of a small subset of variables. By typing down all the variables names can be a hassle and prone to human error.\r\nThis is where the different R functions come to the “rescue”.\r\nFor example, I am interested to find out the boxplot of all the factor variables against PremTot. The code chunk below has shown how we could pipe the different functions together to select all the factor columns and extract out the column names as a list.\r\n\r\n\r\ncat_list <- df_1 %>%\r\n  select_if(is.factor) %>%\r\n  names()\r\n\r\n\r\n\r\nSubsequently, I will use for loop to plot the necessary graphs. I will explain the awesome-ness of ggplot function in my future post.\r\n\r\n\r\nfor (i in cat_list){\r\n  print(ggplot(df_1, aes(x = get(i), y = log(PremTot))) +\r\n      geom_boxplot() +\r\n        xlab(i)\r\n      )\r\n}\r\n\r\n\r\n\r\n\r\nBelow are some of the insights drawn from the graphs above (not limited to):  - Average PremTot for Retailer is higher than the rest  - Average PremTot also varies significantly across different VehPower  - Somehow the premium for a diesel car is higher than a regular car  - PremTot for professional & professional run is higher than private+trip to office \r\nConclusion\r\nOkay, that’s all the sharing for this post!\r\nI have shown the awesome-ness of tidyverse through this post. There are many more functions within tidyverse universe, which are not covered in this post. Do check out their website for many more awesome functions!\r\nFeel free to contact me through email or LinkedIn if you have any suggestions on future topics to share.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-22-data-wrangling/image/data_cowboy.png",
    "last_modified": "2021-09-02T01:31:08+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-my-data-science-journey/",
    "title": "The Unconventional Path",
    "description": "Journey of Joining the \"Dark\" Side",
    "author": [
      {
        "name": "Jasper Lok",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nPhoto taken in Taipei\r\nDuring a stormy night in 2018, it was raining cats and dogs outside. Suddenly there was thunder and lightning. Jeng jeng jeng, I suddenly decided to take up a master degree to study data science.\r\nNah, I always wish it was that simple to make such a big decision in life.\r\nIt took me about 5 years to decide that I would go back to university for a data science degree.\r\nIt took me 2.5 years to have the courage in taking the leap to go for a change in job function for the 1st time. \r\nWas I worried or scared when I made these choice? Oh absolutely.\r\nUnconventional Path - Work After working on actuarial pricing for more than 4 years, I have asked for a change in job function, which I have chickened out twice times after the requests. There were a lot of ‘what-if’ in my head.\r\n“What if I don’t like the new function?”\r\n“What if I struggle to understand the new concepts?”\r\n“What if I screwed up?”\r\n“What if I get along with them?”\r\nHowever, on the third time, I came to the conclusion that if I don’t take the leap of faith, I would never step out from my comfort zone.\r\nMeanwhile, unfortunately sometimes life is not a bed of roses. Even if it is a bed of roses, it is also a bed of roses with full of thorns.\r\nInitially, things were not as smooth as what I wanted. Team mates left one after another one and things were pretty messy. It was through these tough times I discovered my strength, how I could play my strength and make a difference.\r\nFast forward to 1.5 years later, I was being offered to change my job scope again at my work. However, this change in job scope was even more drastic. I was being offered to change my job scope from life insurance to general insurance.\r\nAm I scared? Still as worried as usual.\r\nHowever, I decided that I would regret if I did not take the leap of faith again. My industry friends were shocked, they even checked with me to verify the news. Deep down I knew that worry does not solve anything. So, I worked my a** hard to put in the extra effort to understand the fundamental concepts for general insurance.\r\nWithin less than 3 months after joining the team, I was asked to cover group insurance while my colleague was on maternity leave. At the moment, I was worried as my main expertise was never in short term insurance business and I was expecting to guide one of the junior on this piece of work. Despite of the hiccups at the beginning, things turned out better than what I initially imagined.\r\nThese past success have nevertheless given me more confidence in myself.\r\nUnconventional Path - Data Science Journey I first discovered my interest for data science back in my undergraduate studies. I still remember it was generalized linear model project on Titanic dataset that sparked my interest.\r\n\r\n\r\n\r\nScreenshot of my GLM report\r\nAfter I started working, I never forget my interest in this area. However, as life goes on, I was not sure this data science was for me since actuarial science and data science are still different (in terms of concepts and work) although they are somewhat similar.\r\nUntil few years ago, I was once exposed a bit on data science at work again. That re-kindled my interest for data science. I knew that it is not going to work if I rely others for my learning, eg. waiting to learn from work, waiting for others to teach me and so on. Eventually, I decided to go back into university to study data science.\r\nThroughout these processes, I have encountered many “Why” from myself and others.\r\n“Why would you still want to study, given that you have been studying for your actuarial exams after graduating from university?”\r\n“Why are you still studying this when you hold an actuarial science degree?”\r\n“Why are you studying when your working hours can be quite bad?”\r\n“Why are you spending so much money on getting another degree? Are you sure you can earn back the money?”\r\n“Just why??”\r\nI knew it is going to tough and tiring to work full time + study part time, although I was complaining that I did not its going to be this tiring.\r\nLooking back now, did I regret anything? Absolutely no.\r\nIn fact, I met some of my “gui ren” (ie. people who are great help to one’s life) through my this journey. Prof Kam Tin Seong has spent so much time with me guiding me through my capstone project, explaining the concepts of data science techniques (eg. how to do proper clustering, considerations when building a model), making me working very hard for the weekly data visualization makeover and many more. Another of my gui ren has to be Prof Wong Yuet Nan. The valuable advice and experience he shared with me has built up my knowledge and confidence.\r\nWas I stressed during these process? Super duper stressed. I was so so stressed out until there were days my brain couldn’t think or function.\r\nDeep down I knew what are opportunity cost and price I am paying, but I am willing to going through the pain. Sleeping at 2am almost every day, occasionally running back to work after class ended at 10pm and fell asleep at random places due to lack of enough sleep are really nothing when I compared to these valuable experience and confidence I have gained.\r\nWould I trade these experience for anything? Definitely no.\r\nTakeaway Do I have the certainty of what is going to happen next when I was making the above decisions? Nope, often I am not sure how things will work out.\r\nDo I want to have the certainty in these situations? Oh absolutely yes. However, I knew life does not work this way. I would miss out so much fun if I demand on the certainty.\r\nIt was taking these seemed “unconventional paths” that provided me the opportunities to discover amazing things. Just like the the photo shown below, it was taken with no filter during my Japan trip. Instead of walking along the main road like what other tourists were doing, I followed another path that is less taken and discovered this awesome view.\r\n\r\n\r\n\r\nPhoto taken in Hakone, Japan\r\nNevertheless, this is my story of unconventional path.\r\nLife is short, so go & live a life with no regret! :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-17-my-data-science-journey/image/journey.jpg",
    "last_modified": "2021-01-20T23:23:41+08:00",
    "input_file": {}
  }
]
