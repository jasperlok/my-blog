theme_minimal()
text_df_stem_1_tidy_count %>%
filter(str_detect(term, "(young?)") == TRUE)
text_df_lemma <- text_df %>%
tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma)
text_df_lemma_1 <- text_df_lemma %>%
dfm() %>%
dfm_trim(min_termfreq = 8)
text_df_lemma_1_tidy <- tidy(text_df_lemma_1) %>%
filter(term != "")
text_df_lemma_1_tidy_count <- text_df_lemma_1_tidy %>%
group_by(term) %>%
summarise(tot_count = sum(count))
text_df_lemma_1_tidy_count %>%
filter(tot_count >= 350) %>%
ggplot(aes(label = term, size = tot_count, color = tot_count)) +
geom_text_wordcloud_area(shape  = "square") +
scale_size_area(max_size = 18) +
theme_minimal()
text_df_lemma_1_tidy_count %>%
filter(str_detect(term, "(young?)") == TRUE)
knitr::include_graphics("image/bookshelf.jpg")
View(text_df_stem_1_tidy_count)
options(htmltools.dir.version = FALSE)
packages <- c("captioner", "knitr", "tidyverse", "kableExtra")
for (p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning = FALSE,
out.width="90%")
library(captioner)
knitr::include_graphics("image/typewriter.jpg")
knitr::include_graphics("image/quanteda.png")
knitr::include_graphics("image/regex_cheatsheet.png")
tokenization_df <- data.frame(Types = c("Sentence Tokenization",
"Word Tokenization"),
Descriptions = c("Split text into sentences",
"Split text into words"))
tokenization_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stopwords_df <- data.frame(Grouping = c("Global stopwords",
"Subject-specific stopwords",
"Document-level stopwords"),
Descriptions = c("Words that are almost always low in meaning in a given language",
"Words that are uninformative for a given a subject area",
"Words that do not provide any or much information for a given document"),
Recommendation = c("Use pre-made stopwords lists to remove them",
"May improve performance if we have domain expertise to create a good list to remove them. But likely require us to create the list manually as these words are generally not considered as stopwords in a given language",
"Difficult to classify and would not be worth the effort to identify"))
stopwords_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stem_lemma_df <- data.frame(Category = c("Stemming",
"Lemmatization"),
Descriptions = c("- Rule-based
- Work by cutting off the end or the beginning of the words",
"- Linguistics-based
- Normalize the words based on language structure and how words are used in the their context
- Take the morphological analysis of the words into considerations
- Generally longer runtime than stemming"))
stem_lemma_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
knitr::include_graphics("image/textanalytics_with_r.png")
packages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', 'ggwordcloud', 'lexicon')
for(p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
df <- read_tsv("data/WikiQA.tsv",
quote = "\t")
text_df <- tokens(df$Sentence,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE)
text_df <- text_df %>%
tokens_tolower()
text_df <- text_df %>%
tokens_remove(stopwords(language = "en", source = "smart"), padding = FALSE)
stopwords::stopwords_getsources()
text_df[[51]][2]
df$Sentence[51]
text_df <- text_df %>%
tokens_replace(pattern = c("\\d\\w*"), replacement = c(""), valuetype = "regex")
text_df[[2735]][4]
df$Sentence[2735]
text_df <- text_df %>%
tokens_replace(pattern = c("\\b([^-\\s]+)\\b"), replacement = c( ""), valuetype = "regex")
text_df <- text_df %>%
tokens_replace(pattern = c("united st", "u.s"), replacement = c("united state", "united state"), valuetype = "fixed")
text_df_stem <- text_df %>%
tokens_wordstem(language = "english")
text_df_stem_1 <- text_df_stem %>%
dfm() %>%
dfm_trim(min_termfreq = 8)
text_df_stem_1_tidy <- tidy(text_df_stem_1) %>%
filter(term != "")
text_df_stem_1_tidy_count <- text_df_stem_1_tidy %>%
group_by(term) %>%
summarise(tot_count = sum(count))
options(htmltools.dir.version = FALSE)
packages <- c("captioner", "knitr", "tidyverse", "kableExtra")
for (p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning = FALSE,
out.width="90%")
library(captioner)
knitr::include_graphics("image/typewriter.jpg")
knitr::include_graphics("image/quanteda.png")
knitr::include_graphics("image/regex_cheatsheet.png")
tokenization_df <- data.frame(Types = c("Sentence Tokenization",
"Word Tokenization"),
Descriptions = c("Split text into sentences",
"Split text into words"))
tokenization_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stopwords_df <- data.frame(Grouping = c("Global stopwords",
"Subject-specific stopwords",
"Document-level stopwords"),
Descriptions = c("Words that are almost always low in meaning in a given language",
"Words that are uninformative for a given a subject area",
"Words that do not provide any or much information for a given document"),
Recommendation = c("Use pre-made stopwords lists to remove them",
"May improve performance if we have domain expertise to create a good list to remove them. But likely require us to create the list manually as these words are generally not considered as stopwords in a given language",
"Difficult to classify and would not be worth the effort to identify"))
stopwords_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stem_lemma_df <- data.frame(Category = c("Stemming",
"Lemmatization"),
Descriptions = c("- Rule-based
- Work by cutting off the end or the beginning of the words",
"- Linguistics-based
- Normalize the words based on language structure and how words are used in the their context
- Take the morphological analysis of the words into considerations
- Generally longer runtime than stemming"))
stem_lemma_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
knitr::include_graphics("image/textanalytics_with_r.png")
packages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', 'ggwordcloud', 'lexicon')
for(p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
df <- read_tsv("data/WikiQA.tsv",
quote = "\t")
text_df <- tokens(df$Sentence,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = FALSE)
text_df <- text_df %>%
tokens_tolower()
text_df <- text_df %>%
tokens_remove(stopwords(language = "en", source = "smart"), padding = FALSE)
stopwords::stopwords_getsources()
text_df[[51]][2]
df$Sentence[51]
text_df <- text_df %>%
tokens_replace(pattern = c("\\\-*"), replacement = c(""), valuetype = "regex")
View(text_df_stem_1_tidy_count)
options(htmltools.dir.version = FALSE)
packages <- c("captioner", "knitr", "tidyverse", "kableExtra")
for (p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning = FALSE,
out.width="90%")
library(captioner)
knitr::include_graphics("image/typewriter.jpg")
knitr::include_graphics("image/quanteda.png")
knitr::include_graphics("image/regex_cheatsheet.png")
tokenization_df <- data.frame(Types = c("Sentence Tokenization",
"Word Tokenization"),
Descriptions = c("Split text into sentences",
"Split text into words"))
tokenization_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stopwords_df <- data.frame(Grouping = c("Global stopwords",
"Subject-specific stopwords",
"Document-level stopwords"),
Descriptions = c("Words that are almost always low in meaning in a given language",
"Words that are uninformative for a given a subject area",
"Words that do not provide any or much information for a given document"),
Recommendation = c("Use pre-made stopwords lists to remove them",
"May improve performance if we have domain expertise to create a good list to remove them. But likely require us to create the list manually as these words are generally not considered as stopwords in a given language",
"Difficult to classify and would not be worth the effort to identify"))
stopwords_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stem_lemma_df <- data.frame(Category = c("Stemming",
"Lemmatization"),
Descriptions = c("- Rule-based
- Work by cutting off the end or the beginning of the words",
"- Linguistics-based
- Normalize the words based on language structure and how words are used in the their context
- Take the morphological analysis of the words into considerations
- Generally longer runtime than stemming"))
stem_lemma_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
knitr::include_graphics("image/textanalytics_with_r.png")
packages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', 'ggwordcloud', 'lexicon')
for(p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
df <- read_tsv("data/WikiQA.tsv",
quote = "\t")
text_df <- tokens(df$Sentence,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = FALSE)
text_df <- text_df %>%
tokens_tolower()
text_df <- text_df %>%
tokens_remove(stopwords(language = "en", source = "smart"), padding = FALSE)
stopwords::stopwords_getsources()
text_df[[51]][2]
df$Sentence[51]
text_df <- text_df %>%
tokens_replace(pattern = c("\\\-*"), replacement = c(""), valuetype = "regex")
text_df <- text_df %>%
tokens_replace(pattern = c("\\-*"), replacement = c(""), valuetype = "regex")
text_df[[2735]][4]
df$Sentence[2735]
text_df <- text_df %>%
tokens_replace(pattern = c("\\-"), replacement = c(""), valuetype = "regex")
options(htmltools.dir.version = FALSE)
packages <- c("captioner", "knitr", "tidyverse", "kableExtra")
for (p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning = FALSE,
out.width="90%")
library(captioner)
knitr::include_graphics("image/typewriter.jpg")
knitr::include_graphics("image/quanteda.png")
knitr::include_graphics("image/regex_cheatsheet.png")
tokenization_df <- data.frame(Types = c("Sentence Tokenization",
"Word Tokenization"),
Descriptions = c("Split text into sentences",
"Split text into words"))
tokenization_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stopwords_df <- data.frame(Grouping = c("Global stopwords",
"Subject-specific stopwords",
"Document-level stopwords"),
Descriptions = c("Words that are almost always low in meaning in a given language",
"Words that are uninformative for a given a subject area",
"Words that do not provide any or much information for a given document"),
Recommendation = c("Use pre-made stopwords lists to remove them",
"May improve performance if we have domain expertise to create a good list to remove them. But likely require us to create the list manually as these words are generally not considered as stopwords in a given language",
"Difficult to classify and would not be worth the effort to identify"))
stopwords_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
stem_lemma_df <- data.frame(Category = c("Stemming",
"Lemmatization"),
Descriptions = c("- Rule-based
- Work by cutting off the end or the beginning of the words",
"- Linguistics-based
- Normalize the words based on language structure and how words are used in the their context
- Take the morphological analysis of the words into considerations
- Generally longer runtime than stemming"))
stem_lemma_df %>%
kbl() %>%
kable_paper("hover", full_width = F, html_font = "Cambria", font_size = 15)
knitr::include_graphics("image/textanalytics_with_r.png")
packages <- c('tidyverse', 'readr', 'skimr', 'tidytext', 'quanteda', 'ggwordcloud', 'lexicon')
for(p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
df <- read_tsv("data/WikiQA.tsv",
quote = "\t")
text_df <- tokens(df$Sentence,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
split_hyphens = FALSE)
text_df <- text_df %>%
tokens_tolower()
text_df <- text_df %>%
tokens_remove(stopwords(language = "en", source = "smart"), padding = FALSE)
stopwords::stopwords_getsources()
text_df[[51]][2]
df$Sentence[51]
text_df <- text_df %>%
tokens_replace(pattern = c("\\d\\w*"), replacement = c(""), valuetype = "regex")
text_df[[2735]][4]
df$Sentence[2735]
text_df <- text_df %>%
tokens_replace(pattern = c("\\-"), replacement = c(""), valuetype = "regex")
text_df <- text_df %>%
tokens_replace(pattern = c("united st", "u.s"), replacement = c("united state", "united state"), valuetype = "fixed")
text_df_stem <- text_df %>%
tokens_wordstem(language = "english")
text_df_stem_1 <- text_df_stem %>%
dfm() %>%
dfm_trim(min_termfreq = 8)
text_df_stem_1_tidy <- tidy(text_df_stem_1) %>%
filter(term != "")
text_df_stem_1_tidy_count <- text_df_stem_1_tidy %>%
group_by(term) %>%
summarise(tot_count = sum(count))
View(text_df_stem_1_tidy_count)
options(htmltools.dir.version = FALSE)
packages <- c("captioner", "knitr", "tidyverse", "kableExtra")
for (p in packages){
if(!require (p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
knitr::opts_chunk$set(fig.retina = 3,
echo = TRUE,
eval = TRUE,
message = FALSE,
warning = FALSE,
out.width="90%")
library(captioner)
text_df_stem_1_tidy_count
text_df_stem_1_tidy_count %>%
arrange(desc(tot_count))
text_df_stem_1
text_df
text_df %>%
filter(str_detect(term, "(young?)") == TRUE)
View(text_df)
text_df %>%
filter(str_detect(value, "(young?)") == TRUE)
text_df %>%
dfm() %>%
tidy() %>%
filter(str_detect(value, "(young?)") == TRUE)
text_df %>%
dfm() %>%
tidy() # %>%
#  filter(str_detect(value, "(young?)") == TRUE)
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "")
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(value, "(young?)") == TRUE)
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE)
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE) %>%
unique(term)
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE) %>%
unique()
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE) %>%
unique(term)
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE)
text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE) %>%
unique(term)
unique(temp$term)
temp <- text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE)
unique(temp$term)
temp <- text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young)") == TRUE)
unique(temp$term)
temp <- text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(tree)") == TRUE)
temp <- text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(tree)") == TRUE)
unique(temp$term)
temp <- text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "\\b(tree)") == TRUE)
temp <- text_df %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "\\b(tree)") == TRUE)
unique(temp$term)
text_df_stem_1_tidy_count %>%
arrange(desc(tot_count))
text_df_stem_1_tidy_count %>%
filter(str_detect(term, "(tree?)") == TRUE)
text_df_stem_1_tidy_count %>%
filter(str_detect(term, "\\b(tree)") == TRUE)
text_df_stem_1_tidy_count %>%
filter(str_detect(term, "(young)") == TRUE)
text_df_stem_1_tidy_count %>%
filter(tot_count >= 350) %>%
ggplot(aes(label = term, size = tot_count, color = tot_count)) +
geom_text_wordcloud_area(shape  = "circle") +
scale_size_area(max_size = 18) +
theme_minimal()
text_df_lemma %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE)
text_df_lemma <- text_df %>%
tokens_replace(pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma)
text_df_lemma %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE)
text_df_lemma %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
group_by(term) %>%
filter(str_detect(term, "(young?)") == TRUE)
text_df_lemma %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young?)") == TRUE) %>%
group_by(term) %>%
tally()
text_df_lemma %>%
dfm() %>%
tidy() %>%
filter(term != "") %>%
filter(str_detect(term, "(young)") == TRUE) %>%
group_by(term) %>%
tally()
